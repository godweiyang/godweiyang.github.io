<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>算法编程小白机试指南（大佬勿进）</title>
      <link href="/2019/07/12/algorithm-code-tricks/"/>
      <url>/2019/07/12/algorithm-code-tricks/</url>
      
        <content type="html"><![CDATA[<blockquote><p>大佬就不用往下看了，这篇文章没有任何逻辑，没有任何进阶的指导意义，纯粹为了应付各种机试（夏令营机试、保研机试、程序设计实践考试等等），对正经编程竞赛没有任何帮助。我就想到哪写到哪了，不定期想到新的在更新。</p></blockquote><h1 id="暴力打表法"><a href="#暴力打表法" class="headerlink" title="暴力打表法"></a>暴力打表法</h1><hr><h2 id="题目1"><a href="#题目1" class="headerlink" title="题目1"></a>题目1</h2><p>给定$n$个数字$1, 2, \ldots, n$，求任意取一个排列，任意第$i$个位置上的元素都不等于$i$的概率是多少？</p><p>如果知道结论的话，这就是一道普通的错位排列题，常规做法是求出递推式<br>\[f(n) = (n-1)(f(n-1)+f(n-2))\]<br>然后除以全排列的数量$n!$就行了，这里就不讲怎么求的了，百度有很多。这里讲讲如果不会求怎么办？</p><p>首先想到的暴力方法就是暴力枚举所有排列，然后看看有多少排列满足题目中的错位的条件。C++中的库函数<code>next_permutation</code>正好可以帮助我们枚举全排列，代码如下：</p><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;bits/stdc++.h></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span class="token keyword">typedef</span> <span class="token keyword">long</span> <span class="token keyword">long</span> LL<span class="token punctuation">;</span>LL a<span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">int</span> b<span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> n <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">;</span> n <span class="token operator">&lt;=</span> <span class="token number">20</span><span class="token punctuation">;</span> <span class="token operator">++</span>n<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">&lt;=</span> n<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>            b<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> i<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">int</span> cnt <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token keyword">do</span> <span class="token punctuation">{</span>            <span class="token keyword">int</span> flag <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">&lt;=</span> n<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>b<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> i<span class="token punctuation">)</span> <span class="token punctuation">{</span>                    flag <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>                    <span class="token keyword">break</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>            cnt <span class="token operator">+</span><span class="token operator">=</span> flag<span class="token punctuation">;</span>        <span class="token punctuation">}</span> <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token function">next_permutation</span><span class="token punctuation">(</span>b <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> b <span class="token operator">+</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        a<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">=</span> cnt<span class="token punctuation">;</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d, "</span><span class="token punctuation">,</span> cnt<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>然后就可以跑出$n \le 12$的结果，但是再大就跑不出来了，因为全排列数量太多了，跑得太慢了。但是不用管，因为题目要求的不是错位排列的数量，而是除以全排列数量之后的概率，巧的是，$n &gt; 12$之后概率保留两位小数的结果是完全相同的，所以直接取$n = 12$的概率就行了，代码如下：</p><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;bits/stdc++.h></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span class="token keyword">typedef</span> <span class="token keyword">long</span> <span class="token keyword">long</span> LL<span class="token punctuation">;</span>LL p<span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">}</span><span class="token punctuation">;</span>LL a<span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">44</span><span class="token punctuation">,</span> <span class="token number">265</span><span class="token punctuation">,</span> <span class="token number">1854</span><span class="token punctuation">,</span> <span class="token number">14833</span><span class="token punctuation">,</span> <span class="token number">133496</span><span class="token punctuation">,</span> <span class="token number">1334961</span><span class="token punctuation">,</span> <span class="token number">14684570</span><span class="token punctuation">,</span> <span class="token number">176214841</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">30</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        p<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> p<span class="token punctuation">[</span>i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>LL<span class="token punctuation">)</span>i<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">int</span> T<span class="token punctuation">;</span>    <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>T<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>T<span class="token operator">--</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span> n<span class="token punctuation">;</span>        <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>n<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>n <span class="token operator">></span> <span class="token number">12</span><span class="token punctuation">)</span>            n <span class="token operator">=</span> <span class="token number">12</span><span class="token punctuation">;</span>        <span class="token keyword">double</span> res <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">double</span><span class="token punctuation">)</span>a<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">/</span> p<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100.0</span><span class="token punctuation">;</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%.2f%%\n"</span><span class="token punctuation">,</span> res<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>这样即使你完全不会计算，也可以100分通过这题啦。</p><h2 id="题目2"><a href="#题目2" class="headerlink" title="题目2"></a>题目2</h2><p><a href="https://acm.ecnu.edu.cn/problem/3337/" target="_blank" rel="noopener">原题链接</a></p><p>这题其实就是给你$n$个数组，计算任意两个指定数组相同元素的个数。</p><p>首先想到的最暴力的方法就是，两层循环遍历两个数组咯，看有多少一样的元素就行了。那我们试试：</p><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;bits/stdc++.h></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span class="token keyword">const</span> <span class="token keyword">int</span> MAXN <span class="token operator">=</span> <span class="token number">40000</span> <span class="token operator">+</span> <span class="token number">10</span><span class="token punctuation">;</span>vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> G<span class="token punctuation">[</span>MAXN<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">int</span> len<span class="token punctuation">[</span>MAXN<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> n<span class="token punctuation">,</span> m<span class="token punctuation">;</span>    <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>n<span class="token punctuation">,</span> <span class="token operator">&amp;</span>m<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> m<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span> u<span class="token punctuation">,</span> v<span class="token punctuation">;</span>        <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>u<span class="token punctuation">,</span> <span class="token operator">&amp;</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>        G<span class="token punctuation">[</span>u<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>        G<span class="token punctuation">[</span>v<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span>u<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">&lt;=</span> n<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        len<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> G<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">int</span> q<span class="token punctuation">;</span>    <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>q<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>q<span class="token operator">--</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span> s<span class="token punctuation">,</span> t<span class="token punctuation">;</span>        <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>s<span class="token punctuation">,</span> <span class="token operator">&amp;</span>t<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> cnt <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> len<span class="token punctuation">[</span>s<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> len<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>G<span class="token punctuation">[</span>s<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> G<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                    cnt<span class="token operator">++</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d\n"</span><span class="token punctuation">,</span> cnt<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p><img src="3337-1.jpg" alt></p><p>结果已经不错了，过了大多数样例了，这时你实在不想做了，拿了这点分数也可以做下一题了。</p><p>但是你稍微动点脑子，就可以发现，可以把所有数组提前排个序啊，然后遍历的时候就不需要每次都从头找起了，代码如下：</p><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;bits/stdc++.h></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span class="token keyword">const</span> <span class="token keyword">int</span> MAXN <span class="token operator">=</span> <span class="token number">40000</span> <span class="token operator">+</span> <span class="token number">10</span><span class="token punctuation">;</span>vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> G<span class="token punctuation">[</span>MAXN<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">int</span> len<span class="token punctuation">[</span>MAXN<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> n<span class="token punctuation">,</span> m<span class="token punctuation">;</span>    <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>n<span class="token punctuation">,</span> <span class="token operator">&amp;</span>m<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> m<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span> u<span class="token punctuation">,</span> v<span class="token punctuation">;</span>        <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>u<span class="token punctuation">,</span> <span class="token operator">&amp;</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>        G<span class="token punctuation">[</span>u<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>        G<span class="token punctuation">[</span>v<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span>u<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">&lt;=</span> n<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">sort</span><span class="token punctuation">(</span>G<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> G<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        len<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> G<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">int</span> q<span class="token punctuation">;</span>    <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>q<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>q<span class="token operator">--</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span> s<span class="token punctuation">,</span> t<span class="token punctuation">;</span>        <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>s<span class="token punctuation">,</span> <span class="token operator">&amp;</span>t<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> cnt <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> len<span class="token punctuation">[</span>s<span class="token punctuation">]</span> <span class="token operator">&amp;&amp;</span> j <span class="token operator">&lt;</span> len<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>G<span class="token punctuation">[</span>s<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">></span> G<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token operator">++</span>j<span class="token punctuation">;</span>            <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>G<span class="token punctuation">[</span>s<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> G<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                    <span class="token operator">++</span>cnt<span class="token punctuation">;</span>                <span class="token punctuation">}</span>                <span class="token operator">++</span>i<span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d\n"</span><span class="token punctuation">,</span> cnt<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p><img src="3337-2.jpg" alt></p><p>然后你就会发现，结果并没有任何变化。。。不过理论上来说是会快一点的，这里数据可能比较小。</p><p>所以这里应该想不到啥优化的好方法了，不会做的话就下一题吧，分数够了，下面是正确代码，用<code>bitset</code>实现的：</p><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;bits/stdc++.h></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span class="token keyword">const</span> <span class="token keyword">int</span> MAXN <span class="token operator">=</span> <span class="token number">40000</span> <span class="token operator">+</span> <span class="token number">10</span><span class="token punctuation">;</span>bitset<span class="token operator">&lt;</span>MAXN<span class="token operator">></span> G<span class="token punctuation">[</span>MAXN<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> n<span class="token punctuation">,</span> m<span class="token punctuation">;</span>    <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>n<span class="token punctuation">,</span> <span class="token operator">&amp;</span>m<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> m<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span> u<span class="token punctuation">,</span> v<span class="token punctuation">;</span>        <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>u<span class="token punctuation">,</span> <span class="token operator">&amp;</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>        G<span class="token punctuation">[</span>u<span class="token punctuation">]</span><span class="token punctuation">[</span>v<span class="token punctuation">]</span> <span class="token operator">=</span> G<span class="token punctuation">[</span>v<span class="token punctuation">]</span><span class="token punctuation">[</span>u<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">int</span> q<span class="token punctuation">;</span>    <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>q<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>q<span class="token operator">--</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span> s<span class="token punctuation">,</span> t<span class="token punctuation">;</span>        <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>s<span class="token punctuation">,</span> <span class="token operator">&amp;</span>t<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d\n"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>G<span class="token punctuation">[</span>s<span class="token punctuation">]</span> <span class="token operator">&amp;</span> G<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 机试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unsupervised Recurrent Neural Network Grammars</title>
      <link href="/2019/04/20/naacl19-urnng/"/>
      <url>/2019/04/20/naacl19-urnng/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=476513774&auto=1&height=66"></iframe></div><p><strong>论文地址：</strong><a href="http://arxiv.org/abs/1904.03746" target="_blank" rel="noopener">Unsupervised Recurrent Neural Network Grammars</a><br><strong>代码地址：</strong><a href="https://github.com/harvardnlp/urnng" target="_blank" rel="noopener">github</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>这篇是新鲜出炉的NAACL19的关于无监督循环神经网络文法（URNNG）的论文，在语言模型和无监督成分句法分析上都取得了非常不错的结果，主要采用了变分推理和RNNG。本文公式量较大，因此我也推了好久，算法也挺多的，首先上一张我推导的公式笔记：<br><img src="1.jpg" alt></p><p>我这篇博客就不按照论文的顺序来讲了，就按照我上面这张笔记讲一讲我的理解吧，很多细节可能会忽略，请参见原文吧。</p><p>首先对于无监督成分句法分析，常规做法就是学习一个生成模型$p_{\theta}(x, z)$，就比如RNNG就是一个生成模型，但是缺少句法树$z$的监督信号怎么办呢？现在给你的输入只有句子$x$，那么只能用语言模型$p_{\theta}(x)$来做监督了。习惯上我们喜欢取对数，也就是：<br>\[<br>\log p_{\theta}(x) = \log \sum_z p_{\theta}(x, z)<br>\]<br>这里就存在几个问题，比如$z$的状态空间太大了，不可能穷举所有的，所以接下来按步骤讲解如何求解。</p><h1 id="URNNG模型"><a href="#URNNG模型" class="headerlink" title="URNNG模型"></a>URNNG模型</h1><hr><p>先上一张模型图，让大家对整体模型有个大概的认知：<br><img src="2.png" alt><br>左边是一个推理网络（Inference Network），用来根据输入$x$推理出隐变量也就是句法树$z$的概率分布$q_{\phi}(z | x)$。右边是一个生成模型（Generative Model），用来计算从推理网络中采样出来的句法树$z$的联合概率$p_{\theta}(x, z)$，最后根据上面语言模型算出句子的概率，最大化这个概率即可。</p><p>接下来分别讲解这两个部分和具体的优化方法。</p><h2 id="Inference-Network-q-phi-z-x"><a href="#Inference-Network-q-phi-z-x" class="headerlink" title="Inference Network $q_{\phi}(z | x)$"></a>Inference Network $q_{\phi}(z | x)$</h2><p>首先将词向量$e_i$和位置向量$p_i$拼接，作为推理网络LSTM的输入：<br>\[<br>f_i, b_i = {\rm BiLSTM}([e_i, p_i])<br>\]<br>然后算出span $(i, j)$的得分，计算方式和以往一样，用BiLSTM前后向输出做差，然后通过一个前馈神经网络得到分数：<br>\[<br>s_{ij} = {\rm MLP}([f_{j+1} - f_i; b_{i-1} - b_j])<br>\]<br>接下来就需要计算句法树的概率分布了，这里不直接计算句法树$z$，而是计算它的邻接矩阵$B$的概率分布，这个邻接矩阵意思就是如果span $(i, j)$存在，那么$B_{ij} = 1$，否则的话$B_{ij} = 0$。然后就可以用CRF计算出邻接矩阵$B$对应的概率：<br>\[<br>q_{\phi}(B | x) = \frac{1}{Z_T(x)}\exp(\sum_{i \le j} B_{ij}s_{ij})<br>\]<br>其中$Z_T(x)$是配分函数，也就是用来将概率归约到0到1之间的：<br>\[<br>Z_T(x) = \sum_{B’ \in \mathcal B_T} \exp(\sum_{i \le j} B’_{ij}s_{ij})<br>\]<br>注意这里的$\mathcal B_T$并不是所有的01矩阵集合，而是必须满足能产生合法句法树的矩阵，而这情况也很多，不能穷举求解，在这里采用经典的inside算法来求解这个配分函数：<br><img src="3.jpg" alt><br>不过我觉得这里是错的！就是这里的两处$s_{ij}$应该改成$\exp(s_{ij})$。不过具体代码实现的时候并没有这么做，初始值一样都是$\beta[i,i]=s_{ii}$，但是递推的时候采用了如下式子：<br>\[<br>\beta[i, j] = \log\sum_{k=i}^{j-1}\exp(s_{ij}+\beta[i,k]+\beta[k+1,j])<br>\]<br>其实就是用$e^{\beta}$来取代$\beta$了，化简后就是代码实现这个式子，应该是为了防止数值溢出。</p><p>然后就是采样了，推理网络目的就是计算出句法树的概率分布，然后根据这个分布采样出若干个句法树，那么现在给定一棵句法树可以根据上面的算法计算出它的概率了，那怎么采样呢？其实还是可以通过刚刚计算得出的$\beta$数组来采样，采样算法如下:<br><img src="4.jpg" alt><br>其实就是自顶向下的根据概率分布来采样每个span的split，用一个队列来保存所有还没有采样出split的span，然后把所有采样出的span在邻接矩阵中的对应值标为1。</p><p>最后推理网络采样出了若干个句法树$z$，然后根据CRF计算出每个句法树的概率$q_{\phi}(z | x)$，后面的事情就交给生成网络了。</p><h2 id="Generative-Model-p-theta-x-z"><a href="#Generative-Model-p-theta-x-z" class="headerlink" title="Generative Model $p_{\theta}(x, z)$"></a>Generative Model $p_{\theta}(x, z)$</h2><p>上面的推理网络采样出了若干个句法树$z$，生成网络的目的就是计算它的联合概率$p_{\theta}(x, z)$。这个其实不难，在之前的RNNG论文笔记中，我已经大致讲过了，可以去复习一下：<a href="https://godweiyang.com/2018/09/02/RNNG/">Recurrent Neural Network Grammars</a>，这里稍稍做了一些改进。</p><p>首先需要定义一个栈用来存放转移的历史状态，这里定义栈里放的元素为二元组$(h, g)$，一个是stack-LSTM编码的输出，一个是子树的结构表示。首先需要预测下一步的action是什么，所以取出栈顶的元素$(h_{prev}, g_{prev})$，预测action的时候只要用到隐含层输出：<br>\[<br>p_t = \sigma(w^T h_{prev} + b)<br>\]<br>然后根据这个概率预测action是SHIFT还是REDUCE，下面分两种情况讨论。</p><p>如果是SHIFT，那么因为是生成模型，所以需要预测下一个移进的单词是什么：<br>\[<br>x  \sim  softmax(Wh_{prev} + b)<br>\]<br>然后将单词$x$的词向量输入到stack-LSTM中得到下一个时刻的隐含层输出：<br>\[<br>h_{next} = {\rm LSTM}(e_x, h_{prev})<br>\]<br>最后将$(h_{next}, e_x)$推进栈里。</p><p>如果是REDUCE，那么首先需要取出栈顶的两个元素$(h_r, g_r)$和$(h_l, g_l)$，然后用TreeLSTM计算出两个子结点合并后的子树的表示：<br>\[<br>g_{new} = {\rm TreeLSTM}(g_l, g_r)<br>\]<br>接着还是计算stack-LSTM下一个时刻的隐含层输出：<br>\[<br>h_{new} = {\rm LSTM}(g_{new}, h_{prev})<br>\]<br>最后将$(h_{new}, g_{new})$推进栈里。</p><p>为了防止数值溢出，常规上我们计算联合概率的对数：<br>\[<br>\log p_{\theta}(x, z) = \sum_{t=1}^T \log p_{\theta}(x_t | x_{&lt; t}, z_{&lt; n(t)}) + \sum_{j=1}^{2T-1} \log p_{\theta}(z_j | x_{&lt; m(j)}, z_{&lt; j})<br>\]<br>从这个式子可以看出，联合概率定义为所有给定某段单词和action预测下一个单词和给定某段单词和action预测下一个action的概率之积。</p><p>如果是监督任务比如RNNG，那么只需要最大化这个联合概率就足够了，但是现在要做无监督，没有$z$，注意别搞混了，推理网络采样出的$z$可不能用来监督哦，因为那本来就不是正确的，所以接下来要采用语言模型来作为最终的目标函数。</p><h2 id="Variational-Inference"><a href="#Variational-Inference" class="headerlink" title="Variational Inference"></a>Variational Inference</h2><p>句子$x$的对数概率定义为：<br>\[<br>\log p_{\theta}(x) = \log \sum_{z \in {\mathcal Z}_T} {p_{\theta}(x, z)}<br>\]<br>其中${\mathcal Z}_T$是所有合法句法树的集合，但是这里不可能穷举所有的句法树，所以就要用到变分推理，具体的理论知识不仔细介绍了，可以去查阅变分推理相关知识，下面直接推导。<br>\[<br>\begin{array}{l}\log {p_\theta }(x) = \log \sum\limits_{z \in {\mathcal{Z}_T}} { {p_\theta }(x,z)} \\ = \log\sum\limits_{z \in {\mathcal{Z}_T}} { {q_\phi }(z|x)\frac{ { {p_\theta }(x,z)}}{ { {q_\phi }(z|x)}}} \\ = \log { {\mathbb E}_{ {q_\phi }(z|x)}}\left[ {\frac{ { {p_\theta }(x,z)}}{ { {q_\phi }(z|x)}}} \right]\\ \ge { {\mathbb E}_{ {q_\phi }(z|x)}}\left[ {\log \frac{ { {p_\theta }(x,z)}}{ { {q_\phi }(z|x)}}} \right]\end{array}<br>\]<br>其中最后一行叫做先验$\log p_{\theta}(x)$的证据下界（ELBO），要想最大化先验，可以最大化这个ELBO，如果我们对这个ELBO变化一下形式可以得到：<br>\[<br>\begin{array}{l}{\rm ELBO} = { {\mathbb E}_{ {q_\phi }(z|x)}}\left[ {\log \frac{ { {p_\theta }(x,z)}}{ { {q_\phi }(z|x)}}} \right]\\ = { {\mathbb E}_{ {q_\phi }(z|x)}}\left[ {\log \frac{ { {p_\theta }(z|x){p_\theta }(x)}}{ { {q_\phi }(z|x)}}} \right]\\ = { {\mathbb E}_{ {q_\phi }(z|x)}}\left[ {\log {p_\theta }(x)} \right] - { {\mathbb E}_{ {q_\phi }(z|x)}}\left[ {\log \frac{ { {q_\phi }(z|x)}}{ { {p_\theta }(z|x)}}} \right]\\ = \log {p_\theta }(x) - {\rm KL}({q_\phi }(z|x)\parallel {p_\theta }(z|x))\end{array}<br>\]<br>所以这个ELBO和先验就相差了一个KL散度，最大化ELBO的话等价于最小化KL散度，也就是使推理网络产生句法树的概率分布和生成模型尽量接近。</p><p>但是这个ELBO还是不好算，尽管它把$\log$移到了求和符号也就是期望里面，所以转换一下形式：<br>\[<br>{\rm ELBO} = {\mathbb E}_{q_{\phi}(z|x)}\left[ \log p_{\theta}(x,z) \right] - {\mathbb H} \left[ q_{\phi}(z|x) \right]<br>\]<br>因为模型一共有两组参数，一个是推理网络的参数$\phi$，一个是生成网络的参数$\theta$，所以下面分别对两个参数求导。</p><p>首先对$\theta$求偏导，因为只有第一项有这个参数，所以偏导为：<br>\[<br>\nabla_{\theta}{\rm ELBO} = {\mathbb E}_{q_{\phi}(z|x)}\left[ \nabla_{\theta} \log p_{\theta}(x,z) \right]<br>\]<br>这个偏导可以按照概率$q_{\phi}(z|x)$采样得到：<br>\[<br>\nabla_{\theta}{\rm ELBO} \approx \frac{1}{K}\sum_{k=1}^{K} {\nabla_{\theta} \log p_{\theta}(x,z_k)}<br>\]</p><p>然后对$\phi$求偏导，因为有两项含有这个参数，分别求偏导。第二项是熵，它的值其实可以用之前的$\beta$数组算出来，算法如下：<br><img src="5.jpg" alt><br>然后偏导可以交给深度学习库的自动微分，就不用你自己求啦。</p><p>至于第一项的偏导可以用类似于策略梯度的方法解决：<br>\[<br>\begin{array}{l}{\nabla _\phi }{\mathbb{E}_{ {q_\phi }(z|x)}}\left[ {\log {p_\theta }(x,z)} \right]\\ = {\nabla _\phi }\sum\limits_z { {q_\phi }(z|x)\log {p_\theta }(x,z)} \\ = \sum\limits_z {\log {p_\theta }(x,z){\nabla _\phi }{q_\phi }(z|x)} \\ = \sum\limits_z { {q_\phi }(z|x)\log {p_\theta }(x,z){\nabla _\phi }\log {q_\phi }(z|x)} \\ = {\mathbb{E}_{ {q_\phi }(z|x)}}\left[ {\log {p_\theta }(x,z){\nabla _\phi }\log {q_\phi }(z|x)} \right]\\ \approx \frac{1}{K}\sum\limits_{k = 1}^K {\log {p_\theta }(x,{z_k}){\nabla _\phi }\log {q_\phi }({z_k}|x)} \end{array}<br>\]<br>这里最后也是转化为了采样，和策略梯度做法类似，这里加入baseline来提升性能：<br>\[<br>\begin{array}{l}{\nabla _\phi }{\mathbb{E}_{ {q_\phi }(z|x)}}\left[ {\log {p_\theta }(x,z)} \right]\\ \approx \frac{1}{K}\sum\limits_{k = 1}^K {\log {p_\theta }(x,{z_k}){\nabla _\phi }\log {q_\phi }({z_k}|x)} \\ \approx \frac{1}{K}\sum\limits_{k = 1}^K {(\log {p_\theta }(x,{z_k}) - {r_k}){\nabla _\phi }\log {q_\phi }({z_k}|x)} \end{array}<br>\]<br>其中$r_k$定义为所有其他的对数联合概率的均值：<br>\[<br>r_k = \frac{1}{K-1} \sum_{j \ne k} \log p_{\theta}(x, z_j)<br>\]</p><p>至此所有偏导都已求出来了，两个通过采样得到，一个通过inside算法结果自动微分得到，所以去掉导数符号并相加就得到了最终的损失函数：<br>\[<br>{\mathcal L}(\phi, \theta) \approx \frac{1}{K} \sum_{k=1}^K {\left[ \log p_{\theta}(x, z_k) + (\log p_{\theta}(x, z_k) - r_k)\log q_{\phi}(z_k|x) \right]} - {\mathbb H}\left[ q_{\phi}(z|x) \right]<br>\]<br>一定要注意，这里的$\log p_{\theta}(x, z_k) - r_k$在代码实现的时候不能传入梯度，不然的话对$\theta$的偏导就会多出这一项的偏导了！</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><p>实验结果这里就不多说了，细节具体看论文吧，就贴两个结果，一个是语言模型：<br><img src="6.jpg" alt><br>可以看出在标准的PTB数据集上，URNNG效果只比监督学习的RNNG和用URNNG损失函数微调后的RNNG效果略差一点，但是在大数据集上，URNNG的优势就体现出来了。</p><p>另一个是无监督成分句法分析，这里是用的全部长度的测试集：<br><img src="7.jpg" alt><br>这个任务上URNNG效果是最好的。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><hr><p>和之前两篇语言模型做无监督成分句法分析类似，这篇论文用推理网络学习句法树的概率分布并采样句法树，再用生成网络计算这些句法树和句子的联合概率，最后用变分推理最大化句子的概率，也就是学习出一个好的语言模型。</p><p>这篇论文的工作还是挺令人惊叹的，融合了inside算法、RNNG、变分推理等等知识。本来我变分推理听老师讲了好几次了都云里雾里的，看了这篇论文后总算弄懂了一点了，不过所了解的还是很少，EM算法、VAE之类的高级境界根本不会。。。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
            <tag> NAACL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Language Modeling by Jointly Learning Syntax and Lexicon</title>
      <link href="/2019/03/31/iclr18-prpn/"/>
      <url>/2019/03/31/iclr18-prpn/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=32507551&auto=1&height=66"></iframe></div><p><strong>论文地址：</strong><a href="https://arxiv.org/abs/1711.02013" target="_blank" rel="noopener">Neural Language Modeling by Jointly Learning Syntax and Lexicon</a><br><strong>代码地址：</strong><a href="https://github.com/yikangshen/PRPN" target="_blank" rel="noopener">github</a></p><blockquote><p>最近开始转向去看看一些无监督的成分句法分析论文，看看能否有一些启发QAQ。这篇博客摸鱼划水写了整整四天才写完，好累啊啊啊。</p></blockquote><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>一般来说，自然语言是由词汇和句法组成的，但是标准的语言模型一般都只用RNN对词汇进行建模，句法信息都是隐式的学习到的，没有办法显式地拿出来使用。所以本文提出的语言模型的变体可以结合结构上的attention，在中间过程中学习到结构信息，然后这层结构信息可以拿来生成句法树，用来做无监督的句法分析。</p><p>那么为什么要做无监督的句法分析呢？主要原因还是一些小语种标注语料太少了甚至没有，不能用监督句法分析来做。而且无监督句法分析学到的信息还可以用来增强语言模型或者更为下游的任务的性能，使它们能更好的融合句法结构信息。</p><p>本文提出的模型（PRPN）主要有如下三个组成部分：</p><ul><li><strong>可微分的Parsing Network</strong>。主要用来学习句子的句法距离（syntactic distance，这个在之前的博客中有讲到，是同一个组做的工作，链接：<a href="https://godweiyang.com/2018/07/19/ConParsing-Syntactic-Distance/">Straight to the Tree: Constituency Parsing with Neural Syntactic Distance</a>），然后产生出门函数（gate function），也就是句子中每个词的权重，这样跑RNN的时候就没有必要考虑之前的所有信息了，只需要考虑句法距离最近的，也就是同一个短语中的信息。这里可微分后面会详细讲到，也就是不能直接预测离散的序列，不然梯度没办法反向传播，所以预测的是一个分布。</li><li><strong>Reading Network</strong>。这个部分就和普通的RNN差不多，区别就是利用了上面的attention，然后每个时刻的输入不仅仅是上一个时刻的隐含层状态，还包括了历史所有相关的隐含层状态。</li><li><strong>Predict Network</strong>。最后预测下一个词也不是直接采用当前时刻的隐含层输出，而是采用所有attention后的历史隐含层信息。</li></ul><p>最后模型训练好之后，用Parsing Network学习到的句法距离就可以产生出无监督的句法树，当然这里只能产生unlabeled的句法树，也就是说，无监督的成分句法分析都是只评测unlabeled F1值，因为nonterminal信息实在是无法无监督的预测出来，除非结合外部标注器或者人工制定规则？</p><p><strong>由于我觉得这篇论文写作有点粗糙，很多地方写的很乱，甚至还有公式变量和图片不对应，所以下面的公式我有些自行修改过了，如果觉得有疑问的，可以参考原文，并且告诉我。</strong></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><hr><p>下图是一个输入句子及其对应的句法树示例：<br><img src="1.png" alt><br>其中实线表示成分句法树的结构，而虚线表示同一棵子树的若干个子结点之间的联系。我们用$(x_0, x_1, \ldots, x_n)$表示输入句子，用$y_i$表示句法树中的某一个结点，用$r$表示根结点。定义$y_i$所表示的短语范围是$(l(y_i), r(y_i))$，例如在上图中$y_1$表示的范围是$(2, 6)$，所以有$l(y_1) = 2$和$r(y_1) = 6$。</p><p>下图是模型的Reading Network和Predict Network大致框架：<br><img src="2.png" alt><br>其中实线表示Reading Network中RNN每个时刻的输入依赖于之前的哪些时刻，虚线表示Predict Network中预测下一个词时需要考虑哪些时刻的隐含层输出(<strong>这里有个小错误，图中最上面一行函数参数从$m_3$开始，但实际上虚线表明应该从$m_2$开始</strong>)。</p><p>首先是Parsing Network。这种和之前状态的连接在这里被叫做“跨越连接”（skip connection），而具体和哪些状态连接，就要用到门$g_i^t$，表示当前处在$t$时刻，和之前的第$i$时刻有没有关联，有就是1，没有就是0。在这里先定义变量$l_t$为$x_t$和之前最远到哪个位置的单词有关联，分成两种情况：</p><ul><li>如果$x_t$不是某个子树最左边的叶子结点，那么$l_t$就定义为它的父结点的最左边那个叶子结点，也就是$x_t$最左边那个兄弟结点。比如上图中的$x_4$，因为它在子树中位于中间，所以$l_4 = 3$，也就是说它最远和$x_3$有一定关联，而再往前的单词由于不在同一个短语里，关联就不大了。</li><li>如果$x_t$是某一个子树$y_i$最左边的子结点，那么$l_t$就定义为$y_i$最左边那个兄弟结点的最左边的叶子结点。比如上图中$l_3 = 2$，因为$x_3$是$y_3$的最左边的子结点，而$y_2$是$y_3$最左边的兄弟结点，$x_2$又是$y_2$最左边的叶子结点，也就是说$x_3$最远和之前的$x_2$有关联，它们共同组成了更大的短语$y_1$。（<strong>这里定义又不是很严谨，例如对于上面那个例子，$x_2$既是$y_1$又是$y_2$的最左子结点，这种情况下不能按照$y_2$来计算，因为它没有最左的兄弟结点！所以按照$y_1$来算的话得到$l_2 = 0$。</strong>）</li></ul><p>根据这个$l_t$就可以将$g_i^t$定义为：<br>\[<br>g_i^t = \left\{ {\begin{array}{}{1,{l_t} \le i &lt; t}\\{0,0 &lt; i &lt; {l_t}}\end{array}} \right.<br>\]<br>但是因为这是离散的决策，没法传递梯度，所以之后会介绍用概率来替代这里的0和1。</p><p>然后是Reading Network。得到了gates之后，就可以修改RNN，用下面公式计算$t$时刻的隐含层状态：<br>\[<br>m_t = h(x_t, m_0, \ldots, m_{t-1}, g_0^t, \ldots, g_{t-1}^t)<br>\]<br>具体是怎么使用这个gates的，后面再具体介绍，反正只要知道多考虑了很多个历史状态就行了，每个状态都有一个权重，表示考虑了多少。</p><p>最后就是Predict Network，用来预测下一个单词$x_{t+1}$，用到的是历史多个隐含层状态信息：<br>\[<br>p(x_{t+1} | x_0, \ldots, x_t) \approx p(x_{t+1} ; f(m_0, \ldots, m_t, g_0^{t+1}, \ldots, g_t^{t+1}))<br>\]<br>这里后面也会具体讲到。</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><hr><p><strong>这里我不按照论文上的顺序讲，论文是倒序，变量定义都得看完整篇论文才出来，实在是看的太杂乱了。。。</strong></p><h2 id="Parsing-Network"><a href="#Parsing-Network" class="headerlink" title="Parsing Network"></a>Parsing Network</h2><p><strong>第1步：</strong><br>首先第一步要预测出相邻两个单词的句法距离，用宽度为$L+1$的卷积核来对单词进行卷积。例如要预测$x_i$和$x_{i-1}$的句法距离，那么就对$x_{i-L}, x_{i-L+1}, \ldots, x_i$进行卷积（其实就是两层前馈神经网络），得到如下输出（<strong>注意原文中这一段里的词向量$e$应该都是$x$，有误</strong>）：<br>\[<br>\begin{array}{}{h_i = {\rm ReLU}(W_c \left[ {\begin{array}{*{20}{c}}e_{i-L}\\e_{i-L+1}\\ \cdots\\ e_i \end{array}} \right] + b_c)} \\ {d_i = {\rm ReLU}(W_dh_i+b_d)}\end{array}<br>\]<br>当然了，第二行也可以看做是窗口大小为1的卷积核，第一个单词之前要补上$L-1$个0。</p><p>最后得到的句法距离反映出了相邻两个词之间的语义相关程度，如果距离比较大，说明两个词可能位于两个不同的短语中，否则就位于同一个短语。一个形象化的句法距离就是相邻两个单词的最近公共祖先的高度。</p><p>下面这个图就是一个例子，卷积核窗口宽度为2：<br><img src="3.png" alt></p><p><strong>第2步：</strong><br>然后就可以根据句法距离，求出$x_t$最远到哪个位置有联系。用$\alpha_j^t$来衡量$x_t$与$x_j$的分别与前一个单词的句法距离的差距：<br>\[<br>\alpha_j^t = \frac{ {\rm hardtanh}((d_t - d_j) \cdot \tau) + 1}{2}<br>\]<br>这里的${\rm hardtanh}$就是当输入大于1或者小于-1时，截取掉，给它限制在-1到1这个范围内，所以最后$\alpha$的范围就是0到1。可以看出，如果$\alpha_j^t$很大，那么$d_t$就大于$d_j$，否则的话$d_t$就小于$d_j$。这也很合乎直观上的认知，如果$d_t$大于$d_j$，那么说明$x_j$和$x_{j-1}$的最近公共祖先的高度比较低，那么说明$x_j$和$x_t$在同一棵子树中，差值自然大，反之的话$x_j$是当前子树的最左边的子结点，差值就很小。</p><p>所以模型只需要一步步往左寻找第一个$\alpha_j^t$最接近0的点，也就是之前提到过的$l_t$。为了实现这个目标，可以定义$l_t = i$的概率为：<br>\[<br>p(l_t = i | x_0, \ldots, x_t) = (1 - \alpha_i^t) \prod_{j=i+1}^{t-1} {\alpha_j^t}<br>\]<br>为什么这么设计呢？其实还是很有道理的。概率越大的话，说明$\alpha_i^t$越小，而其他的$\alpha_j^t$越大，这就印证了上面提出的观点。而如果取$l_t &gt; i$的话，$1 - \alpha_i^t$一定很小，会拉低概率；反之取$l_t &lt; i$的话，会乘上一个很小的项$\alpha_i^t$，也会拉低概率。所以取$l_t = i$就是最优的。</p><p><strong>第3步：</strong><br>我们的最终目的是求出门$g_i^t$，也就是$x_t$与$x_i$的相关度。在之前的动机中，直接将$l_t$之后的设为1，之前的设为0，但是这样是离散序列，无法传播梯度，所以这里采用另一种方法来求。注意到只有$p(l_t = i | x_0, \ldots, x_t)$接近于1，而其他的概率都接近于0，所以只需要用概率密度函数来作为$g_i^t$就行了：<br>\[<br>g_i^t = P(l_t \le i) = \prod_{j=i+1}^{t-1} {\alpha_j^t}<br>\]<br>注意到这个概率密度函数在$i &lt; l_t$时值基本很小接近于0，而大于等于它时很大，越来越接近于1。这和设计的初衷已经很像了，所以可以近似用来作为$g_i^t$。</p><p><strong>这里要提几点证明，可看可不看。</strong></p><p><strong>证明开始</strong></p><hr><p>首先这个概率$p(l_t = i | x_0, \ldots, x_t)$的形式其实是一个狄利克雷过程，有两个特殊值要定义一下，一个是$l_t = t-1$时，概率为：<br>\[<br>p(l_t = t - 1 | x_0, \ldots, x_t) = (1 - \alpha_{t - 1}^t)<br>\]<br>还有就是当$l_t = 0$时，因为$d_0$实际不存在，所以定义为句法距离无穷大，那么$\alpha_0^t$定义为0，所以概率为：<br>\[<br>p(l_t = 0 | x_0, \ldots, x_t) = \prod_{j=1}^{t-1} {\alpha_j^t}<br>\]<br>然后可以求出$g_i^t$的期望（<strong>这里论文中又写的一塌糊涂，符号定义都不统一。。。</strong>）：<br>\[<br>\mathbb E(g_i^t) = \prod_{j=1}^{t-1} {\alpha_j^t} + (1 - \alpha_1^t)\prod_{j=2}^{t-1} {\alpha_j^t} + \ldots + (1 - \alpha_i^t)\prod_{j=i+1}^{t-1} {\alpha_j^t}<br>\]<br>进一步可以写为：<br>\[<br>\mathbb E(g_i^t) = \sum_{j=0}^i {p(l_t = j | x_0, \dots, x_t)} = P(l_t \le i)<br>\]<br>上面的求和裂项相消可以得到：<br>\[<br>P(l_t \le i) = \prod_{j=i+1}^{t-1} {\alpha_j^t}<br>\]<br>这里也可以验证出$P(l_t &lt; t) = 1$，所以最终得到下面的门是正确的：<br>\[<br>g_i^t = \prod_{j=i+1}^{t-1} {\alpha_j^t}<br>\]</p><p>然后要证明的就是根据这个策略求出的两个单词的关联区间没有交叉（但是可以完全覆盖）。首先放宽一下条件，证明当$\alpha_j^t$只取0或者1时，性质成立，也就是定义（<strong>这里原文又打错了。。。</strong>）：<br>\[<br>\alpha_j^t = \frac{ {\rm sign}(d_t - d_j) + 1}{2}<br>\]<br>也就是在原来的定义中令$\tau$为正无穷。这样的话如果$d_t &gt; d_j$，那么$\alpha_j^t$就是1，否则就是0。放宽了条件之后，$g_i^t$取值就只有1和0了，所以求出的$l_t$一定是满足句法距离$d_i &gt; d_t$所有位置中最右边的一个。而对于所有的$l_t &lt; i &lt; t$，都满足$d_i &lt; d_t$。证明也很简单，假设存在两个单词$x_v$和$x_n$，其中$v &lt; n$，它俩的关联区间交叉了，那么假设$x_v$的$l_t$为$u$，而$x_n$的$l_t$为$m$，所以$u &lt; m &lt; v &lt; n$。那么根据定义，有：<br>\[<br>\begin{array}{} {d_u &gt; d_v &gt; d_m \\ d_m &gt; d_n &gt; d_v} \end{array}<br>\]<br>这两个式子显然矛盾，所以证明了在宽松的条件下，这个不交叉的性质是成立的。</p><p>另一个极端是令$\tau = 0$，这种情况下$\alpha_j^t = \frac{1}{2}$，最终算下来$g_i^t$是一个首项为$\frac{1}{2^{t-1}}$公比为2的等比数列。这里的话论文里说最终会形成一棵所有叶子结点都直接连接在根结点上面的句法树，但是我是没看出来为什么，首先这种情况下句法距离$d$对最终的损失函数没有影响了，所以梯度无法传播回来，句法距离$d$应该只受到词向量影响了，最终就是一个普通的语言模型，句法距离最终会是什么样子谁也不知道。</p><p>在具体实现时，将$\tau$设为一个中间值，但是这样会产生交叉的关联区间，所以测试的时候再将其设为正无穷。</p><p><strong>证明结束</strong></p><hr><h2 id="Reading-Network"><a href="#Reading-Network" class="headerlink" title="Reading Network"></a>Reading Network</h2><p>上面的Parsing Network求了半天就是为了得到这个gates $g_j^t$，然后怎么使用呢？既然是语言模型，下面当然要用LSTM来对句子进行编码了。众所周知，在LSTM中有两个隐含层状态$h$和$c$，所以在这里不直接使用上一个时刻传过来的状态，而是用历史所有时刻的状态信息，再与gates加权后输入到下一时刻。在论文中并没有直接使用gates作为权重，而是计算了一个新的权重，这里称作结构化attention：<br>\[<br>\begin{array}{} {k_t = W_h h_{t-1} + W_x x_t \\ \tilde s_i^t = {\rm softmax}(\frac{h_i k_t^{T}}{\sqrt {\delta_k}})} \end{array}<br>\]<br>当然这还不是最终的权重，再通过加入gates，定义一个新的结构化intra-attention：<br>\[<br>s_i^t = \frac{g_i^t \tilde s_i^t}{\sum_i g_i^t}<br>\]<br>上面两个式子我也不是很清楚怎么解释，反正最后就用这个权重来对历史状态进行编码：<br>\[<br>\left[ \begin{array}{*{c}}{\tilde h_t \\ \tilde c_t}\end{array} \right] = \sum_{i=1}^{t-1} {s_i^t \cdot m_i} = \sum_{i=1}^{t-1} {s_i^t \cdot \left[ \begin{array}{*{c}}{h_i \\ c_i}\end{array} \right]}<br>\]<br>最后将加权求和后的隐含层状态作为下一个时刻的隐含层输入，得到新的隐含层表示。</p><h2 id="Predict-Network"><a href="#Predict-Network" class="headerlink" title="Predict Network"></a>Predict Network</h2><p>最后就是预测下一个单词是什么了，这里并没有直接用下一时刻$t + 1$的隐含层输出来预测，还是采用了attention，但是存在一个问题，$g_j^{t+1}$的值依赖于$d_{t+1}$，也就是依赖于$x_{t+1}$，但是这是我们需要预测的，那怎么办呢？论文中就直接用另一套参数直接预测出一个临时的$d_{t+1}’$：<br>\[<br>d_{t+1}’ = {\rm ReLU}(W_d’h_t + b_d’)<br>\]<br>最后对历史状态加权求和，然后用一个前馈神经网络预测下一个单词：<br>\[<br>f(m_0, \ldots, m_t, g_0^{t+1}, \ldots, g_t^{t+1}) = \hat f([h_{l:t-1}, h_t])<br>\]</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><p>具体实验设置啥的去看论文吧，这里就贴几个结果图解释一下。</p><p><img src="4.jpg" alt><br>上面这个是字符级别的语言模型的结果，PRPN取得了SOTA的结果。</p><p><img src="5.jpg" alt><br>上面这个是词级别的语言模型的结果，除了那个用了4层LSTM和跨越连接的模型（参数多）外，PRPN效果是最好的。</p><p>当然我最关心的还是无监督成分句法分析上面的结果：<br><img src="6.jpg" alt><br>目前为止，无监督成分句法分析的标准做法还是用WSJ10数据集（也就是长度小于等于10的句子），然后用unlabeled F1评测。可以看到，PRPN效果只能说一般般，不是很好。</p><p>其实我也很疑惑，这里的无监督句法分析全靠Parsing Network产生的句法距离来产生，但是一个关键问题是句法距离并没有监督，唯一的监督信号来自于最后Predict Network的语言模型，那么这个句法距离真的能够学习到真实的句法距离吗？我比较怀疑，不过效果证明，这个对语言模型还是有帮助的，对无监督成分句法分析的话不好说。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>这篇论文几个review都给出了还算不错的评价，思想是不错的，但是和其中一个review观点一样，我觉得文章写得太烂了，一堆的笔误，逻辑很混乱，即使这已经是review后修改过的版本了，还是有一堆错误。</p><p>过两天再看一篇ICLR2019的还是这个组的PRPN的改进版本，看看到底有何改进。不过真要搞无监督句法分析的话，还是老老实实去做聚类吧。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
            <tag> ICLR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Better, Faster, Stronger Sequence Tagging Constituent Parsers</title>
      <link href="/2019/03/11/naacl19-conparsing/"/>
      <url>/2019/03/11/naacl19-conparsing/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=416892296&auto=1&height=66"></iframe></div><blockquote><p>为了看懂论文里的策略梯度，又去把强化学习看了一遍。。。</p></blockquote><p><strong>论文地址：</strong><a href="https://arxiv.org/abs/1902.10985" target="_blank" rel="noopener">Better, Faster, Stronger Sequence Tagging Constituent Parsers</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>这篇论文主要是在之前的那篇论文<a href="https://godweiyang.com/2019/03/11/ConParSeqLab/">Constituent Parsing as Sequence Labeling</a>基础上解决了如下三个问题：</p><ul><li>太长的短语预测错误率高。</li><li>输出空间太大导致label稀疏性。</li><li>贪心解码导致的错误传播。</li></ul><p>本文提出的解决方法分别是：</p><ul><li>采用融合了相对编码和绝对编码的动态编码。</li><li>将预测任务分解为多个子任务。</li><li>采用辅助任务和策略梯度。</li></ul><h1 id="三大问题以及解决方法"><a href="#三大问题以及解决方法" class="headerlink" title="三大问题以及解决方法"></a>三大问题以及解决方法</h1><hr><h2 id="过长短语预测的高错误率"><a href="#过长短语预测的高错误率" class="headerlink" title="过长短语预测的高错误率"></a>过长短语预测的高错误率</h2><p>由下面这张图可以看出，当$n_i$太小时，准确率就会大幅下降。这个问题主要体现在过长短语的闭合上，右括号的预测尤其困难。其实这也跟数据稀疏性有很大关系，训练集中过长短语毕竟占少数。<br><img src="1.jpg" alt></p><p>解决方法就是采用动态编码，如下图所示：<br><img src="2.jpg" alt><br>第一行是相对值编码，第二行是绝对值编码，之前文章都已经解释过了。第三行是结合了上面两种编码的动态编码，具体取值情况是大多数时候都还采用相对值编码，因为毕竟相对值编码空间比较小，可以适当缓解数据稀疏性。但是当满足如下两种情况的时候，就采用绝对值编码：</p><ul><li>绝对值$n_t’ \leq 3$，也就是说CA的个数不能超过3个，这样也是为了降低数据的稀疏性。</li><li>相对值$n_t \leq -2$，也就是说将上图中准确率比较低的那些负数值全部用绝对值替代了，在句法树中表现为$w_{t+1}$所在的子树比$w_t$低两层以上。</li></ul><h2 id="输出空间太大导致label稀疏性"><a href="#输出空间太大导致label稀疏性" class="headerlink" title="输出空间太大导致label稀疏性"></a>输出空间太大导致label稀疏性</h2><p>这个问题主要是由于三元组$(n_t, c_t, u_t)$太稀疏了导致的。假设$n_t \in N, c_t \in C, u_t \in U$，那么这个三元组的状态空间是$\left| N \right| \times \left| C \right| \times \left| U \right|$，可以通过将三元组分解为三个不同的子任务将复杂度降低为$\left| N \right| + \left| C \right| + \left| U \right|$。最后的损失函数定义为三个子任务的损失之和：<br>\[<br>\mathcal{L} = \mathcal{L}_n + \mathcal{L}_c + \mathcal{L}_u<br>\]<br>具体实现上，可以将任务$U$的输出给任务$N$和$C$作为输入。</p><h2 id="贪心解码导致的错误传播"><a href="#贪心解码导致的错误传播" class="headerlink" title="贪心解码导致的错误传播"></a>贪心解码导致的错误传播</h2><p>这个问题在基于贪心的方法中基本都存在，也就是所谓的一步错步步错，这里主要提出了两种解决方法。</p><p><strong>辅助任务</strong> 辅助任务主要就是用来帮助主任务学习到一些不太容易学到的信息。这里才用了两个辅助任务，一个是在预测$n_t$的同时再预测一个$n_{t+1}$，这样就能往后多预测一步，适当的减少了贪心的影响。另一个方法就是将之前博客写到的句法距离（syntactic distances）加入到模型中一起预测：<br><img src="3.jpg" alt><br>对于不同的辅助任务，最后将他们的损失求和加到最终的损失函数中去：<br>\[<br>\mathcal{L} = \mathcal{L}_n + \mathcal{L}_c + \mathcal{L}_u + \beta \sum_a \mathcal{L}_a<br>\]</p><p><strong>策略梯度</strong> 这个方法可以从全局的角度来对模型进行优化。假设模型在$t$时刻的状态为$s_t$，输出标签为$l_t = (n_t, c_t, u_t)$，那么模型选择$l_t$的概率定义为策略$\pi$，模型最终可以获得的奖励为$\mathcal R_{tree}$，定义为句法树的F1值。</p><p>定义句法树的概率为每一步决策的概率之积：<br>\[<br>p(tree) = \prod\limits_{t = 1}^T {\pi ({l_t}|{s_t};\theta )}<br>\]<br>所以模型最终就是要最大化如下的奖励：<br>\[<br>\mathcal R = \sum\limits_{tree} { {\mathcal R_{tree}}p(tree)}<br>\]<br>按照梯度上升的方向更新参数$\theta$，求梯度可得：<br>\[<br>\begin{array}{l}\Delta \mathcal R = \sum\limits_{tree} { {\mathcal R_{tree}}\Delta p(tree)} \\ = \sum\limits_{tree} {p(tree){\mathcal R_{tree}}\frac{ {\Delta p(tree)}}{ {p(tree)}}} \\ = \sum\limits_{tree} {p(tree){\mathcal R_{tree}}\Delta \log p(tree)} \\ = {\mathbb{E}_{tree \sim p}}({\mathcal R_{tree}}\Delta \log p(tree))\end{array}<br>\]<br>将$p(tree)$代入可得：<br>\[<br>\begin{array}{l}\Delta \mathcal R = {\mathbb{E}_{tree \sim p}}({\mathcal R_{tree}}\Delta \log p(tree))\\ = {\mathbb{E}_{tree \sim p}}({\mathcal R_{tree}}\Delta \log \prod\limits_{t = 1}^T {\pi ({l_t}|{s_t};\theta )} )\\ = {\mathbb{E}_{tree \sim p}}(\sum\limits_{t = 1}^T { {\mathcal R_{tree}}\Delta \log \pi ({l_t}|{s_t};\theta )} )\\ \approx \frac{1}{N}\sum\limits_{i = 1}^N {\sum\limits_{t = 1}^T {\mathcal R_{i}\Delta \log \pi ({l_t}|{s_t};\theta )} } \end{array}<br>\]<br>其中$\mathcal R_{i}$是根据分布$p$采样出来的$N$棵句法树的奖励。</p><p>具体实现的时候有好几个小Tips。</p><p>第一个就是要将奖励减去一个baseline，这里定义为模型直接根据贪心求得的句法树的F1值：<br>\[<br>\Delta \mathcal R \approx \frac{1}{N}\sum\limits_{i = 1}^N {\sum\limits_{t = 1}^T {\Delta \log \pi ({l_t}|{s_t};\theta )({\mathcal R_i} - {B_i})} }<br>\]<br>这么做的目的就是为了让奖励有正有负，不然全部都是正数的话，因为采样不可能全部采样到，可能会导致高概率的样本概率越来越高，而没有采样到的低概率样本可能奖励非常高，却因此概率越来越低。</p><p>第二个Tip就是加入熵作为正则项：<br>\[<br>\Delta \mathcal R \approx \frac{1}{N}\sum\limits_{i = 1}^N {\sum\limits_{t = 1}^T {\Delta \log \pi ({l_t}|{s_t};\theta )({\mathcal R_i} - {B_i}) + \beta \Delta H(\pi ({l_t}|{s_t};\theta ))} }<br>\]<br>目的就是使概率尽量不要太小，不然的话采样数不够的话就有可能造成采样不到小概率的样本。</p><p>还有就是给策略加入噪声：<br>\[<br>\Delta \mathcal R \approx \frac{1}{N}\sum\limits_{i = 1}^N {\sum\limits_{t = 1}^T {\Delta (\log \pi ({l_t}|{s_t};\theta ) + N)({\mathcal R_i} - {B_i}) + \beta \Delta H(\pi ({l_t}|{s_t};\theta ) + N)} }<br>\]<br>目的同样是加大概率，防止概率太接近于0，当然这个可加可不加。。。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><hr><p>首先测试了不同设置的影响：<br><img src="4.jpg" alt><br>可以看出上面提到的几种方法对性能都有提升，其中采用动态编码、多任务（也就是减少输出空间）、辅助任务（主要是预测前一个$n_{t-1}$）还有策略梯度可以获得最好的结果。</p><p>最终模型在测试集上取得了90.6的F1值，虽然不是很高，但比之前的序列标注模型提升还是不少。<br><img src="5.jpg" alt></p><p>最后再来看一下模型在负数预测上的准确率，可以看出有了非常大的提升：<br><img src="6.jpg" alt></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>这篇论文提出了不少的小Tips来提升序列模型的准确率，但是效果却还是远远低于syntactic distances那篇论文（F1值91.8），具体原因我也不得而知，我猜测跟树到序列映射编码关系可能不是特别大，可能还是跟序列建模有关，那篇论文的序列采用了两次LSTM，中间还夹杂了一次CNN卷积操作。所以编码器的好坏还是直接决定了最后性能的好坏，怪不得Elmo和Bert的效果那么的突出。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
            <tag> NAACL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Constituent Parsing as Sequence Labeling</title>
      <link href="/2019/03/11/conparseqlab/"/>
      <url>/2019/03/11/conparseqlab/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=421885447&auto=1&height=66"></iframe></div><blockquote><p>貌似已经有好几个月没怎么看过论文了，之前一直在写论文，一直没空更新博客，最近闲下来把最后几篇没看完的论文看了。</p></blockquote><p><strong>论文地址：</strong><a href="http://aclweb.org/anthology/D18-1162" target="_blank" rel="noopener">Constituent Parsing as Sequence Labeling</a></p><p><strong>代码地址：</strong><a href="https://github.com/aghie/tree2labels" target="_blank" rel="noopener">Code</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>本文定义了一种新的树的序列化方法，将树结构预测问题转化为了序列预测问题。该序列用相邻两个结点的<strong>公共祖先（CA）数量</strong>和<strong>最近公共祖先（LCA）的label</strong>来表示一棵树，并且证明了这个树到序列的映射是单射但不是满射的，但是提出了一系列方法来解决这个问题。</p><p>相比于之前的序列方法，比如<a href="http://www.aclweb.org/anthology/D16-1257" target="_blank" rel="noopener">Parsing as Language Modeling</a>，本文的序列化有所不同，主要体现在之前的方法都是seq2seq的，也就是输入句子，直接输出树的括号表达式序列。但是这种方法输出不是定长的，所以结果可能会比较差。本文的方法将输出长度固定在了句子长度减1上（只针对不存在一元产生式的句法树，这种情况之后讨论），所以可以将每个预测分配到每个单词上，然后用序列标注的方法来解决。</p><h1 id="树的序列化"><a href="#树的序列化" class="headerlink" title="树的序列化"></a>树的序列化</h1><hr><h2 id="记号和基础知识"><a href="#记号和基础知识" class="headerlink" title="记号和基础知识"></a>记号和基础知识</h2><p>记输入句子为$\textbf{w} = [w_1, w_2, \ldots, w_N]$，其中$w_i \in V$。$T_N$为拥有$N$个叶子结点的不含有一元产生式的句法树集合。句法分析的任务就是将输入句子$\textbf{w}$映射到句法树$T_N$。</p><p>为了将句法分析转化为序列标注任务，需要定义一个树的序列化方法：$\Phi_N : T_N \to L^{N - 1}$，也就是将一棵有$N$个叶子结点的句法树转化为长度为$N - 1$的序列。并且该映射函数还得满足一定的条件，首先它一定得是一个<strong>函数</strong>（<em>也就是对于所有的句法树，都得找到一个对应的序列</em>），然后这个函数还得有<strong>单射性</strong>（<em>也就是句法树和序列要一一对应，不能存在两个句法树对应同一个序列，否则的话预测出来一个序列可能解码出两棵句法树，那就尴尬了</em>），当然要是还满足<strong>满射性</strong>就最好了（<em>也就是对于每一个序列，最好都能找到一棵句法树与之对应，不然预测出一个序列无法找到对应的句法树也很尴尬</em>），当然找不到也没事，后文有解决方法。</p><p>然后需要定义一个函数，将句子映射为序列：$F_{N, \theta} : V^N \to L^{N - 1}$。这个映射就通过序列标注的LSTM来实现了，$\theta$就是LSTM的参数。</p><p>最后通过函数$F_{N, \theta} \circ \Phi_N^{-1}$将输入句子转化为对应的句法树。那么$F_{N, \theta}$没什么好说的，就是一个序列标注模型，下面重点就是介绍如何设计函数$\Phi_N$。</p><h2 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h2><p>之前说到了将一棵有$N$个叶子结点的句法树转化为长度为$N - 1$的序列，这个序列是这样生成的：对于单词$w_i$，分配给它一个二元label $l_i = (n_i, c_i)$，其中$n_i$为单词$w_i$和$w_{i + 1}$的CA数量，$c_i$为它俩的LCA的label。</p><p><img src="1.jpg" alt></p><p>如上图所示，这个序列的$n_i$有两种表示方法。一种就表示成CA的绝对数量，如图中第一行所示。还有一种表示成后一个数与前一个数的差值，这样能减少元组的数量，但是会出现负数。当然在这个例子中貌似并不能看出数量减少了。。。</p><p><strong>$k$叉树编码：</strong>如果句法树所有产生式全部是$k$叉的，那么还可以将编码进一步简化，具体做法就是将所有的负数$n_i$统一为一个负数就行。为什么这里就不需要对负数进行区分了呢？这还得从句法树的解码说起，我们看一看是怎么从序列解码成句法树的。</p><p>当遇到一个负数$n_i$的时候，说明${\rm LCA}(w_{i}, w_{i+1})$到根结点路径的长度比${\rm LCA}(w_{i-1}, w_i)$到根结点路径长度少$-n_i$个结点。大致结构如下图所示（图画的丑，不要介意）：</p><p><img src="2.jpg" alt></p><p>可以看出，$w_{i+1}$这棵子树接在了从$w_i$到根结点路径上的第$-n_i + 2$个结点上。但是$w_{i+1}$具体在哪还无法确定，只能确定它的子树根结点位置。另外需要解释的是，为什么这里是常数2？因为$w_i$到$w_{i-1}$与$w_i$的LCA的距离一定是2，如果不是的话，中间就一定会有其他结点，那么就一定存在结点位于$w_{i-1}$与$w_i$之间，这显然不可能。最后可以注意到，这种情况下，</p><p>如果$n_i$是正数的话，说明${\rm LCA}(w_{i}, w_{i+1})$到根结点路径的长度比${\rm LCA}(w_{i-1}, w_i)$到根结点路径长度多$n_i$个结点。大致结构如下图所示：</p><p><img src="3.jpg" alt></p><p>这种情况下，$w_{i+1}$这棵子树接在了从${\rm LCA}(w_{i-1}, w_i)$到$w_i$路径上的第$n_i + 1$个结点处。同样也无法确定它的准确位置，但是它所在的子树确定了从这分叉出去的。</p><p>回到正题，之前说到了对于$k$叉树，所有负数都可以统一起来，为什么呢？继续看上面$n_i$负数那张图，对于$w_{i+1}$所在子树，需要在从$w_i$到根结点这条路径上寻找一个分叉点，也就是它俩的LCA。如果这是一个$k$叉树，那么这个分叉点就一定是第一个孩子数不满$k$个的结点。因为如果再往下的话，孩子数都满了，再加子树孩子数一定大于$k$。再往上的话，就会导致这第一个结点孩子数小于$k$，因为从左到右遍历的，子树之间不会交叉，以后都不会有子树插入到这个结点处了。</p><p>下图就是简化序列化后的二叉树例子，第三行将所有的负数都用一个负号替代了：</p><p><img src="4.jpg" alt></p><p>我尝试过了按照这个序列构建出一棵树的过程，画了个草图给大家看看，可能有点乱（参照的是上面那个非二叉树的图）：<br><img src="5.jpg" alt></p><p>还有一个小trick就是对于有些直接连到根结点的叶子，用$({\rm ROOT}, c_i)$作为它们的label。</p><h2 id="理论证明"><a href="#理论证明" class="headerlink" title="理论证明"></a>理论证明</h2><p>主要证明两个性质，一个就是充分性（即每个句法树都能映射为一个序列），另一个就是单射性（即每个序列只能唯一对应一个句法树）。</p><p><strong>充分性：</strong><br>这个显而易见，对于每个句法树，相邻两个单词一定存在唯一的LCA，且它的label也是唯一的，所以充分性肯定能保证的。</p><p><strong>单射性：</strong><br>为了简便，首先证明不包含非终结符的树结构映射的单射性，再证明加上非终结符也是单射的。</p><p>如果用$(\bullet_i)$表示第$i$个叶子结点，那么句法树可以表示成如下的括号表达式：<br>\[<br>\alpha_0 (\bullet_1) \alpha_1 (\bullet_2) \ldots \alpha_{\left| w \right| - 1} (\bullet_{\left| w \right|}) \alpha_{\left| w \right|}<br>\]<br>更进一步，每个$\alpha_i$形式肯定是$[)]^*[(X]^*$，因为如果存在一个闭合的括号对，那么中间肯定还存在着一个叶子结点，这显然不可能。所以我们可以用$\alpha_{i)}$来替代$[)]^*$，用$\alpha_{i(}$来替代$[(X]^*$，将$\alpha_i$改写为$\alpha_{i)} \alpha_{i(}$，括号表达式可以重写为：<br>\[<br>\alpha_{0)} \alpha_{0(} (\bullet_1) \alpha_{1)} \alpha_{1(} (\bullet_2) \ldots (\bullet_{\left| w \right|}) \alpha_{\left| w \right|)} \alpha_{\left| w \right|(}<br>\]<br>注意到首尾两个元素一定是空的，接下来用$\beta_i$替换$\alpha_{i-1(}(\bullet_i)\alpha_{i)}$，得到序列：<br>\[<br>\beta_1 \beta_2 \ldots \beta_{\left| w \right|}<br>\]<br>更进一步，可以证明$\beta_i$一定只含有$[(X]^*(\bullet_i)$和$(\bullet_i)[)]^*$中的一个。因为如果两个都含有的话，说明存在$(X(\bullet_i))$这种一元产生式，但是因为一元产生式都提前处理过了，所以不可能存在。</p><p>接下来可以给每个$\beta_i$分配一个值$\delta(\beta_i)$，如果$\beta_i$左右两边都没有括号，那这个值就是0，如果左边有$k$个括号，那值就是$+k$，如果右边有$k$个括号，那值就是$-k$。如果将这些值写成序列：<br>\[<br>\delta(\beta_1)\delta(\beta_2)\ldots\delta(\beta_{\left|w\right| - 1})<br>\]<br>这个序列正好对应了之前的第二种编码，也就是编码成LCA的个数之差。这是为什么呢？可以看出，一直到$\beta_i$结束，没有闭合的括号数量正好就是$w_i$和$w_{i+1}$的LCA数量。所以$\delta(\beta_i)$就是$w_i$和$w_{i+1}$的LCA数量与$w_{i-1}$和$w_{i}$的LCA数量的差值。</p><p>最后这就验证了括号序列和之前的编码是一一对应的，单射性得证。解码的时候只需要将数字直接转化成对应的括号序列就行了。</p><p>而加上了非终结符之后，单射性不会受到影响。因为虽然两棵相同结构但是拥有不同非终结符的句法树，转化成括号序列后是相同的。但是因为之前的定义中，还有一个变量$c_i$来表示这个非终结符了，所以还是能够唯一对应过去的。</p><h2 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h2><p>上面定义的序列化函数有两个缺点：一是非满射，二是不能处理一元产生式，下面介绍一下解决方法。</p><p><strong>对于一元产生式：</strong><br>有两种一元产生式，一种是中间结点，还有一种是叶子结点的label。</p><p>对于中间结点，直接将一条链上的label合并成一个新的label就行了，方法和之前文章介绍的一样。</p><p>而对于叶子结点的label，一个方法是在解码之前先用一个函数预测一下每个叶子结点的label，如果为空，说明没有label，否则就加上这个label，然后再进行正常的解码。另一个方法是将之前的序列化的二元组扩展为三元组$(n_i, c_i, u_i)$，其中第三个元素就是每个叶子结点的label。</p><p><strong>非满射：</strong><br>非满射会导致的问题就是产生出来的序列可能无法映射到某一棵句法树。根据文中所说，一共有两种无法映射的情况。</p><p>一种情况是对于多叉树，相邻两对叶子结点的LCA的label预测不同。比如在最上面一张图中，“the red toy”如果预测为两个不同的label，那么就会产生矛盾。这种情况很好解决，只要在解码的时候只取第一个label，忽略后一个就行了。</p><p>另一种情况是序列可能会产生一元产生式，如下图所示：</p><p><img src="6.jpg" alt></p><p>根据图中序列，会产生下面那棵句法树，一元结点X并没有预测到。但其实因为一元结点已经提前合并了，所以如果预测到了一元结点，直接删掉不要就行了。</p><h1 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h1><hr><p>这里就不细讲了，用的就是基本的BiLSTM + CRF序列标注模型，具体可以看这篇论文：<a href="https://arxiv.org/abs/1603.01354" target="_blank" rel="noopener">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</a>。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><p>这篇论文最大的卖点不是效果，而是速度快，下面是和其他模型的速度对比，可以看出，速度的确快了不少，达到了大几百句每秒。但是还是存在序列标注模型的老毛病，效果并不好，虽然比之前的高了，但是还是只有90%的F1。</p><p><img src="7.jpg" alt></p><h1 id="结论与展望"><a href="#结论与展望" class="headerlink" title="结论与展望"></a>结论与展望</h1><hr><p>这篇论文定义了一种新的句法树序列化方法，将句法树序列化为长度减1的序列，其中每个值就是相邻两个单词的CA个数和LCA的label。</p><p>看完这篇，我仔细想了想，其实之前的chart-based方法也都可以转化成序列，只不过都得特别处理一下一元产生式和多叉树，比较麻烦。以后可以考虑在这方面有所突破，速度快还是很nice的。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
            <tag> EMNLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex撰写论文常用技巧总结</title>
      <link href="/2019/01/13/latex-pdf/"/>
      <url>/2019/01/13/latex-pdf/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=28636660&auto=1&height=66"></iframe></div><p>这篇博文专门用来记录Latex写论文过程中遇到的一些技巧与心得。</p><h1 id="插入矢量图片"><a href="#插入矢量图片" class="headerlink" title="插入矢量图片"></a>插入矢量图片</h1><hr><p>首先在Office PowerPoint中画好模型图，然后有两种方法在Latex中插入矢量图。<br><img src="1.jpg" alt><br><strong>方法一：</strong><br>直接另存为pdf，例如存为<code>figure.pdf</code>。<br><img src="2.jpg" alt><br>注意到pdf打开来左右两个侧边栏有较大的空余空间，所以最好在ppt中绘制模型图的时候就调整好。<br>然后在Latex中使用如下代码插入pdf图片即可：</p><pre class=" language-latex"><code class="language-latex"><span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">figure*</span><span class="token punctuation">}</span><span class="token punctuation">[</span>htbp<span class="token punctuation">]</span>    <span class="token function selector">\centering</span>    <span class="token function selector">\includegraphics</span><span class="token punctuation">[</span>width=<span class="token function selector">\textwidth</span><span class="token punctuation">]</span><span class="token punctuation">{</span>figure.pdf<span class="token punctuation">}</span>    <span class="token function selector">\caption</span><span class="token punctuation">{</span> Model. <span class="token punctuation">}</span>    <span class="token function selector">\label</span><span class="token punctuation">{</span><span class="token keyword">fig::model</span><span class="token punctuation">}</span><span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">figure*</span><span class="token punctuation">}</span></code></pre><p>其中参数<code>width=\textwidth</code>是用来调整图片宽度，使得图片占满整个论文，效果如下：<br><img src="3.jpg" alt><br>注意到这里左右两个侧边栏间距的确有点大了，没有占满整个页面。<br>如果想要图片只显示在一半的页面上，那么只需要用如下代码即可：</p><pre class=" language-latex"><code class="language-latex"><span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">figure</span><span class="token punctuation">}</span><span class="token punctuation">[</span>htbp<span class="token punctuation">]</span>    <span class="token function selector">\centering</span>    <span class="token function selector">\includegraphics</span><span class="token punctuation">[</span>width=<span class="token function selector">\linewidth</span><span class="token punctuation">]</span><span class="token punctuation">{</span>figure.pdf<span class="token punctuation">}</span>    <span class="token function selector">\caption</span><span class="token punctuation">{</span> Model. <span class="token punctuation">}</span>    <span class="token function selector">\label</span><span class="token punctuation">{</span><span class="token keyword">fig::model</span><span class="token punctuation">}</span><span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">figure</span><span class="token punctuation">}</span></code></pre><p>效果如下：<br><img src="4.jpg" alt></p><p><strong>2019.1.14更新：</strong><br>关于生成出来的模型图pdf侧边距过大的问题，可以下载“迅捷pdf编辑器”，使用它强大的页面裁剪功能。当然非会员会留下水印，只需要修改水印透明度为0即可。</p><p><strong>方法二：</strong><br>ppt绘制好的模型图右键另存为<code>emf</code>格式，这是一种Windows的矢量图格式，然后下载一款软件叫<code>Metafile to EPS Converter</code><a href="http://wiki.lyx.org/uploads/Windows/metafile2eps/metafile2eps.exe" target="_blank" rel="noopener">地址</a>，将图片转为Latex支持的<code>eps</code>格式，例如命名为<code>figure.eps</code>。</p><p>最后在Latex使用相同的代码插入图片即可，效果如下：<br><img src="5.jpg" alt><br>可以看出两侧间距比原来小了很多。</p><p><strong>2019.3.6更新：</strong></p><h1 id="中文支持"><a href="#中文支持" class="headerlink" title="中文支持"></a>中文支持</h1><hr><pre class=" language-latex"><code class="language-latex"><span class="token function selector">\usepackage</span><span class="token punctuation">{</span><span class="token keyword">CJK</span><span class="token punctuation">}</span><span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">document</span><span class="token punctuation">}</span><span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">CJK*</span><span class="token punctuation">}</span><span class="token punctuation">{</span>GBK<span class="token punctuation">}</span><span class="token punctuation">{</span>song<span class="token punctuation">}</span><span class="token comment" spellcheck="true">% 正文</span><span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">CJK*</span><span class="token punctuation">}</span><span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">document</span><span class="token punctuation">}</span></code></pre><h1 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h1><hr><p><strong>单幅图片：</strong></p><pre class=" language-latex"><code class="language-latex"><span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">figure</span><span class="token punctuation">}</span><span class="token punctuation">[</span>htbp<span class="token punctuation">]</span>    <span class="token function selector">\centering</span>    <span class="token function selector">\includegraphics</span><span class="token punctuation">[</span>width=<span class="token function selector">\linewidth</span><span class="token punctuation">]</span><span class="token punctuation">{</span>figure.pdf<span class="token punctuation">}</span>    <span class="token function selector">\caption</span><span class="token punctuation">{</span>xxxxx.<span class="token punctuation">}</span>    <span class="token function selector">\label</span><span class="token punctuation">{</span><span class="token keyword">Fig:xxxxx</span><span class="token punctuation">}</span><span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">figure</span><span class="token punctuation">}</span></code></pre><p>跨双栏的话把<code>figure</code>改成<code>figure*</code>，<code>htbp</code>控制位置，自己看着调。</p><p>难点是跨双栏图片置顶，一般情况下会自动跑到下一页去，找了半天才找到解决方法：</p><pre class=" language-latex"><code class="language-latex"><span class="token function selector">\twocolumn</span><span class="token punctuation">[</span><span class="token punctuation">{</span>    <span class="token function selector">\renewcommand</span><span class="token function selector">\twocolumn</span><span class="token punctuation">[</span>1<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">{</span>#1<span class="token punctuation">}</span>    <span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">center</span><span class="token punctuation">}</span>        <span class="token function selector">\centering</span>        <span class="token function selector">\includegraphics</span><span class="token punctuation">[</span>width=<span class="token function selector">\textwidth</span><span class="token punctuation">]</span><span class="token punctuation">{</span>figure.pdf<span class="token punctuation">}</span>        <span class="token function selector">\captionof</span><span class="token punctuation">{</span>xxxxx.<span class="token punctuation">}</span>    <span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">center</span><span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">]</span></code></pre><p>缺点是无法添加<code>label</code>，正文只能手动加引用了。</p><p><strong>两幅图片同一行显示：</strong></p><pre class=" language-latex"><code class="language-latex"><span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">figure*</span><span class="token punctuation">}</span><span class="token punctuation">[</span>htbp<span class="token punctuation">]</span>    <span class="token function selector">\centering</span>    <span class="token function selector">\subfigure</span><span class="token punctuation">[</span>fig1.<span class="token punctuation">]</span><span class="token punctuation">{</span>        <span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">minipage</span><span class="token punctuation">}</span><span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">{</span>0.58<span class="token function selector">\textwidth</span><span class="token punctuation">}</span>        <span class="token function selector">\includegraphics</span><span class="token punctuation">[</span>width=<span class="token function selector">\textwidth</span><span class="token punctuation">]</span><span class="token punctuation">{</span>figure1.pdf<span class="token punctuation">}</span>        <span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">minipage</span><span class="token punctuation">}</span>        <span class="token function selector">\label</span><span class="token punctuation">{</span><span class="token keyword">Fig:fig1a</span><span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token function selector">\subfigure</span><span class="token punctuation">[</span>fig2.<span class="token punctuation">]</span><span class="token punctuation">{</span>        <span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">minipage</span><span class="token punctuation">}</span><span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">{</span>0.34<span class="token function selector">\textwidth</span><span class="token punctuation">}</span>        <span class="token function selector">\includegraphics</span><span class="token punctuation">[</span>width=<span class="token function selector">\textwidth</span><span class="token punctuation">]</span><span class="token punctuation">{</span>figure2.pdf<span class="token punctuation">}</span>        <span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">minipage</span><span class="token punctuation">}</span>        <span class="token function selector">\label</span><span class="token punctuation">{</span><span class="token keyword">Fig:fig1b</span><span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token function selector">\caption</span><span class="token punctuation">{</span>xxxxx.<span class="token punctuation">}</span>    <span class="token function selector">\label</span><span class="token punctuation">{</span><span class="token keyword">Fig:fig1</span><span class="token punctuation">}</span><span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">figure*</span><span class="token punctuation">}</span></code></pre><p>注意的是，两个图片宽度之和<code>0.58 + 0.34 = 0.92</code>要尽量小于1，不然会显示出问题。</p><h1 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h1><hr><pre class=" language-latex"><code class="language-latex"><span class="token function selector">\renewcommand</span><span class="token punctuation">{</span><span class="token function selector">\algorithmicrequire</span><span class="token punctuation">}</span><span class="token punctuation">{</span> <span class="token function selector">\textbf</span><span class="token punctuation">{</span>Input:<span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token function selector">\renewcommand</span><span class="token punctuation">{</span><span class="token function selector">\algorithmicensure</span><span class="token punctuation">}</span><span class="token punctuation">{</span> <span class="token function selector">\textbf</span><span class="token punctuation">{</span>Output:<span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">algorithm</span><span class="token punctuation">}</span><span class="token punctuation">[</span>t<span class="token punctuation">]</span>    <span class="token function selector">\caption</span><span class="token punctuation">{</span> alg1. <span class="token punctuation">}</span><span class="token function selector">\label</span><span class="token punctuation">{</span><span class="token keyword">Alg:Alg1</span><span class="token punctuation">}</span>    <span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">algorithmic</span><span class="token punctuation">}</span><span class="token punctuation">[</span>1<span class="token punctuation">]</span>        <span class="token function selector">\Require</span>            <span class="token comment" spellcheck="true">% 输入</span>        <span class="token function selector">\Ensure</span>            <span class="token comment" spellcheck="true">% 输出</span>        <span class="token comment" spellcheck="true">% 过程</span>        <span class="token function selector">\Function</span> <span class="token punctuation">{</span>xxxxxx<span class="token punctuation">}</span><span class="token punctuation">{</span><span class="token equation string">$i, j$</span><span class="token punctuation">}</span>            <span class="token comment" spellcheck="true">% 函数体</span>        <span class="token function selector">\EndFunction</span>    <span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">algorithmic</span><span class="token punctuation">}</span><span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">algorithm</span><span class="token punctuation">}</span></code></pre><h1 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h1><hr><pre class=" language-latex"><code class="language-latex"><span class="token function selector">\newcommand</span><span class="token punctuation">{</span><span class="token function selector">\tabincell</span><span class="token punctuation">}</span><span class="token punctuation">[</span>2<span class="token punctuation">]</span><span class="token punctuation">{</span><span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">tabular</span><span class="token punctuation">}</span><span class="token punctuation">{</span>@<span class="token punctuation">{</span><span class="token punctuation">}</span>#1@<span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">}</span>#2<span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">tabular</span><span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">table</span><span class="token punctuation">}</span><span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token function selector">\normalsize</span><span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">center</span><span class="token punctuation">}</span><span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">tabular</span><span class="token punctuation">}</span><span class="token punctuation">{</span>l|l|ccc<span class="token punctuation">}</span><span class="token function selector">\hline</span><span class="token function selector">\multicolumn</span><span class="token punctuation">{</span>2<span class="token punctuation">}</span><span class="token punctuation">{</span>c|<span class="token punctuation">}</span><span class="token punctuation">{</span>Model<span class="token punctuation">}</span> <span class="token punctuation">&amp;</span> LR <span class="token punctuation">&amp;</span> LP <span class="token punctuation">&amp;</span> F1<span class="token function selector">\\</span><span class="token function selector">\hline</span><span class="token function selector">\hline</span><span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">tabular</span><span class="token punctuation">}</span><span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">center</span><span class="token punctuation">}</span><span class="token function selector">\caption</span><span class="token punctuation">{</span> xxxxx.<span class="token punctuation">}</span><span class="token function selector">\label</span><span class="token punctuation">{</span><span class="token keyword">Tab:CompDiffConfig</span><span class="token punctuation">}</span><span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">table</span><span class="token punctuation">}</span></code></pre><p>两个难点，一个是合并同一行的单元格，用<code>\multicolumn{cols}{pos}{text}</code>。</p><p>一个是合并同一列的单元格，用：</p><pre class=" language-latex"><code class="language-latex"><span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">table</span><span class="token punctuation">}</span>    <span class="token function selector">\centering</span>    <span class="token function selector">\begin</span><span class="token punctuation">{</span><span class="token keyword">tabular</span><span class="token punctuation">}</span><span class="token punctuation">{</span>|c|c|c|c|<span class="token punctuation">}</span>        <span class="token function selector">\hline</span>        <span class="token function selector">\multirow</span><span class="token punctuation">{</span>2<span class="token punctuation">}</span>*<span class="token punctuation">{</span>合并两行一列<span class="token punctuation">}</span> <span class="token punctuation">&amp;</span> 二 <span class="token punctuation">&amp;</span> 三 <span class="token punctuation">&amp;</span> 四 <span class="token function selector">\\</span>        ~ <span class="token punctuation">&amp;</span> 2 <span class="token punctuation">&amp;</span> 3 <span class="token punctuation">&amp;</span> 4 <span class="token function selector">\\</span>        <span class="token function selector">\hline</span>    <span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">tabular</span><span class="token punctuation">}</span><span class="token function selector">\end</span><span class="token punctuation">{</span><span class="token keyword">table</span><span class="token punctuation">}</span></code></pre><p>注意第二行第一列要用<code>~</code>补上空位。用<code>\cline{start-end}</code>来代替<code>\hline</code>划线。</p><p><strong>暂时就想到这些了，等想到了再更吧，祝我paper顺利。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 软件安装与配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LaTeX </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二零一八年终总结</title>
      <link href="/2018/12/31/2018-conclusion/"/>
      <url>/2018/12/31/2018-conclusion/</url>
      
        <content type="html"><![CDATA[<blockquote><p>某天，你无端想起了一个人，她曾让你对明天有所期许，但是却完全没有出现在你的明天里。</p></blockquote><p>2017年好像还历历在目，2018年却要过去了，貌似这一年并没有发生什么特别值得纪念的事情吧，平平淡淡，淡的就像一杯白开水。</p><p>细数这一年发生的事，前半年写毕业论文，送走了一个又一个熟悉的面孔，略有感伤；后半年荒废，偶尔看看论文偶尔写两行代码，仍然时常因为一些无关紧要的事情而感伤。</p><ul><li>毕业</li><li>炼丹</li><li>想开</li></ul><h1 id="毕业季"><a href="#毕业季" class="headerlink" title="毕业季"></a>毕业季</h1><hr><p>整个上半年差不多都在忙着读论文、改代码、写毕设吧。虽然当时的眼光有限，看的论文还都是几年前的传统方法，例如CKY算法，模型也基本都是基于Grammar的，在现在看来效率实在是低下，效果也不尽人意。幸好论文写得马马虎虎，也算是萌混过关吧QAQ，最后也混了个并没有什么卵用的优秀毕业论文。</p><p>毕业论文封面：<br><img src="1.jpg" alt></p><p>忙完了毕业论文，接下来就是毕业典礼了吧，也是个忧伤的时期吧。还记得毕业典礼那天大家一起去了闵行，自从两年前搬来中北后，就再也没有回去过了吧。闵行的校园，闭着眼睛也能知道走到哪了吧，毕竟曾经都一起走过，到处都留下了回忆。那天天气非常的好，又把校园都走了一遍吧，只是没有去曾经的三舍看看，略有遗憾，当年去四舍帮忙搬家的情景又浮现在了眼前。也许是回忆过多，没有久留，一伙曾经玩的最好的朋友们一起在宝龙广场吃了饭唱了歌，就当做是散伙饭了吧。</p><p>之后就是暑假了吧，有人升学到了其他学校，有人开始了工作生涯，也有人继续和我一样留在了本校，继续做了室友。特别值得一提的是永日至秦吧，那个打游戏打的我心态爆炸的选手。暑假每天一起半夜出来跑了最后的步，还一起骑着单车骑到了外滩，再上一次去那看夜景还是不知几年前和她去的了吧，换了个人去，别有一番滋味。现在永日至秦也一个人去了北京，不知何时再能见到，希望下次见到李院士的时候你不是一个人（此处貌似有歧义）。</p><p>最后附上孙子临走前的美照一张，有意向勾搭的找我要联系方式：<br><img src="2.jpg" alt></p><h1 id="炼丹生涯开始"><a href="#炼丹生涯开始" class="headerlink" title="炼丹生涯开始"></a>炼丹生涯开始</h1><hr><p>送走了所有人之后，暑假搬了宿舍，就正式开始了研究僧的炼丹生涯（划水混毕业）了吧。一整个暑假都在看论文，补基础，也差不多看完了成分句法分析近几年来的所有深度学习方法了吧，基本对此有了一个初步的认识。还开通了<a href="https://zhuanlan.zhihu.com/godweiyang" target="_blank" rel="noopener">知乎专栏：自然语言处理与深度学习</a>，经营到现在也才900不到的粉丝吧，希望有一天能像大佬们那样几万的粉丝，在这个领域做出自己的贡献吧。通过看论文，写博客，也认识了许多志同道合的人吧，还和大佬们有所交流。</p><p><img src="4.jpg" alt></p><p>其实写博客也是很累的，从刚开始计划的paper daily到后来的paper weekly，再到现在的paper randomly。有时候一篇论文真的要精读好久才能读懂，有时候实在读不懂了，就直接开写，在写的过程中，逐渐的就把自己说服懂了。其实这也和讲presentation一样，我也是比较喜欢做presentation的，一个原因是我确实喜欢和别人分享各种好的paper，好的思想，另一个原因是在讲的过程中，有些我一直不懂的问题可能讲着讲着我就懂了，也可以和台下的老师同学们自由讨论，理解得更加透彻。</p><p>可能会有人问我，你写了没人看，哪里有成就感？确实，刚开始时只有寥寥无几的老同学们来捧个场子，而且他们也都看不懂，只是点个赞。但是如果只是为了出名，那就不要写博客了吧，写博客是主要是为了提升自己，记录自己的点点滴滴，然后是和别人分享，同时自己也能获得进步。至于成就感的话，慢慢就有了，最起码每次写完一篇博客，看着整理的满满当当的博文，自己就会有一种成就感，不需要别人来证明。现在每天看着知乎多了新粉丝，偶尔会有新评论和私信，询问学术方面的问题，还是会有小小的欣慰的。</p><p>现在论文看的不如以前多了，看来还是得去补补基础，或者去看看其他方面的论文，涉猎广一点，有些东西是通用的，可以拿来为之所用。代码的话之前写的也不多，一直都是面向过程，写的代码都不堪入目。直到开学才拿了伯克利的优质代码，重构了一下，整合了比较流行的几种模型，在上面进行了修改，最近也一直在跑。虽然现在没有对象，但是面向对象的思想还是要有的，不然以后代码维护起来都很困难的。</p><p>开学了课虽然不多，但是事情还是挺繁杂的吧。周二要给大三的操作系统实践做助教，每周都要改作业，还是挺麻烦的，混学分不容易。周四一天满课，知识分析课每周都要抄作业度日，其他几门课也都没怎么听，现在担忧期末考试怎么办，本科认真学习的劲头不知道都去哪里了，可能现在只要不挂科就行了吧，心态变了。而且现在想找个能一起学习的朋友都很难了。</p><h1 id="庸人自扰"><a href="#庸人自扰" class="headerlink" title="庸人自扰"></a>庸人自扰</h1><hr><p>并没有遇到什么在意的人吧，就说说我一直以来的心路历程吧。</p><p>一个人好过歹过也已经有一年半多了，在旁人看来这么久了，也早该忘得一干二净了，但是毕竟这是初恋吧，被义务教育压榨了这么多年后的第一次解脱，很难忘记。而且我也不是一个很随便的人，没那么容易忘。直到一个多月前吧，她终于遇到了对她真正好的人了，我想我也该彻底释怀了，知道消息时也没多大的情绪波动，一切都在意料之中，早晚会来临的。</p><p>可能会有人觉得我喜欢撩妹吧，学妹加了挺多。的确，是加了不少，都是每年开学水新生群加的吧，但绝大多数都安静地躺尸在了好友列表里。偶尔会有一些“慕名前来”问问题的人，以前我还能认真回答回答，毕竟我一直喜欢帮助别人，分享自己的经验。但是现在我改变了这些想法，二十三年了，第一次在这方面感到累了，也许自闭才是最好的解决方法吧，帮助别人到最后什么都得不到吧，还是照顾好自己。既然喜欢的人不可得，那就安心学习吧，对自己好才是最重要的。如果对任何人都这么有求必应，可能真的变成了小太阳了吧。</p><p>以前的我也跟小标题一样，时常庸人自扰，无病呻吟吧，可能是我想太多了吧，总是被一些无关痛痒的事影响，每天都是唉声叹气的。现在我想开了，真的没必要为这些琐事担忧烦心，放任它去吧。每当再遇到了困扰的事，出去散散心倒是最好的解决方式，虽然找不到最合适的人一起散心，但是能呼吸一下外面透心凉的空气，心里的结也能一下子解开了。见的多了自然而然就不会为一些事而感到郁郁寡欢了，从前每次遇到不顺心的事总会连着影响几天，饭也吃不好觉也睡不着，学习也没有任何的动力。现在想开了，努力练就一颗强大的内心吧，虽然铁石心肠不好，但是还是要坚强呀。你已经是个大人了，要学着自己控制情绪了，不要再做一个幼稚的小孩了，成熟点吧。<br><img src="3.jpg" alt></p><h1 id="2019畅想"><a href="#2019畅想" class="headerlink" title="2019畅想"></a>2019畅想</h1><hr><p>关于下一年，首先把课混过去，然后好好看论文敲代码做实验吧，但愿能有个不错的想法和实验效果，让我能在明年就安然毕业。至于发A什么的，就先想想吧，朝着这方面努力，希望我的微信名可以早日改掉。</p><p>情感方面，就准备单着三年吧，除非天上掉馅饼了，好好做学术才是正道。</p><p>多出去走走吧，本科四年都呆在学校了，三点一线式的类高中生活，谈的也是高中式恋爱吧，累了累了。</p><p>其他没啥好想的了，鄙人不善幻想，只喜欢晚上做梦，最后送上泰戈尔的一句名言吧，与君共勉：</p><blockquote><p>有一个夜晚我烧毁了所有的记忆，从此我的梦就透明了；<br>有一个早晨我扔掉了所有的昨天，从此我的脚步就轻盈了。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing</title>
      <link href="/2018/11/07/emnlp18-dynamic-oracle/"/>
      <url>/2018/11/07/emnlp18-dynamic-oracle/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=240141&auto=1&height=66"></iframe></div><blockquote><p>有一句话，宾语是你。“吉下两点一口，又有欠字相依。”</p></blockquote><p><strong>论文地址：</strong><a href="http://aclweb.org/anthology/D18-1161" target="_blank" rel="noopener">Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing</a><br><strong>代码地址：</strong><a href="https://github.com/danifg/Dynamic-InOrderParser" target="_blank" rel="noopener">github</a></p><p>本文是发表在EMNLP18上的一篇关于Dynamic Oracle的论文，主要介绍了针对自顶向下和中序两种移进归约成分句法分析模型的Dynamic Oracles。在PTB数据集上，取得了单模型最高的F1值92.0（截至论文发稿时是最高的，张岳TACL18的论文已经取得了92.4的最高F1值）。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>Dynamic Oracle是用在转移系统中，防止错误传播的一种手段。而转移系统主要有分为三种：bottom-up、top-down和in-order的转移系统。</p><p>其中bottom-up转移系统的Dynamic Oracle在<a href="http://aclweb.org/anthology/D16-1001" target="_blank" rel="noopener">Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles</a>中有很详细的证明，也可以参看我之前的博客<a href="https://godweiyang.com/2018/08/03/dynamic-oracles/">Deep Understanding of Dynamic Oracle in Constituent Parsing</a>。</p><p>而本文就提出了另外两种转移系统的Dynamic Oracle，其中top-down转移系统主要基于<a href="http://aclweb.org/anthology/N16-1024" target="_blank" rel="noopener">Recurrent Neural Network Grammars</a>，in-order转移系统主要基于<a href="http://aclweb.org/anthology/Q17-1029" target="_blank" rel="noopener">In-Order Transition-based Constituent Parsing</a>。</p><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><hr><h2 id="形式化定义"><a href="#形式化定义" class="headerlink" title="形式化定义"></a>形式化定义</h2><p>bottom-up的转移系统这里就不讨论了，这里主要讨论另外两种转移系统。转移系统的状态用五元组$c = \left\langle {\sum ,i,f,\gamma ,\alpha } \right\rangle $表示，五元组内元素分别表示stack、buffer第一个单词的下标、in-order转移系统中结束标记、已经生成的短语成分集合、stack中非终结符集合。</p><p>每个短语成分用三元组$(X, l, r)$表示，其中X是非终结符，l和r是短语的边界下标。而非终结符用二元组$(X, j)$表示，其中j表示X入栈后下一个入栈的单词的下标。</p><p><img src="1.jpg" alt></p><p>例如对于上图中的句法树，它的gold短语成分集合是<br>\[(S,0,6),(NP,0,2),(VP,2,5),(ADVP,3,4),(ADJP,4,5)\]如果采用top-down的转移系统，非终结符入栈的顺序为<br>\[(S, 0), (NP, 0), (VP, 2), (ADVP, 3), (ADJP, 4)\]如果采用in-order的转移系统，非终结符入栈的顺序为<br>\[(NP, 1), (S, 2), (VP, 3), (ADVP, 4), (ADJP, 5)\]<br>正如之前所说，top-down中非终结符的下标就是短语的第一个单词的下标，但是in-order不是的，因为短语的第一个子结点已经在非终结符入栈之前形成了，所以它的下标是第二个子结点表示的短语的第一个单词的下标。</p><p>之前的top-down和in-order转移系统中并没有用到预测的短语集合$\gamma$和stack里的非终结符集合$\alpha$，但是在本文的转移系统中需要用到，因为本文要用它来改进loss函数，以此来实现Dynamic Oracle。</p><h2 id="top-down转移系统"><a href="#top-down转移系统" class="headerlink" title="top-down转移系统"></a>top-down转移系统</h2><p><img src="2.jpg" alt><br><img src="3.jpg" alt></p><p>上面两张图分别是top-down转移系统的转移过程和具体的转移示例。注意到REDUCE动作会将新的短语加入到$\gamma$集合中，并且从非终结符集合$\alpha$中删去该非终结符。而NT-X动作会将新的非终结符X加入到非终结符集合$\alpha$中。</p><h2 id="in-order转移系统"><a href="#in-order转移系统" class="headerlink" title="in-order转移系统"></a>in-order转移系统</h2><p><img src="4.jpg" alt><br><img src="5.jpg" alt></p><p>上面两张图分别是in-order转移系统的转移过程和具体的转移示例，大致细节和top-down转移系统类似。</p><h2 id="Dynamic-Oracles简介"><a href="#Dynamic-Oracles简介" class="headerlink" title="Dynamic Oracles简介"></a>Dynamic Oracles简介</h2><p>最后再解释一下Dynamic Oracle是干嘛用的，传统的Static Oracle就是在转移的每一步按照标准转移序列中的action进行转移，但是这样会有一个问题，如果预测的时候某一步预测错了，遇到了一个训练阶段没有出现过的状态，那么该怎么进行转移呢？这时候就要用到Dynamic Oracle，用来针对不同的错误情况进行动态的指导，引导它转移到正确的状态中去。另外在训练时可以手动加入一些错误状态，来训练模型，不然的话遇到的错误状态还是太少了，不足以训练好模型。</p><h1 id="Dynamic-Oracles"><a href="#Dynamic-Oracles" class="headerlink" title="Dynamic Oracles"></a>Dynamic Oracles</h1><hr><p>Goldberg (2012)证明了Dynamic Oracle可以通过定义一个损失函数来直接实现，而这个损失函数可以用来衡量当前状态可以产生的最优句法树和标准句法树的距离。最小化这个距离就会使得错误状态也会转移到最终错误最少的状态。而这个损失函数就要和当前状态c挂钩了，这样才能达到和传统的Dynamic Oracle类似的效果。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>传统的损失函数定义为预测短语成分集合和标准短语成分集合不相交的元素数量，即：<br>\[\mathcal{l}(c) = \min_{\gamma | c \to \gamma} \mathcal{L}(\gamma, \gamma_G) = \left| { {\gamma _G}\backslash \gamma } \right| + \left| {\gamma \backslash {\gamma _G}} \right|\]</p><p>根据<a href="https://godweiyang.com/2018/11/06/arxiv-18-SR-CP-bottom-up/">上一篇博文</a>的推导，该损失函数可以计算为<br>\[\mathcal{l}(c) = \min_{\gamma | c \to \gamma} \mathcal{L}(\gamma, \gamma_G) = \left| \mathcal{U}(c, \gamma_G) \right| + \left| {\gamma_c \backslash {\gamma _G}} \right|\]</p><p>上面的损失函数是上一篇论文中介绍的bottom-up的转移系统的Dynamic Oracle，但是本文主要讨论top-down和in-order的转移系统，因为转移系统多了non-terminal，所以需要新加入两项损失，用来衡量当前状态可以产生的最优句法树与标准句法树之间的汉明损失。</p><p>这两项新加的损失分别是：</p><ul><li>当前栈中已经生成的non-terminal集合$\alpha_c$中不包含在标准non-terminal集合$\alpha_G$中的non-terminal数量，即$\left| \alpha_c \backslash \alpha_G \right|$。</li><li>当前栈中违反了标准树中non-terminal顺序的non-terminal数量。</li></ul><p>所以最终的损失函数为：<br>\[\mathcal{l}(c) = \min_{\gamma | c \to \gamma} \mathcal{L}(\gamma, \gamma_G) = \left| \mathcal{U}(c, \gamma_G) \right| + \left| {\gamma_c \backslash {\gamma _G}} \right| + \left| \alpha_c \backslash \alpha_G \right| + out \_ of \_ order(\alpha_c, \alpha_G)\]</p><p>前面三项都很容易求得，至于最后一项，可以通过计算栈里的gold non-terminal序列的最长上升子序列来得到，而序列中每个non-terminal的标号就是它在标准树转移序列的non-terminal顺序标号。</p><h2 id="短语的可达性"><a href="#短语的可达性" class="headerlink" title="短语的可达性"></a>短语的可达性</h2><p>在这里用短语集合${(X, l, r)}$来表示一棵句法树，我们假设状态c的短语集合为$\sum = [(Y_p, i_p, i_{p-1}) \cdots (Y_2, i_2, i_1)|(Y_1, i_1, j)]$，那么我们说，标准句法树中的一个短语$(X, l, r) \in \gamma_G$当且仅当满足如下三个条件之一时，称它是“各自可达短语”：</p><p>对于top-down转移系统：</p><ul><li>$(X, l, r) \in \gamma_c$（因为短语已经包含在了状态c已生成的短语集合里，那么它当然是可达的）。</li><li>$j \le l &lt; r \wedge (X, l) \notin \alpha_c$（因为短语还在buffer中，并且短语的non-terminal还没有入栈，所以可以通过入栈$(X, l)$，再不断SHIFT然后REDUCE得到）。</li><li>$l \in \{i_k | 1 \le k \le p\} \wedge j \le r \wedge (X, l) \in \alpha_c$（这种情况表明了短语的左端点恰好位于栈里某个短语的边界处，而右端点又还在buffer里，所以还可以通过不断SHUFT然后REDUCE得到短语。但是如果左端点不是栈里短语的边界，那说明产生了交叉，自然不会可达了。而如果右端点已经在栈里了，那之后也不会得到了，因为转移系统每次都是REDUCE栈顶的短语，不可能从栈里面开始REDUCE的，当然这些前提条件当然是non-terminal$(X, l)$已经在栈里了）。</li></ul><p>对于in-order转移系统：</p><ul><li>$(X, l, r) \in \gamma_c$（因为短语已经包含在了状态c已生成的短语集合里，那么它当然是可达的）。</li><li>$j \le l &lt; r$（因为短语还在buffer中，所以可以通过入栈第一个左儿子，再入栈$(X, m)$，再不断SHIFT然后REDUCE得到）。</li><li>$l \in \{i_k | 1 \le k \le p\} \wedge j \le r \wedge (X, m) \notin \alpha_c$（这种情况表明了第一个左儿子已经生成了一部分或者完全生成了，并且根结点non-terminal还没有入栈，所以依然可以生成）。</li><li>$l \in \{i_k | 1 \le k \le p\} \wedge j \le r \wedge (X, m) \in \alpha_c \wedge \exists (Y,l,m) \in \sum $（这种情况表明了第一个左儿子已经完全生成了，并且根结点non-terminal在栈里，所以依然可以生成）。</li></ul><p>枚举标准树中的所有短语，根据以上规则可以得到可达短语集合$\mathcal{R}(c, \gamma_G)$，然后从标准短语集合中排除掉这部分短语，剩下的就是不可达短语集合$\mathcal{U}(c, \gamma_G) = \gamma_G \backslash \mathcal{R}(c, \gamma_G)$。这部分短语就是不论采取何种动作序列，最后都不可能生成的短语集合。</p><p>关于这两个Dynamic Oracles的正确性，这里就不再证明了，证明过程和上一篇bottom-up的差不多。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><hr><p>本文和基础的几个转移系统做了对比，代码也是在他们基础上进行修改的，结果如下：<br><img src="5.jpg" alt><br>可以发现，加了Dynamic Oracles之后，结果还是有略微提高的。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
            <tag> EMNLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Faster Shift-Reduce Constituent Parsing with a Non-Binary, Bottom-Up Strategy</title>
      <link href="/2018/11/06/arxiv-18-sr-cp-bottom-up/"/>
      <url>/2018/11/06/arxiv-18-sr-cp-bottom-up/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=33516495&auto=1&height=66"></iframe></div><p>论文地址：<a href="http://arxiv.org/abs/1804.07961" target="_blank" rel="noopener">Faster Shift-Reduce Constituent Parsing with a Non-Binary, Bottom-Up Strategy</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>这篇论文提出了一种非二叉化、自底向上的转移系统，并且针对它提出了一种Dynamic Oracle，用损失函数的形式来实现它。</p><p>之前的模型针对多叉树的处理都是采用head规则进行二叉化，或者采用空结点作为临时结点来进行隐式二叉化。但是本文将REDUCE动作扩展为REDUCE-k动作，从而可以对k叉树进行预测，这样减少了很多二叉树预测的中间过程，降低了模型的训练时间。并且为了提升准确率，还提出了一种用损失函数实现的Dynamic Oracle。</p><p>自底向上的转移系统就不详细介绍了，之前都已经介绍过了，这里只说明一下之后要用到的记号。</p><p>转移系统由一个stack和buffer组成，每个时刻的状态通常表示为$c = \left\langle {\sum ,i,f,\gamma} \right\rangle $，四个元素分别表示stack、buffer第一个单词的单词下标、分析结束标记、已经生成的短语成分的集合。</p><h1 id="自底向上的转移系统"><a href="#自底向上的转移系统" class="headerlink" title="自底向上的转移系统"></a>自底向上的转移系统</h1><hr><p>传统的转移系统REDUCE操作都只是将栈顶的两个元素归约为一个结点，而本文提出的转移系统将REDUCE扩展为REDUCE-X#k动作，归约栈顶概率最大的k个结点为结点X。举个例子，对于产生式$VP \to is \ ADVP \ ADJP$，使用的动作为REDUCE-VP#3，表示归约栈顶的三个结点。</p><p><img src="1.jpg" alt><br><img src="2.jpg" alt><br>具体的转移系统和例子如上图所示，为了区分具有不同数量儿子的结点X，将结点的label细化为X#k，表示具有k个儿子。例如对于VP结点，如果有两个儿子，那么它的label就是VP#2，如果有三个儿子就是VP#3。</p><h1 id="Dynamic-Oracle"><a href="#Dynamic-Oracle" class="headerlink" title="Dynamic Oracle"></a>Dynamic Oracle</h1><hr><p>本文采用的Dynamic Oracle是用损失函数来实现的，损失函数衡量的是状态c可以产生的最优句法树和标准句法树之间的距离，这样就可以计算出采取每一个动作之后下一个状态的损失函数值，选择损失函数值最小的动作。</p><p>对于状态c，损失函数$l(c)$定义为状态c可以产生的最终的句法树t和标准句法树$t_G$之间的最小汉明距离，即：<br>\[l(c) = \min_{t|c \to t} \mathcal{L}(t, t_G)\]<br>一个训练正确的Dynamic Oracle应当使得预测的下一个状态$\tau(c)$不会增加损失函数值，即<br>\[l(\tau(c)) - l(c) = 0\]<br>这个最小汉明损失可以定义为$\left| { {t_G}\backslash t} \right| + \left| {t\backslash {t_G}} \right|$，下面就将讨论这两部分怎么计算，主要用到短语的可达性和可分解性。</p><h2 id="短语的可达性"><a href="#短语的可达性" class="headerlink" title="短语的可达性"></a>短语的可达性</h2><p>在这里用短语集合${(X, l, r)}$来表示一棵句法树，我们假设状态c的短语集合为$\sum = [(Y_p, i_p, i_{p-1}) \cdots (Y_2, i_2, i_1)|(Y_1, i_1, j)]$，那么我们说，标准句法树中的一个短语$(X, l, r) \in \gamma_G$当且仅当满足如下三个条件之一时，称它是“各自可达短语”：</p><ul><li>$(X, l, r) \in \gamma_c$（因为短语已经包含在了状态c已生成的短语集合里，那么它当然是可达的）。</li><li>$j \le l &lt; r$（因为短语还在buffer中，所以可以通过不断SHIFT然后REDUCE得到）。</li><li>$l \in \{i_k | 1 \le k \le p\} \wedge j \le r$（这种情况表明了短语的左端点恰好位于栈里某个短语的边界处，而右端点又还在buffer里，所以还可以通过不断SHUFT然后REDUCE得到短语。但是如果左端点不是栈里短语的边界，那说明产生了交叉，自然不会可达了。而如果右端点已经在栈里了，那之后也不会得到了，因为转移系统每次都是REDUCE栈顶的短语，不可能从栈里面开始REDUCE的）。</li></ul><p>枚举标准树中的所有短语，根据以上规则可以得到可达短语集合$\mathcal{R}(c, \gamma_G)$，然后从标准短语集合中排除掉这部分短语，剩下的就是不可达短语集合$\mathcal{U}(c, \gamma_G) = \gamma_G \backslash \mathcal{R}(c, \gamma_G)$。这部分短语就是不论采取何种动作序列，最后都不可能生成的短语集合。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>对于每一个状态c，可以定义它的损失函数为<br>\[l(c) = \min_{\gamma |c \to \gamma} \mathcal{L}(\gamma, \gamma_G) = \left| { \mathcal{U}(c, \gamma_G)} \right| + \left| {\gamma_c \backslash {\gamma_G}} \right|\]<br>其中第一个因子惩罚的是False Negative短语，也就是漏报的短语，即正确的但是不可能被生成的短语。第二个因子惩罚的是False Positive短语，也就是误报的短语，即已经生成的但是是错的短语。</p><h2 id="正确性证明"><a href="#正确性证明" class="headerlink" title="正确性证明"></a>正确性证明</h2><p>那么我们如何证明，按照这个最小的损失函数值走下去，一定能得到最优的句法树呢？也就是要证明，这个状态c的损失函数，的确就是从状态c能得到的最优句法树和标准树的汉明损失。<br>\[\min_{\gamma |c \to \gamma} \mathcal{L}(\gamma, \gamma_G) = \left| { \mathcal{U}(c, \gamma_G)} \right| + \left| {\gamma_c \backslash {\gamma_G}} \right|\]<br>首先证明这个损失函数是<strong>短语可分解</strong>的，也就是证明，对于一个标准树中的短语集合，如果其中的每一个短语都是各自可达的，那么整个集合中的短语可以同时生成。</p><p>证明这个性质要用到数学归纳法。首先$m = 1$时显然成立，然后假设集合元素个数为$m, (m &gt; 1)$时性质成立，下面证明集合T元素个数为$m + 1$时性质也成立。</p><p>令$(X, l, r)$表示集合T中偏序最小的短语，即l是最小的，如果l有相等的，就再取r最小的。根据假设，$(X, l, r)$是从状态c可到达的gold短语。令$T’ = T \backslash (X, l, r)$，所以集合T’有m个元素，根据递归定义，整个集合都是从状态c可达的。</p><p>如果短语的可达性条件中第一种情况满足，那么$(X, l, r)$已经存在于状态c已生成短语集合中了，那么整个T集合当然是可达的。</p><p>如果第二种情况满足，即$j \le l &lt; r$，那么可以通过不断SHIFT再一个REDUCE来得到短语$(X, l, r)$。那么T’集合又如何能全部生成呢？可以发现T’集合中的短语，要么是左边界等于l并且右边界大于r的（根据定义），这种可以继续SHUFT再REDUCE得到（满足条件3）。要么是左边界大于等于r的（因为都是标准树中的短语，所以不会有边界交叉），这种满足条件2，也可达。<strong>论文中就说了这两种情况，是否还存在一种左边界大于等于l，右边界小于等于r的情况呢？当然这种情况满足条件1，因为在生成$(X, l, r)$的时候就已经生成了。</strong>所以最终T集合还是全部可达的。</p><p>如果第三种情况满足，即l是栈里某个短语的边界，而r大于等于j，那么这种情况依然可以通过不断SHIFT再REDUCE得到，而T集合仍然可以全部可达，原因和上一种情况类似。</p><p>所以可以证得，从状态c开始，存在某个转移序列，使得所有可达短语全部生成，那么只有不可达的短语会被错过，即：<br>\[\min_{\gamma |c \to \gamma} \left| {\gamma_G \backslash {\gamma}} \right| = \left| { \mathcal{U}(c, \gamma_G)} \right|\]</p><p>最后一步就是证明另一项$\left| {\gamma \backslash {\gamma_G}} \right|$等于$\left| {\gamma_c \backslash {\gamma_G}} \right|$。首先因为前者肯定包含了后者，因为随着转移的进行，预测错误的短语只会增加，不会减少。然后证明最优句法树不会再增加新的错误短语，即从状态c开始的最优句法树一定是$\mathcal{R}(c, \gamma_G) \cup \left| {\gamma_c \backslash {\gamma_G}} \right|$。这里不是很好想，可以想象从包含当前栈顶短语的最小的标准短语开始，一步步的进行转移，按照<a href="https://www.aclweb.org/anthology/D/D16/D16-1001.pdf" target="_blank" rel="noopener">James and Huang</a>中的Dynamic Oracle。</p><p>至此已经证明了，这个损失函数可以保证每一步都按照最优的策略来进行转移。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><p>实验采用的转移模型都是基于<a href="http://aclweb.org/anthology/N16-1024" target="_blank" rel="noopener">Dyer et al.</a>，并且也采用了<a href="https://www.aclweb.org/anthology/D/D16/D16-1001.pdf" target="_blank" rel="noopener">James and Huang</a>中的exploration策略来增加错误状态，提高Dynamic Oracle的准确率。</p><p>在PTB上的实验结果如下：<br><img src="3.jpg" alt></p><p>结果其实也不是很高，现在来看算低的了，本文只和其他的转移系统结果进行了比较，可以说在转移系统上还算比较高的吧，虽然今年转移系统也做到了92.0了。在运行速度上，本文的模型也比其他转移系统略有提升，我感觉虽然不需要二叉化了，但是REDUCE#k动作的增加同样会增加复杂度，这是自底向上转移系统的一个固有的问题。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>本文提出了一个非二叉化的自底向上的转移系统，主要有如下几个贡献点吧：</p><ul><li>非二叉化预测，采用REDUCE#k动作。</li><li>采用损失函数来实现Dynamic Oracle。</li><li>准确率上超过了除了in-order的大多数转移系统。</li><li>训练速度上是所有转移系统中最快的。</li></ul><p>看完这篇，我准备在chart-based的top-down模型上面也搞一个这种Dynamic Oracle试试，需要改变的就是每个状态的损失函数，现在的F1还只有91.87，希望能有所突破吧。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Meta Multi-Task Learning for Sequence Modeling</title>
      <link href="/2018/10/25/metamtl/"/>
      <url>/2018/10/25/metamtl/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=32507551&auto=1&height=66"></iframe></div><blockquote><p>这篇文章是知识分析课准备讲的论文，随便拿来看一看了，简单介绍一下吧，论文是复旦邱锡鹏老师组写的。</p></blockquote><p><strong>论文地址：</strong><a href="https://arxiv.org/pdf/1802.08969.pdf" target="_blank" rel="noopener">Meta Multi-Task Learning for Sequence Modeling</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>多任务学习一般的模型是共享特征表示层，也就是最底层的特征表示层是共享的，上层的神经网络都是随具体任务而不同的。但是这有个问题，比如用LSTM对句子进行建模的时候，不同的短语的组合函数是一样的，比如动词+名词、形容词+名词。但是组合函数应该定义成不同的比较好，于是这篇文章提出了针对不同的任务，不同的时刻产生不同的参数矩阵的动态参数生成方法。</p><p>本文主要有如下三个贡献点：</p><ul><li>不同于以往的特征层的共享，本文模型提出了函数层的共享，也就是针对不同任务动态的生成不同的组合函数。</li><li>不仅对于多任务，Meta-LSTM对于单任务也有提升，因为是动态生成参数，所以每个时刻的参数都不一样，可以更好地表示不同的短语语义。</li><li>模型还可以被用作迁移学习，Meta-LSTM在训练完成后可以直接被用于新任务上面作为先验知识，而任务特定的LSTM就作为后验知识。</li></ul><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><hr><h2 id="任务介绍"><a href="#任务介绍" class="headerlink" title="任务介绍"></a>任务介绍</h2><p>本文主要在序列标注和文本分类两个任务上面做实验，而且是多任务的，序列标注包括NER和POS tagging，文本分类包括多个不同领域的文本分类。</p><h2 id="传统模型"><a href="#传统模型" class="headerlink" title="传统模型"></a>传统模型</h2><p>传统的多任务模型共享一个私有LSTM特征表示层，用这个私有LSTM学习出句子的表示，然后和词向量拼接共同输入到任务特定的公有LSTM去。具体结构如下图所示：<br><img src="1.jpg" alt><br>输出层每个任务都是不共享的，和一般的模型一样，这里就不介绍了。最终的损失函数为所有任务的损失函数加权之和。</p><p>多任务模型的训练策略如下所示：</p><ul><li>首先随机选择一个任务。</li><li>然后从这个任务的数据集中随机选择一个mini-batch。</li><li>然后用这个任务的mini-batch数据去训练并更新参数。</li><li>不断重复以上三个过程。</li></ul><p>这样就可以训练出一个适用于所有任务的多任务模型。</p><h2 id="元多任务学习"><a href="#元多任务学习" class="headerlink" title="元多任务学习"></a>元多任务学习</h2><p>传统模型只共享了特征表示层，也就是共享了私有LSTM。本文的模型创新就是通过Meta-LSTM动态生成针对每个任务、每个时刻不同的参数，然后用每个任务特定的Basic-LSTM进行编码。具体结构如下图所示：<br><img src="2.jpg" alt><br>其中Basic-LSTM的结构和普通的LSTM基本一样，唯一区别就是每个时刻的参数W和b是通过Meta-LSTM动态生成的，形式化定义如下：<br><img src="3.jpg" alt><br>因为W维度过大，计算复杂度太高，并且也容易导致过拟合，所以这里采用了SVD分解：<br><img src="4.jpg" alt><br>而这里的$z_t$就是通过Meta-LSTM动态生成的，形式化定义如下：<br><img src="5.jpg" alt><br>如果精简的表示出这个LSTM之间的关系，可以写成如下形式：<br><img src="6.jpg" alt><br>概括起来就是：<strong>Basic-LSTM上一个时刻的输出$h_{t - 1}$、Meta-LSTM上一个时刻的输出$\hat h_{t - 1}$和当前时刻的单词表示$x_t$作为Meta-LSTM当前时刻的输入，产生的输出$z_t$用来产生Basic-LSTM当前时刻的参数矩阵。</strong></p><p>Meta-LSTM主要有如下两个优点：</p><ul><li>一个就是每个时刻的参数动态生成。</li><li>另一个就是比普通的LSTM参数数量更少，因为有SVD分解。</li></ul><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><p>文本分类任务是在16个购物网站评论数据集上做的，数据集大小如下所示：<br><img src="7.jpg" alt><br>最后在大多数数据集上，Meta-LSTM都能做到最好结果，具体结果如下：<br><img src="8.jpg" alt></p><h2 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h2><hr><p>序列标注任务是在三个数据集上面做的，两个是NER数据集，一个是POS tagging数据集，具体结果如下：<br><img src="9.jpg" alt><br>只能说比最基础的LSTM+CRF模型高了那么一丢丢吧。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>本文提出了一种function-level的多任务共享机制，即使用Meta-LSTM来动态产生Basic-LSTM每个时刻的参数矩阵。</p><blockquote><p>看完后我在想，这个动态参数生成的机制能不能用在成分句法分析上面，例如对于top-down的chart-based模型，可以自顶向下通过Tree-LSTM动态产生每一个树结点的参数矩阵，然后用这个参数矩阵来预测结点的label和split。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 元学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 迁移学习 </tag>
            
            <tag> AAAI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Two Local Models for Neural Constituent Parsing</title>
      <link href="/2018/10/18/coling18-2localmodels/"/>
      <url>/2018/10/18/coling18-2localmodels/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=167876&auto=1&height=66"></iframe></div><blockquote><p>我们究竟是活了365天，还是活了1天，重复了364遍。</p></blockquote><p><strong>论文地址：</strong><a href="http://aclweb.org/anthology/C18-1011" target="_blank" rel="noopener">Two Local Models for Neural Constituent Parsing</a><br><strong>代码地址：</strong><a href="https://github.com/zeeeyang/two-local-neural-conparsers" target="_blank" rel="noopener">github</a></p><p>今天要介绍的论文来自COLING 2018，本文主要探讨了局部特征对成分句法分析到底有多大的影响，并同时提出了两种局部特征模型，在PTB上面取得了92.4的F1值。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>对于传统句法分析模型，需要使用大量的全局特征来指导每一步的决策。同时全局的损失函数也是必不可少的，可以用来避免预测偏差的问题，也就是loss mismatch问题。但是随着神经网络的加入，越来越多的模型使用了局部的特征来进行句法分析，主要原因是可以采用双向LSTM预先对句子进行编码，从而捕获全部特征。</p><p>因此本文主要探讨局部特征对句法分析的影响，顺带提出了两个句法分析模型。模型是chart-based的，但是将结构预测和标签预测分成了两个模型，首先通过双仿射模型来产生一棵无标签的句法树，然后再上面跑一遍Tree-LSTM来产生每一个结点的label。</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><hr><p>模型分为两部分，第一部分是无标签模型，用来预测不带label的句法树，第二部分是标签模型，用来在无标签模型预测出来的句法树上预测出每一个结点的label。</p><p>对于无标签模型，有两种方法来生成句法树，第一种叫做span模型，预测任意span是否属于标准树，也就是二分类。第二种叫做rule模型，也就是普通的chart-based方法，类似于CKY算法，预测每个span的得分最高的split。</p><p>对于标签模型，在生成的句法树上跑一遍Tree-LSTM，计算出每个结点的向量表示，这是Encoder部分。然后对于每个结点，运行一遍Decoder，也就是一个LSTM，解码出该结点处的label，如果label只有一个，那么就解码出“X - &lt;\L&gt;”，其中&lt;\L&gt;是结束符，而如果label不止一个，也就是一元产生式，那么就会解码出一系列非终结符。</p><p>下面我们一个个介绍编码和解码模型，我尽量不使用太多的公式来解释模型，一是因为本质上模型也都是很老的了，二是因为公式太多了我也懒得打嘻嘻。</p><h2 id="Span模型"><a href="#Span模型" class="headerlink" title="Span模型"></a>Span模型</h2><p>主要思想就是对于句子的每一个可能的span，预测它是否属于标准树，属于标签就是1，不属于就是0。因为一共有$n(n + 1) / 2$个span，所以时间复杂度为$O(n^2)$。在代码实现中，因为长度为1和长度为n的span一定属于标准树，所以不考虑这两种span。</p><p>模型通过一个双向LSTM来对短语进行编码，这个之前的文章已经说过很多次了，可以用来捕获全局信息。span$(i, j)$的向量表示和之前略有不同，这里不是用两端的差值，而是改用直接拼接得到：<br>\[v[i, j] = [f_{i + 1}; r_i; f_{j + 1}; r_j]\]<br>然后将每个span的表示输入到一个两层的前馈神经网络中，最后通过一个softmax进行二分类：<br>\[\begin{array}{l}o[i,j] = \tanh ({W_o}v[i,j] + {b_o})\\\\u[i,j] = {W_u}o[i,j] + {b_u}\\\\P({Y_{[i,j]}}|S,\Theta ) = softmax(u[i,j])\end{array}\]<br>其中$Y_{[i, j]}$可以取值1或者0，最终要使得正确span标签为1的概率与错误span标签为0的概率之和最高。</p><p>训练完毕之后，在预测阶段，使用CKY算法求解最优的句法树。注意这里不能直接采用$Y_{[i, j]} = 1$概率比较大的那些span组成句法树，因为可能根本就是一棵不合法的句法树。对于span$(i, j)$，它的split为k的概率为：<br>\[P(r|S, \Theta) = P(Y_{[i, k]} = 1|S, \Theta)P(Y_{[k + 1, j]} = 1|S, \Theta)\]</p><p>模型还有个扩展版本，就是在softmax层改二分类为预测每个label的概率。然后损失函数也稍做修改，对于每个标准树中的span，对所有label求和（求和主要针对的是一元产生式的多个label），使概率之和最大。在预测阶段解码时，依然使用CKY算法，只不过span的概率要修改为：$Y_{[i, j]} = 1$的概率为label不为终结符的概率之和，$Y_{[i, j]} = 0$的概率为label是终结符的概率。</p><h2 id="Rule模型"><a href="#Rule模型" class="headerlink" title="Rule模型"></a>Rule模型</h2><p>span模型是训练出每个span属于标准树的概率，然后预测时计算出产生式的概率。而rule模型就直接训练每个产生式的概率，对于span$(i, j)$，假设它的split为k的分数为$ps_k$，那么产生式的概率就为：<br>\[P([i, j] \to [i, k][k + 1, j] | S, \Theta) = \frac{\exp (ps_k)}{\sum\nolimits_{k’ = i}^{j - 1} {\exp (p{s_{k’}})} }\]<br>最终的损失函数就是标准树中所有产生式概率的负对数之和。解码依然使用CKY算法，只是这里产生式概率直接得到了，所以直接计算即可。</p><p>这种方法本质上其实就是用神经网络来拟合PCFG，得到和它类似的效果，最后再用CKY算法解码出句法树。</p><p>而上面的$ps_k$是怎么得到的呢？这里首先还是用双向LSTM的边界差值作为span的表示（我也不知道这里为什么又突然用这个表示了）：<br>\[s[i, j] = [f_{j + 1} - f_i; r_i - r_{j + 1}]\]<br>然后将span之前和span和span之后三部分的表示拼接起来：<br>\[sr[i, j] = [s[0, i - 1]; s[i, j]; s[j + 1, n - 1]]\]<br>然后将它输入到一个单层前馈神经网络：<br>\[r[i, j] = \phi (W_r^M sr[i, j] + b_r^M)\]<br>注意到这里的W和b是分为三种：父结点、左儿子、右儿子。</p><p>然后就是最后一步了，终于可以得到最终的span表示了。。。这里又有两种方法，一种是线性模型，直接对左右儿子的r向量加权求和。另一种是双仿射模型，这个方法也可以解决span长度不一导致的类别数可变的多分类问题，双仿射模型得分首先在父结点和左儿子向量上进行操作：<br>\[lps_k = {[r[i,j];1]^T}{W_{pl}}[r[i,k];1]\]<br>然后在父结点和右儿子向量上进行类似操作得到$rps_k$，最后相加得到最终的split得分：<br>\[ps_k = lps_k + rps_k\]</p><h2 id="标签生成模型"><a href="#标签生成模型" class="headerlink" title="标签生成模型"></a>标签生成模型</h2><p>首先用Tree-LSTM得到每个span的向量表示，然后再通过一个LSTM依次得到span的一个或者若干个label，这个就不详细解释了。</p><h2 id="联合训练"><a href="#联合训练" class="headerlink" title="联合训练"></a>联合训练</h2><p>整个模型总的损失函数是无标签模型损失加上标签生成模型的损失，其中无标签模型分为三种：二分类、多分类、rule模型。</p><p>整个模型的流程如下图所示：<br><img src="1.jpg" alt><br>span模型和rule模型的span得分计算如下图所示：<br><img src="2.jpg" alt></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><hr><p>几种模型在PTB上的F1值如下：<br><img src="3.jpg" alt><br>可以看出对于二分类span模型，用$v[i, j]$作为span表示效果最好，而对于rule模型，采用的是双仿射，那么用$sr[i, j]$，也就是拼接上span左右的span效果更好。总体来说，多分类的span模型F1值是最高的。</p><p>而和其他模型的比较如下表所示：<br><img src="4.jpg" alt><br>左侧是多模型融合的结果，右侧是单模型，本文的几个模型结果都是最高的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>这篇论文模型和变体有点多，其实仔细分析下来可以发现本质上也还是那么些东西，在这里简单总结一下几种模型：<br>首先是结构预测：</p><ul><li>二分类span模型：预测每个span属于标准树的概率，测试阶段使用CKY算法解码，通过span正确的概率计算得到产生式概率。</li><li>多分类span模型：在二分类的基础上，预测每个span是每个label的概率，最后求和得到它属于标准树的概率，测试过程和二分类相同。</li><li>线性组合rule模型：通过split的得分直接预测每个产生式的概率，采用线性组合计算概率，解码采用CKY算法。</li><li>双仿射rule模型：除了概率计算采用双仿射，其他部分和线性组合rule模型完全相同。</li></ul><p>然后是标签预测：<br>都是采用Tree-LSTM产生每个结点的编码，然后创新地使用LSTM来预测label，而不是用传统的label列表。</p><p>这篇论文可借鉴的地方还挺多的，结构和标签分开预测，span特征继续堆叠，双仿射变换等等。准备之后在以往模型上加上去试试，然后可以想想神经网络训练PCFG这类点子，其实本质上和chart-based的没啥大区别。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
            <tag> COLING </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear-Time Constituency Parsing with RNNs and Dynamic Programming</title>
      <link href="/2018/10/15/acl18-beamspanparser/"/>
      <url>/2018/10/15/acl18-beamspanparser/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=533259686&auto=1&height=66"></iframe></div><blockquote><p>好像已经很久没有看论文了呢，开学了一堆事情，以后还是要抽空阅读论文，保持一定的阅读量，并且不能光看最新的论文，还得去前人传统的方法中去寻找有没有能应用于深度学习的东西，说不定就发ACL了呢（手动滑稽）。</p></blockquote><p><strong>论文地址：</strong><a href="http://aclweb.org/anthology/P18-2076" target="_blank" rel="noopener">Linear-Time Constituency Parsing with RNNs and Dynamic Programming</a><br><strong>代码地址：</strong><a href="https://github.com/junekihong/beam-span-parser" target="_blank" rel="noopener">github</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>这次要介绍的论文是huang liang发表在ACL18的一篇短文，提出了一个基于转移系统的线性时间句法分析器。本文的主要贡献点主要有如下几点：</p><ul><li>传统的基于转移的句法分析模型都是贪心解码，不能考虑到所有的状态空间，所以本文的模型采用beam search将状态空间提升到了指数级别。</li><li>首次采用cube pruning将分析的时间复杂度降低到了$O(nb \log b)$。</li><li>采用max-violation损失函数代替原来的求和的损失函数，并且对cross-span的span进行了惩罚。</li><li>在单模型上取得了最高的F1值。</li><li>采用图结构的栈（GSS）代替了原来的stack，这样不需要时刻保存历史信息。</li></ul><h1 id="模型基础"><a href="#模型基础" class="headerlink" title="模型基础"></a>模型基础</h1><hr><h2 id="基于span的转移系统"><a href="#基于span的转移系统" class="headerlink" title="基于span的转移系统"></a>基于span的转移系统</h2><p>这个我已经在之前的文章<a href="https://godweiyang.com/2018/09/26/constituent-parsing-summary/#系统改进">成分句法分析综述</a>中详细阐述过了。核心思想就是stack里面保存的不再是短语结构树，而是span的左右边界下标$(i, j)$，初始时stack里面是$(0, 0)$，终止状态栈里是$(0, n)$，SHIFT之后栈顶变为$(j, j + 1)$，REDUCE之后栈顶变为$(k, j)$（假设之前栈顶两个元素是$(k, i)$和$(i, j)$）。</p><h2 id="Bi-LSTM特征"><a href="#Bi-LSTM特征" class="headerlink" title="Bi-LSTM特征"></a>Bi-LSTM特征</h2><p>状态转移时用双向LSTM两端的差值计算每个span的表示，然后计算出得分，用来预测action。</p><h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><hr><h2 id="句法树得分"><a href="#句法树得分" class="headerlink" title="句法树得分"></a>句法树得分</h2><p>还是和之前chart-based模型一样，用每个span的label得分之和作为句法树的总得分。</p><h2 id="图结构栈（Graph-Struct-Stack-GSS）"><a href="#图结构栈（Graph-Struct-Stack-GSS）" class="headerlink" title="图结构栈（Graph-Struct Stack, GSS）"></a>图结构栈（Graph-Struct Stack, GSS）</h2><p>因为要采用动态规划来枚举每个时刻所有的状态，不是用普通的stack，使用GSS来保存每个时刻的状态。GSS每个时刻只需要保存栈顶的span就行了，假设为$(i, j)$。如果action是SHIFT，那么下一步就变成了$(j, j + 1)$，如果action是REDUCE，那么还需要知道栈顶第二个元素是什么。因为考虑到了所有的状态空间，所以所有的$(k, i)$都是有可能的。</p><p>GSS的具体结构如下图所示：<br><img src="1.jpg" alt><br>每个时刻的状态仅用一个span表示，在具体实现的时候，每个span还保存了一个span指针数组，指向它前面所有可能的span，还保存了当前span以及之前所有span的分数之和$c$和当前span子树的分数之和$v$。每个状态还保存了一个时刻标记$l$，易知一共有$2n - 1$个时刻。</p><p>当采取SHIFT动作时，状态变为了$(j, j + 1)$，并且新的span$(j, j + 1)$的指针数组中新增加一个span也就是$(i, j)$。prefix分数变为$c + \xi$，其中$\xi$是span$(j, j + 1)$的最高label得分，而inside分数就是span$(j, j + 1)$的分数$\xi$。</p><p>当采取REDUCE动作时，枚举span$(i, j)$指针数组中所有的前一个span$(k, i)$，然后合并成一个span$(k, j)$，prefix分数变为$c’ + v + \sigma$，其中$\sigma$就是span$(k, j)$的最高label得分，inside分数变为了$v’ + v + \sigma$。实际代码实现中，REDUCE完了后，span$(k, j)$的指针数组要更新为span$(k, i)$的指针数组。</p><h2 id="Beam-Search和Cube-Pruning"><a href="#Beam-Search和Cube-Pruning" class="headerlink" title="Beam Search和Cube Pruning"></a>Beam Search和Cube Pruning</h2><p>在每个时刻，只保存prefix得分最高的前b个span状态，这样时间复杂度可以降为$nb^2$，但是$b^2$相对于句子长度来说还是太大了，所以采用cube pruning继续降到$nb \log b$。</p><p>cube pruning原理是这样的：普通的beam search每个时刻枚举至多b个span，每个span和之前的至多b个span结合，所以一共最多产生$b^2$个span。</p><p>而cube pruning在每个时刻都建立一个堆，首先用上一个时刻的beam里的b个span，来产生b个SHIFT的span，送入堆里。理论上来说还应该产生至多$b^2$个REDUCE的span，但是在这里对于每个span，只取它的指针数组里得分最高的那个span，来和它结合产生新的span，送入堆里。然后在产生好的堆里，每次取出得分最高的span，出堆，如果它是REDUCE得到的span，那么就继续按照它的指针数组得分从高到低顺序产生一个span，REDUCE完之后送入堆里。依次下去，直到出栈了b个span为止。</p><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><hr><p>还是使用max-margin loss来训练，但是有几点小小的改进。</p><h2 id="cross-span损失"><a href="#cross-span损失" class="headerlink" title="cross-span损失"></a>cross-span损失</h2><p>以往的损失函数里有个$\Delta (t,t’)$，衡量的是预测树和标准树不同的span的数量。但是这有个问题，因为用了隐式二叉化，所以在预测树里存在label为空的情况。如果这个span在标准树中label也是空，那么原来的损失就不惩罚这一项了，但是要考虑到如果这个span在标准树中与某个标准的span产生了交叉，那么它根本就不可能是对的，也得进行惩罚。</p><h2 id="max-violation-updates"><a href="#max-violation-updates" class="headerlink" title="max violation updates"></a>max violation updates</h2><p>这是huang liang在2012提出来的，其实就是计算出每个时刻预测得分和标准得分的差距，然后取差距最大的那个时刻的得分差距作为最终的损失函数，之前都是用每个时刻得分差距之和来作为损失函数的。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><p>下图是不同的beam大小对不同长度句子分析速度的影响：<br><img src="2.jpg" alt><br>最终综合考虑速度和准确率，选择beam大小为20。</p><p>最后是beam-span模型在PTB测试集上的准确率，在单模型上取得了最好的结果：<br><img src="3.jpg" alt></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>这篇论文是短论文，所以相比之下创新没有那么的大吧，主要还是速度上比chart-based有了提升，准确率上比普通的转移系统有了提升，另外还提出了几点小的改进吧，例如cross-span问题、max-violation损失之类的，转移系统也改成了适合用来进行beam search的GSS，为了进一步加快速度，还用了cube剪枝。</p><p>这么多改进其实个人感觉也是影响不大的，不是很通用，cross-span和max-violation损失可以考虑拿来用一下。下一步的工作还是考虑如何增加特征表示、加入头结点之类的吧。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACL </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给python新手的一些建议</title>
      <link href="/2018/10/11/python-advice/"/>
      <url>/2018/10/11/python-advice/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1293951677&auto=1&height=66"></iframe></div><blockquote><p>首先很荣幸被邀请给大一新入学的学弟学妹们谈谈编程入门经验，尤其是他们正在学的python语言。我就随便谈谈我大一时是如何编程入门的，以及怎么才能学好python编程，其他语言类似，都是相通的。最后再回答一下大家问的比较多的一些问题。</p></blockquote><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><hr><p>大多数人应该和我刚上大学时一样，完全没有接触过编程，所以在上python课的时候可能是一脸懵逼，完全不知道代码的原理是什么，为什么写出来就能运行，也不知道如何熟练地写出一段完整的代码。这都很正常，我大学之前也是对计算机一窍不通的，但是高中毕业的那个暑假，刚刚接触电脑、用上互联网的我，用了一个月自学了谭浩强的《C语言程序设计》，从此打开了新世界的大门。因此我下面结合我自身的经历给大家一些编程学习的经验，当然很多都要结合你自身的喜好和习惯，不用完全按我的来。</p><h1 id="一个称手的代码编辑器"><a href="#一个称手的代码编辑器" class="headerlink" title="一个称手的代码编辑器"></a>一个称手的代码编辑器</h1><hr><p>要想代码写的舒服，首先得用一个称手的代码编辑器，最常用的python IDE是pycharm，如果你刚入门很依赖调试的话，可以使用这款IDE。当然不能过于依赖IDE的调试功能，要练就肉眼调试的本领，这样能极大的提高你的代码能力。所以我熟练了之后换上了sublime代码编辑器，这是一款十分漂亮的编辑器，个人觉得比pycharm好看到不知哪里去了。而具体的安装与python环境配置，去看看我的教程就行了：<a href="https://godweiyang.com/2017/10/02/sublime/">Sublime Text安装与配置教程</a>。</p><h1 id="多练-gt-多看"><a href="#多练-gt-多看" class="headerlink" title="多练&gt;多看"></a>多练&gt;多看</h1><hr><p>很多人还以为和高中一样，上课听听就完事了，课后也就不花时间去写代码了。其实不然，要想学好python，光课上听是没什么用的，还需要课后不停的找题目练习，只有试了才知道自己可能会踩到什么坑，才能不断总结出许多代码编写的经验。我刚开始也是一个劲看书，觉得语法都看的差不多了，以为自己都懂了，但是真正自己写起来才发现有很多的问题。</p><p>课本首先得看熟了，上面的例子都要自己敲过去，还有布置的编程作业都得认真做完。推荐大家一个在线编程网站：<a href="http://www.runoob.com/python/python-tutorial.html" target="_blank" rel="noopener">Python 基础教程 | 菜鸟教程</a>，上面可以边学边练习，可以一定程度上提高你的学习积极性。</p><h1 id="善于使用EOJ"><a href="#善于使用EOJ" class="headerlink" title="善于使用EOJ"></a>善于使用EOJ</h1><hr><p>有很多同学学到最后都不知道有EOJ这个东西，这是我们学校的在线刷题网站，现在已经做得十分精美了。如果让你干巴巴的去写书上的代码，可能没多久你也没什么兴趣写了，但是如果让你用python做出来一道道的编程题，那么依旧会获得难以描述的快感，这能极大的提升你的积极性和代码能力。</p><p>首先去<a href="http://acm.ecnu.edu.cn/" target="_blank" rel="noopener">EOJ</a>注册一个账号，然后去做过的人多的简单题，很多题首先考察的不是代码能力，而是你的算法，下面我还会讲到算法的重要性。做题需要注意输入输出格式，注意题目中的各种坑点，代码不是自然语言，和数学题一样容不得半点差错。</p><h1 id="程序-算法-数据结构"><a href="#程序-算法-数据结构" class="headerlink" title="程序=算法+数据结构"></a>程序=算法+数据结构</h1><hr><p>光会写几行代码没用，还要学会使用基本的数据结构和算法，这样才能写出高效率高质量的代码。比如python中的列表<code>list</code>、元组<code>tuple</code>、字典<code>dict</code>等等，熟练使用它们将会大大减轻你的工作量。算法也是非常重要的，比如要对若干个数进行排序，如果暴力的话速度会很慢很慢，这时候你就要学会用快速排序算法，就能立即得到结果啦。</p><h1 id="学会查阅资料"><a href="#学会查阅资料" class="headerlink" title="学会查阅资料"></a>学会查阅资料</h1><hr><p>很多人遇到什么函数不会用了，使用报错了都不知道怎么办，到处问人，其实可以直接去<a href="https://docs.python.org/3/" target="_blank" rel="noopener">python官网</a>查看文档，里面都有各个函数的用法说明。遇到错误也可以百度错误内容，会有很多人写博客说明解决方法的，实在找不到推荐一个网站：<a href="https://stackoverflow.com/" target="_blank" rel="noopener">stackoverflow</a>，这是国外的一个问答网站，上面有超多编程相关问题，只要你提问了基本都有人替你解答。</p><h1 id="良好的编码风格"><a href="#良好的编码风格" class="headerlink" title="良好的编码风格"></a>良好的编码风格</h1><hr><p>很多同学刚开始写的代码风格都很丑，我自己也是，要尽快形成自己的代码风格，python相比c语言好很多，强制了缩进，不然编译都不给你通过，但是还是有很多地方需要你自己注意，比如表达式运算符两边要加空格、变量命名要可以直接看出用途等等。推荐大家去读一读google的<a href="https://www.runoob.com/w3cnote/google-python-styleguide.html" target="_blank" rel="noopener">python编程规范</a>，写出美观的代码，以后也便于维护。</p><h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><hr><ul><li><p><strong>Q: python相对于其他语言有哪些优势和区别？</strong></p></li><li><p><em>A:*</em> 首先python语言最大的优势就是简单，非常适合没有编程基础的人入门，它去除了c语言繁琐的指针类型，而且代码也很容易读懂，与人类语言有点类似。python也支持面向对象，并且不像c++和java那样繁琐，保持了它的简洁性。因为它是开源的，所以有很多很多的第三方库可以调用，很多功能根本不需要你自己动手写，别人都帮你写好封装好了，你直接调用一个函数就解决了。但是python也有缺点，一大缺点就是相比于c++和java这种较为底层的语言，python的运行速度非常慢，当然可以选择使用cython来加速python。并且python的格式要求非常严格，强制代码缩进，两条语句不能写在同一行等等。</p></li><li><p><strong>Q: 老师上课提到“面向对象”和“面向过程”，是什么意思？</strong></p></li><li><p><em>A:*</em> 简单来说，面向过程就是按照解决问题的顺序，一步步的编写代码、调用函数。而面向对象就是将你要处理的事物分解为一个个对象（也就是类），每个对象有很多属性，编写代码时只要生成这些类，并且调用它们的类方法就好了。面向对象的一大好处就是可维护性好，比如如果你要对两个相同的对象进行操作，如果用面向过程，那你就要写两段相同的代码，如果改了一段，还得同时修改另一段；而面向对象就很简单了，因为你的两个对象都封装在了一个类里，所以只要改一下类就好啦。</p></li><li><p><strong>Q: 拿到一个问题应该如何思考？如何解决？步骤是什么？</strong></p></li><li><p><em>A:*</em> 拿到一个问题（比如编程题），不要直接下手写代码，先想想该用什么方法解决？方法的时间复杂度和空间复杂度能否承受？要用到哪些函数？总体框架应该写成什么样子？然后再一个个模块编写，不要写一步想一步。</p></li><li><p><strong>Q: 什么是递归思维？有什么好处？</strong></p></li><li><p><em>A:*</em> 递归思想也就是递归算法是经常用到的一种算法，可以将规模很大的问题化解为一个个的小问题，便于求解也增加了可读性。举个例子吧，求解阶乘，常规方法是从$f(1)$开始逐项计算，而用递归思想就是要计算$f(n)$，就先计算出$f(n - 1)$然后乘以$n$，而$f(n - 1)$继续递归算下去就行了。</p></li><li><p><strong>Q: 对于萌新，基础薄弱，学长有什么好的方法？推荐一下有用的参考资料</strong></p></li><li><p><em>A:*</em> 基础薄弱那就先把书上的语法看懂了，边看边敲了自己试一试怎么用，不要光看不写。资料的话看再多不如自己敲来的记忆深刻，推荐《python简明教程》吧，最方便的还是不会的用法直接百度或者谷歌，看博客讲解。</p></li><li><p><strong>Q: 如何避免语法符号规范的错误？</strong></p></li><li><p><em>A:*</em> 语法错误很正常，我也经常遇到，特别是没怎么用过的函数，这个只能靠查文档，然后用多了自然就记得怎么用了。规范之类的上面说过了，去看google的python编码规范，养成一个好的编码习惯，既是为了别人能看懂你的代码，也是为了你自己今后还能看懂你自己的代码。</p></li><li><p><strong>Q: 如何判断什么时候该用什么函数？</strong></p></li><li><p><em>A:*</em> 这个就根据你要实现的功能啊，比如你要切分一个句子变成若干个单词，那么自然而然想到要用<code>string.split()</code>函数。当然刚开始你可能根本就不知道有哪些函数可以给你用，那么你就需要百度“python切分一个句子变成若干个单词”，然后就会告诉你要用什么方法了，多查多记以后就会用了。</p></li><li><p><strong>Q: OJ系统是什么？我们如何利用它？</strong></p></li><li><p><em>A:*</em> OJ全称是Online Judge，是一个在线评测系统，上面有很多很多编程题，你可以写代码实现它，然后提交给OJ运行，它会告诉你你的代码是否正确。大家可以利用我们学校自己的EOJ来进行编程训练，先从简单的题开始做起，记住，不要一遇到不会做的题就去百度找题解，一定要自己思考，自己想出了方法收获才是最大的，切记。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>成分句法分析综述</title>
      <link href="/2018/09/26/constituent-parsing-summary/"/>
      <url>/2018/09/26/constituent-parsing-summary/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=461153715&auto=1&height=66"></iframe></div><blockquote><p>一直以来想对保研到现在一年多看过的论文进行一个总结，正好赶上下周二要讲组会，所以将自己看过的成分句法分析相关的论文梳理一下，写一个粗略的综述。可能有很多细节还不是很懂，理解有误，还请大家指正。</p></blockquote><p><strong>PPT地址：</strong><a href="https://github.com/godweiyang/files-backup/tree/master/lecture/ppt1" target="_blank" rel="noopener">A Summary of Constituent Parsing</a><br><strong>代码地址：</strong><a href="https://github.com/godweiyang/ConstituentParsing" target="_blank" rel="noopener">Constituent Parsing</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>成分句法分析（constituent parsing）是自然语言处理中的一个基础任务，它的任务是给定一个句子，分析出句子的短语结构句法树。例如给定句子“The little boy likes red tomatoes .”，它的成分句法树如下图所示：<br><img src="syntactic_tree.png" alt><br>成分句法分析可以利用到许多下游任务中去，比如情感分析可以利用句子的成分句法树来进行递归神经网络建模，从而分析出句子的情感。也可以利用在其他基础任务中去，比如可以将训练好的成分句法树直接转为依存句法树，从而提升依存句法分析的准确率。</p><p>传统的成分句法分析方法主要是规则（grammar）和统计的，比如结合两者的概率上下文无关文法（PCFG），在此基础上产生了应用广泛的CKY解码算法。CKY算法本质上是一种动态规划算法，本文之后要讲到的chart-based模型的解码算法也是基于动态规划算法的，和CKY算法十分地相似。</p><p>Socher在2013年又提出了组合向量文法（CVG），将递归神经网络应用到了成分句法分析中，给每个短语结构赋予了向量表示。但是这种方法还是需要用到规则，采用CKY算法解码，时间效率比较低。还有一种基于CRF的神经网络句法分析方法，将离散的特征转化为了连续的特征表示。</p><p>不过，上面这些方法统统都不在本文的讨论范围之内。本文讨论近两年来研究最热门的几种模型，主要包括基于转移系统（transition-based）的模型、基于动态规划（chart-based）解码的模型、基于自顶向下贪心（greedy top-down）解码的模型和一些将预测树结构转化为预测序列（sequence to sequence）的模型。</p><h1 id="基于转移系统的模型"><a href="#基于转移系统的模型" class="headerlink" title="基于转移系统的模型"></a>基于转移系统的模型</h1><hr><p>基于转移系统的模型主要分为三大类。第一种是自底向上（bottom-up）的系统，代表性论文有Transition-based Neural Constituent Parsing等。第二种是自顶向下（top-down）的系统，代表性论文有Recurrent Neural Network Grammars和Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles等。最后一种是2017年提出的基于中序遍历（in-order）的系统，代表性论文有In-Order Transition-based Constituent Parsing等。</p><p>在这三类系统的基础之上，许多人又做了非常多的改进。例如提出了动态指导（dynamic oracle）技术，来解决序列预测中的exposure bias问题（具体含义之后会讲到）。还有使用强化学习中的policy gradient来代替dynamic oracle，解决了针对不同转移系统需要设计不同的dynamic oracle的问题。</p><p>基于转移的句法分析系统主要包含两个组成成分，一个是栈（stack），用来存放已分析的句法结构，另一个是缓存（buffer），用来存放待分析的句子。而预测句法树结构就转化为了预测转移系统每一个时刻应该采取的动作（action）序列。下面我们分别介绍几种不同的转移系统，我们用三元组$[S, B, T]$来表示转移系统每一个时刻的状态，分别代表栈、buffer的第一个单词下标、句法分析结束标志。</p><h2 id="自底向上的转移系统"><a href="#自底向上的转移系统" class="headerlink" title="自底向上的转移系统"></a>自底向上的转移系统</h2><p>自底向上的转移系统是根据句法树的后序遍历（post-order）顺序进行句法分析的，首先将buffer中的单词移进栈里，然后将栈顶的若干个单词归约为它们的父结点，直至最后buffer为空并且栈里只有一个根节点。</p><p>在句法分析之前，首先要对句法树进行二叉化，这一点在传统的CKY算法中也会用到。例如对于之前的那棵句法树，二叉化后就变成了下图所示：<br><img src="binarized_syntactic_tree.png" alt></p><p>自底向上转移系统的action形式化定义如下：<br><img src="bottom_up.jpg" alt><br>其中SHIFT动作就是将buffer里面的第一个单词移进栈里。REDUCE-L/R-X动作就是将栈顶的两个元素出栈，并且归约为它们的父结点X，然后再将父结点入栈，而L和R就是用来区分左儿子和右儿子谁是头结点（head branch）。Unary-X动作就是将栈顶元素出栈，并且归约为父结点X，这个动作是用来预测一元产生式的。最后FINISH动作用来判断句法分析是否结束。</p><p>注意到这里有一个问题：为什么这里一定要提前对句法树进行二叉化？主要原因是因为自底向上系统有个弊端，就是在不停地SHIFT之后，你不仅要预测哪一步开始REDUCE，还得预测REDUCE的话要REDUCE栈顶的多少个元素，这样预测的状态数就大大增加，导致训练时间也增加了许多。而二叉化后每次预测就只需要预测哪一步REDUCE就行了，每次REDUCE只REDUCE栈顶的两个元素。</p><p>对于上面的句法树，用自底向上系统分析的过程如下图所示：<br><img src="bottom_up_example.jpg" alt><br>自底向上转移系统的优点就是可以充分利用已经生成的子树信息，来辅助后面的子树预测。</p><p>但是缺点也很显然，因为无法知道父结点以及再上层的父结点信息，所以丢失了许多有用的全局信息，这也有点类似于CKY算法的弊端了，同样只能根据局部的子树信息预测当前子树。</p><p>另一个缺点就是需要提前进行二叉化，虽然二叉化加入了head结点信息，事实证明是很有用的，但是head结点的标注需要许多语义学知识，也可以用神经网络来自己学习到head结点，但是二叉化总归是比较麻烦的。一个较为简洁的做法就是，用空结点$\emptyset$来作为本不应该归约的两个结点的临时结点，在还原树结构的时候忽略这种空结点，这样就可以隐式地进行二叉化操作了。</p><h2 id="自顶向下的转移系统"><a href="#自顶向下的转移系统" class="headerlink" title="自顶向下的转移系统"></a>自顶向下的转移系统</h2><p>自顶向下的转移系统利用的是句法树的前序遍历（pre-order）序列，首先将父结点入栈，然后不断操作直到它的子结点全部入栈，这时将父结点连同所有子结点全部归约为上一层的父结点。</p><p>自顶向下转移系统的action形式化定义如下：<br><img src="top_down.jpg" alt><br>其中SHIFT动作和之前一样，都是将buffer的第一个单词入栈。而NT-X动作就是将父结点X入栈。REDUCE动作就是将栈顶若干个元素直到它们的第一个父结点为止都出栈，然后归约为一个结点，再次入栈。注意到这里不同于自底向上系统的地方是没有FINISH动作，笔者也没有找到相关解释，猜测可能是因为自底向上系统存在一元动作Unary-X，所以最后根节点可能会无限归约下去，需要通过FINISH来提前终止分析。当然其实转移系统的动作定义并没有严格的要求，不同论文定义的也都不一样，但是都大同小异，也就是都存在SHIFT-REDUCE动作。</p><p>对于上面的句法树，用自顶向下系统分析的过程如下图所示：<br><img src="top_down_example.jpg" alt></p><p>自顶向下系统的优缺点和自底向上系统恰好互补。优点就是可以充分利用全局信息，例如父结点的信息，并且不需要进行二叉化，因为REDUCE的时候只要往栈里找到第一个父结点就行了。而缺点就是无法利用局部信息，也就是子树信息，同样NT-X动作也可能会出现无限多次执行的情况，所以要加上一些限制条件。</p><h2 id="In-order转移系统"><a href="#In-order转移系统" class="headerlink" title="In-order转移系统"></a>In-order转移系统</h2><p>Zhang和Liu两人在2017年提出了in-order转移系统，它利用的是句法树的中序遍历（in-order）序列，首先将一个子结点SHIFT入栈，然后将父结点入栈，再不断操作直到该父结点的剩余子结点全部入栈，然后对它们进行归约。</p><p>in-order转移系统的action形式化定义如下：<br><img src="in_order.jpg" alt><br>其中SHIFT动作和之前一样，都是将buffer的第一个单词入栈。PJ-X动作是预测出当前栈顶的元素的父结点X。REDUCE动作就是将栈顶的若干个元素归约为最里面倒数第二个元素，也就是它们的父结点。</p><p>对于上面的句法树，用in-order系统分析的过程如下图所示：<br><img src="in_order_example.jpg" alt></p><p>in-order转移系统提出的动机也很符合人类的直觉，在你读一个句子的时候，如果你第一个看到的单词是“The”，那么你脑海中可能会想到后面紧跟着的可能是个名词短语NP，然后你继续往后看，果然验证了你的猜想，后面的单词序列是“red tomatoes”。</p><p>in-order转移系统的优点恰好结合了前面两种转移系统，既可以考虑到局部信息，又可以考虑到全局信息。</p><p><strong>模型变体：</strong>in-order系统就是在自顶向下系统的基础上，在父结点入栈之前先入栈了1个子结点。那么如果稍加修改，还可以提前入栈两个、三个等等。假设在父结点入栈之前先入栈了$k$个子结点，那么称这种转移系统为k-in-order系统。特别地，如果$k = 0$，那么这就是自顶向下转移系统；如果$k = 1$，那么这就是in-order转移系统；$k = \infty$，那么这就是自底向上转移系统。</p><h2 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h2><p>上面说到的三种基于转移的句法分析系统，都可以概括为预测每一个时刻的action，那么每一个时刻的状态如何表示就是最重要的问题。在每一个时刻，最重要的组成部分有三个，分别是当前栈的状态、当前buffer的状态、当前已生成的action序列的状态。</p><p>当前状态的表示通过下图所示的三个LSTM得到：<br><img src="transition_framework.png" alt><br>其中栈和buffer的编码使用的是stack-LSTM，而action的编码使用的是普通的LSTM。最后将三种LSTM输出拼接到一起，用softmax预测出正确的action。</p><p>另一个问题就是如何编码栈里的短语。在以前都是通过递归神经网络或者树状LSTM来编码树状结构短语的，而在这里的话直接将父结点和子结点合为一个序列，送入到双向LSTM中就行了，具体形式如下图所示：<br><img src="composition.png" alt></p><h2 id="系统改进"><a href="#系统改进" class="headerlink" title="系统改进"></a>系统改进</h2><p><strong>基于span的自底向上的转移系统</strong><br>黄亮在2016年论文里提出了转移系统的栈里用span的左右边界数值来代替分析出来的子树，并且将REDUCE动作和预测label分开来进行，他们的转移系统action形式化定义如下：<br><img src="span_based_bottom_up.png" alt><br>可以看出，在第偶数个时刻，只预测结构化动作sh和comb(对应于之前转移系统的SHIFT和REDUCE)，sh动作从buffer中移进栈里一个单词$w_{j+1}$，栈顶的span就变为了$span(j, j+1)$。而comb动作就是将栈顶的两个span归约为一个span。在第奇数个时刻，只预测栈顶span的label，如果这个span的确能构成一个短语，那么就预测它的label，否则的话说明只是个临时结点，就预测为空结点，这一点在之前就已经提到过了。</p><p>论文里给出了一个具体的例子：<br><img src="span_based_bottom_up_tree.png" alt><br><img src="span_based_bottom_up_example.png" alt></p><p>文章开头提供的代码也是基于这个转移系统的，一个好处是用span来表示的话比较方便，代码也好写。另一个好处就是每一个时刻的状态可以不再用三个LSTM动态地算出来了，而是可以提前用双向LSTM对句子进行编码，然后用两个单词输出的差值作为单词之间span的表示，最后用它来计算转移状态的向量表示。</p><p>例如在某个时刻，栈首元素为$(i, k, j)$，那么结构化预测就采用四个span特征：$span(0, i), span(i, k), span(k, j), span(j, n)$，为什么要用这四个呢？因为comb动作涉及到栈首的两个span，而之前已经生成的$span(0, i)$也可以作为局部信息指导预测，buffer中的$span(j, n)$也要考虑到，因为可能预测为sh动作，同时也可以作为全局信息参考。</p><p>而label预测就只要用到三个span特征：$span(0, k), span(k, j), span(j, n)$，因为label动作只需要对栈首的span进行预测，所以三个特征就足够了。</p><p><strong>生成模型RNNG</strong><br>RNNG是2016年提出的一种文法，全称叫做“Recurrent Neural Network Grammar”，是一种生成式模型。RNNG本质上就是自顶向下的转移系统，动作定义和之前介绍的基本一致。只是之前介绍的自顶向下的转移系统是判别式模型，每次SHIFT的单词都是buffer中给定的。而RNNG每次SHIFT的单词需要通过动作GEN(x)预测得出，最终模型对预测出来的句子分析出句法树。</p><p>正式一点就是，对于句子$x$和对应的句法树$y$，判别式模型是对条件概率$p(y | x)$进行建模，而生成式模型是对联合概率$p(x, y)$进行建模。</p><p>而RNNG的另一个重要应用是语言模型（language model），也就是建模$p(x)$。因为$p(x) = \sum\nolimits_{y \in \mathcal{Y}(x)} {p(x,y)}$，所以只需要枚举出所有可能的句法树$y$即可，但是这是指数级别的，显然不现实，这时候就需要用到“重要性采样（importance sampling）”。</p><p>令$q(y | x)$为RNNG作为判别式模型的时候产生句子$y$的条件概率，那么$p(x)$可以改写为<br>\[p(x) = \sum\nolimits_{y \in \mathcal{Y}(x)} {p(x,y)}  = \sum\nolimits_{y \in \mathcal{Y}(x)} {q(y|x)w(x,y)}  = {E_{q(y|x)}}w(x,y)\]<br>然后就可以采用蒙特卡罗方法进行采样了，从分布$q(y | x)$中采样$N$个样本：<br>\[{y^{(i)}} \sim q(y|x),i = 1,2, \ldots ,N\]<br>那么$p(x)$就可以近似表示为：<br>\[p(x) = {E_{q(y|x)}}w(x,y)\mathop  \approx \limits^{MC} \frac{1}{N}\sum\limits_{i = 1}^N {w(x,{y^{(i)}})} \]</p><p><strong>Dynamic Oracle</strong><br>文章开头提到了一个转移系统会遇到的问题：“exposure bias问题”，这个问题意思就是训练的时候转移系统的每个时刻都是按照标准的action来进行训练的，但是如果测试的时候遇到了一个训练时从来没有遇见过的状态，这时候该怎么预测？如果预测错了，那么之后的时刻可能错的越来越离谱，偏差越来越大。</p><p>解决的方法就是采用Dynamic Oracle技术，在预测错误的时候，按照标准树的结构指导转移系统向着错误尽可能小的状态进行转移。但是比较麻烦的是，对于每一个转移系统，可能大家定义的状态都不尽相同，所以Dynamic Oracle要针对特定的转移系统单独设计，一个解决方法就是之后要提到的Policy Gradient方法。</p><p>这里举一个针对上面的“基于span的自底向上的转移系统”的Dynamic Oracle例子。</p><p>首先是结构化oracle，如果当前的栈首span是$span(i, j)$，那么就在标准树中寻找所有包含$span(i, j)$并且最小的span，记为$span(p, q)$，那么下一步可以采取的动作定义如下：<br><img src="structural_oracle.png" alt><br>也就是说，如果$span(p, q)$比$span(i, j)$右边界多出一部分，那么为了向着标准span靠近，就只能sh单词入栈；否则如果左边界多出一部分，那么就必须先comb之前的两个span，扩大span左边的边界；否则的话左右两边都有空出，那就随机预测一个动作就行了。</p><p>然后是label oracle，这个就很简单了，如果当前的栈首span是$span(i, j)$，只需要在标准树中寻找是否存在$span(i, j)$，如果存在，那么就给他正常预测label就行了；如果不存在，那么直接预测为空结点。oracle定义如下:<br><img src="label_oracle.png" alt><br>而关于这个Dynamic Oracle的证明和更加深入的理解，参见我之前写过的一篇博客：<a href="https://godweiyang.com/2018/08/03/dynamic-oracles/">深入理解成分句法分析中的Dynamic Oracle</a>。</p><p>但是如果直接按照这个Dynamic Oracle来实现代码的话，效果不会有什么提升，原因就是训练的时候遇到的错误情形还是太少了，不足以应付所有的测试阶段遇到的未知情形。所以要在训练阶段加上exploration操作，也就是转移的每一个时刻，不要总是预测概率最大的action，而是以一定的概率随机选择一个action，诱导系统进入一个错误的状态，这样系统就能学到更多错误状态下的回正技巧了。</p><p><strong>Policy Gradient</strong><br>序列预测存在着两个问题：一个就是之前提到的exposure bias问题，另一个就是loss mismatch问题，意思就是在每一个状态的loss累和得到最终整个序列的loss，但是因为是贪心解码，并没有考虑到之后的结果，所以某一个状态的loss其实并不能代表整个序列的loss。</p><p>Dynamic Oracle可以解决第一个问题，如果修改一下也可以解决第二个问题，但是Dynamic Oracle需要针对特定的转移系统单独设计，不能通用，所以这里引入了强化学习中的Policy Gradient来解决这个问题。</p><p>首先用风险函数（risk objective）代替原来的损失函数：<br>\[\mathcal{R}(\theta ) = \sum\limits_{i = 1}^N {\sum\limits_y {p(y|{x^{(i)}};\theta )\Delta (y,{y^{(i)}})} } \]<br>其中$(x^{(i)}, y^{(i)})$是训练集中的标准数据。可以看出，风险函数其实就是所有可能的句法树和标准树的差异${\Delta (y,{y^{(i)}})}$的期望，训练的目的就是最小化所有句法树和标准树的差异，这样就消除了之前提到的两个问题。</p><p>但是可以发现，显然不可能枚举所有可能的句法树，这时候想到了之前用到的重要性采样方法。</p><p>但是不能直接对风险函数进行重要性采样，不然就会发现采样后的函数$\theta$消失了！那就没办法求导了。所以先对风险函数求导：<br>\[\begin{array}{l}\nabla \mathcal{R}(\theta ) = \sum\limits_{i = 1}^N {\sum\limits_y {p(y|{x^{(i)}})\Delta (y,{y^{(i)}})\nabla \log p(y|{x^{(i)}};\theta )} } \\ \approx \sum\limits_{i = 1}^N {\sum\limits_{y \in \mathcal{Y}({x^{(i)}})} {\Delta (y,{y^{(i)}})\nabla \log p(y|{x^{(i)}};\theta )} } \end{array}\]<br>这里的$y$是根据分布${p(y|{x^{(i)}})}$采样得到的结果。实验中可以将标准树也加入到采样结果中，可以提升准确率。<br>至于$\log$项是怎么来的，可以如下推导得来：<br>\[\nabla p(y|{x^{(i)}};\theta ) = p(y|{x^{(i)}})\frac{ {\nabla p(y|{x^{(i)}};\theta )}}{ {p(y|{x^{(i)}};\theta )}} = p(y|{x^{(i)}})\nabla \log p(y|{x^{(i)}};\theta )\]</p><h1 id="编码解码模型"><a href="#编码解码模型" class="headerlink" title="编码解码模型"></a>编码解码模型</h1><hr><p>上面介绍完了基于转移的句法分析系统，下面开始介绍编码解码（Encoder-Decoder）模型。</p><p>模型的大致框架如下图所示：<br><img src="chart_based.png" alt><br>首先通过编码器将句子编码成向量，然后用解码器对向量操作，解码出句法树。</p><h2 id="编码器（Encoder）"><a href="#编码器（Encoder）" class="headerlink" title="编码器（Encoder）"></a>编码器（Encoder）</h2><p>编码器的主要目的是将每个短语编码成语义向量，用来给解码器预测splits和labels。</p><p>编码器主要有两种，一种是简单的双向LSTM编码，下图是一个用双向LSTM对句子进行编码的示例：<br><img src="bi_lstm.jpg" alt><br>例如要编码“played soccer in”这个短语，那么就用“in”处的前向LSTM输出减去“She”处的前向LSTM输出，得到了短语的前向LSTM表示。类似的，用“played”处的反向LSTM输出减去“the”处的反向LSTM输出，得到了短语的反向LSTM表示。</p><p>另一种是multi-headed self-attention编码。Attention是谷歌在“Attention is all you need”中提出的一种方法，严格来说它并不能算作一种模型，只能说是一种机制。具体原理在这里就不细讲了，可以直接去看一下原文。</p><p>大体框架就是，每个单词的词向量经过三个不同的$W$矩阵变换之后得到了三个不同的向量表示$q, k, v$，分别拼接起来组成了矩阵$Q, K, V$，其中$Q, K$相乘就得到了任意两个单词之间的相似度矩阵，然后对矩阵每一行进行softmax就得到了每一个单词对于其他所有单词的权重。再乘上矩阵$V$就得到了它对其他所有单词的加权求和，以此来作为它的向量表示。</p><p>下图就是self-attention的框架图：<br><img src="single_attention.jpg" alt><br>形式化定义就是：<br>\[S(X) = \left[ { {\rm{softmax}}\left( {\frac{ {Q{K^{\rm{T}}}}}{ {\sqrt { {d_k}} }}} \right)V} \right]{W_O}\]<br>其中$Q = XW_Q, K = XW_K, V = XW_V$，$d_k$是向量$q, k$的维度，用它作为分母是为了防止数值太大溢出。最后的矩阵$W_O$是为了将输出映射到与输入相同的维度。</p><p>而multi-headed self-attention就是将刚刚的attention计算8次，并且相加：<br>\[M(X) = \sum\limits_{i = 1}^8 {S(X)} \]<br>注意这8个attention的参数矩阵是不共享的，也可以不相加，改为直接拼接。</p><p>最终的编码器模型如下图所示：<br><img src="multi_headed_attention.jpg" alt><br>也就是说，将刚刚的multi-headed self-attention经过一层layernorm之后再经过一层前馈神经网络，最后再经过一层layernorm得到输出。将上述模型复制8份，首尾拼接，即前面的输出作为后面的输入，即可得到编码器最终的输出，也就是每个单词最终的向量表示。</p><p>至于每个短语的表示，和双向LSTM编码一样，用短语边界两个单词向量的差值作为短语的表示。只是这里没有前向后向的概念，所以要将每个单词向量一分为二，前一半作为前向向量，后一半作为后向向量。当然在实际实现中，将单数维度提取出来作为前向表示，双数维度提取出来作为后向表示。</p><h2 id="解码器（Decoder）"><a href="#解码器（Decoder）" class="headerlink" title="解码器（Decoder）"></a>解码器（Decoder）</h2><p>得到了每个短语的向量表示之后，就需要对它们进行解码，得到最终的句法树，解码的方法主要有两种。<br><strong>基于动态规划解码的模型</strong><br>这种方法在论文中被叫做“chart-based model”，正如其名，就是利用一个数组来进行动态规划，求出每个span的最优split和最优label。</p><p>定义一棵句法树的分数为所有子结点的label分数之和，即：<br>\[{s_{tree}}(T) = \sum\limits_{(l,(i,j)) \in T} { {s_{label}}(i,j,l)} \]<br>其实原本论文中的定义还多了一项span的分数，但是由于具体实现中去掉这一项并没有什么影响，所以为了简便我就只算label分数了。</p><p>要使得句法树分数最大，不可能枚举所有的句法树，那就只能用动态规划算法求解了。对于任意一个$span(i, j)$，我们将它通过编码器产生的表示$s_{ij}$输入到前馈神经网络中，直接取得分最高的那一维作为最优label，即：<br>\[s_{label}(i, j, l) = [Vg(Ws_{ij}+b)]_l\]<br>而对于split，遍历所有的split，取两个子结点与自己结点得分之和最高的那个split即可：<br>\[s_{split}(i, j, k) = s_{label}(i, j, l_{ij}) + s_{label}(i, k, l_{ik}) + s_{label}(k, j, l_{kj})\]</p><p>最后的训练过程和以往一样，采用max-margin训练方法，即使得标准树的得分比预测树的得分至少高一个margin，在这里margin大小定义为两棵树不同短语的数量，最终的损失函数定义为：<br>\[\mathcal{L}(\theta ) = \max \left( {0,\Delta (\hat T,T) - {s_{tree}}(T) + {s_{tree}}(\hat T)} \right)\]</p><p><strong>基于自顶向下贪心解码的模型</strong><br>基于动态规划的解码算法时间复杂度为$O(n^3)$，对于长度大一点的句子来说还是有点不可接受的。但是如果采用自顶向下、贪心地去选择每一个span的最优split和最优label，那么时间复杂度将降到$O(n^2)$。具体操作过程如下，首先从根节点也就是$span(0, n)$开始，选择一个split，使得两个子结点与自己结点得分之和最高，而label还是向之前那样直接通过短语的向量计算得出。具体公式为：<br>\[\begin{array}{l}\hat l = \mathop {\arg \max }\limits_l [{s_{label}}(i,j,l)]\\\hat k = \mathop {\arg \max }\limits_k [{s_{split}}(i,k,l)]\end{array}\]<br>而由于贪心解码和转移系统action预测一样，在预测阶段可能会遇到训练阶段没有碰到过的状态，所以也需要用到Dynamic Oracle。同样也需要用到exploration，来增加训练阶段遇到的错误状态数。</p><h1 id="Sequence-to-Sequence模型"><a href="#Sequence-to-Sequence模型" class="headerlink" title="Sequence to Sequence模型"></a>Sequence to Sequence模型</h1><hr><p>上面最主流的两大模型：转移系统和编码解码模型都已经介绍完了，下面介绍几种比较新颖的方法。</p><p>大家都知道句法树和某些序列存在一一对应关系，句法树可以唯一转换成序列，序列也可以唯一转换成句法树，所以预测句法树的问题就转变为了预测序列问题，下面几种方法都是将句法树转换为了某种序列来进行预测。</p><h2 id="树结构转化为括号序列"><a href="#树结构转化为括号序列" class="headerlink" title="树结构转化为括号序列"></a>树结构转化为括号序列</h2><p>这种方法思路特别简单，因为训练集里原始数据的表示形式就是括号序列嘛，所以就采用语言模型直接预测出括号序列的概率。</p><p>但是不可能枚举出所有的句法树括号序列，所以最终还是只对其他句法分析器预测出来的最好的若干棵树进行预测概率，然后重排序选出概率最高的一棵树。</p><h2 id="句法距离（Syntactic-Distance）"><a href="#句法距离（Syntactic-Distance）" class="headerlink" title="句法距离（Syntactic Distance）"></a>句法距离（Syntactic Distance）</h2><p>这个方法就很新颖了，本质上也是将树结构转换成了唯一对应的序列。<br><img src="syntactic_distance_example.jpg" alt><br>首先看上面一张图，对于长度为$n$的句子，存在一个长度为$n - 1$的数字序列，满足如下条件：$n$个单词存在$n - 1$个两两相邻的单词对，而两个相邻的单词的最近公共祖先（LCA）在句法树中有一个高度，所以这$n - 1$个数的大小关系恰好对应了从左向右任意两个相邻单词对的LCA的高度的大小关系。</p><p>拿上面那张图为例，“She”和“enjoys”的最近公共祖先是“S”，所以高度最高，对应的数字也最大。“enjoys”和“playing”的最近公共祖先是“VP”，高度排第三，所以对应的数字大小也是排第三。依次类推，剩下的数也满足这个性质。可以证明，这个数字序列和句法树是一一对应的。更进一步可以发现，这个序列其实就是“中序遍历的结点的高度”，文中将其称为句法距离。</p><p>预测这个序列也很简单，通过一个双向LSTM，然后将每相邻两个单词的输出做一次卷积操作（因为要预测相邻两个单词的LCA高度嘛），然后再将输出送到一个双向LSTM中去，最后通过一个前馈神经网络得到每相邻两个单词的数字。</p><p>而从树到序列和从序列到树的算法都很简单，这里就不详述了，可以直接去看论文。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>下面列出了成分句法分析领域目前为止最好的一些结果：<br><img src="results.jpg" alt><br>最好的是采用self-attention编码器+外部预训练词向量ELMo的模型，第二是模型融合+重排序之后的结果，之后的模型也基本都是本文介绍过的，最厉害的就是最后一个2006年的模型，十几年了依然如此强悍。</p><h2 id="心得体会"><a href="#心得体会" class="headerlink" title="心得体会"></a>心得体会</h2><p>虽然看起来貌似已经看了不少的成分句法分析相关的工作了，但是其实还有很多细节性的工作还没有去了解。前两年ACL等顶会成分句法分析的论文都很少，但是18年又好像多了起来，但终究还是伯克利Dan Klein、斯坦福Socher、黄亮等一批大佬在做这个，想在巨人的肩膀上面做出点东西还是很有挑战性的。</p><p>目前能想到的工作只有在编码器上面做文章，学习出语义更加丰富的短语表示。或者可以采用失传多年的递归神经网络，解码时对句法树进行建模，但是随便试了一下，速度很慢而且存在梯度消失的问题，效果也不是很好。转移系统的话暂时也想不出什么好的点子，序列预测的话如果能再想出个新颖的一一对应的序列就好了。</p><p>前路还很长，说长也不长了，只有三年不到的时间了，做不出东西就要延毕了。但愿能在有限的三年时间里做出点成果，提高自己的代码能力，对这个领域也有更加深入的理解！</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><hr><p>[ACL15] Transition-based Neural Constituent Parsing<br>[NAACL16] Recurrent Neural Network Grammars<br>[EMNLP16] Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles<br>[TACL17] In-Order Transition-based Constituent Parsing<br>[EMNLP17] Effective Inference for Generative Neural Parsing<br>[ACL18] Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing<br>[ACL17] A Minimal Span-Based Neural Constituency Parser<br>[ACL18] Constituency Parsing with a Self-Attentive Encoder<br>[EMNLP16] Parsing as Language Modeling<br>[ACL18] Straight to the Tree: Constituency Parsing with Neural Syntactic Distance</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Recurrent Neural Network Grammars</title>
      <link href="/2018/09/02/rnng/"/>
      <url>/2018/09/02/rnng/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=25706282&auto=1&height=66"></iframe></div><p><strong>论文地址：</strong><a href="https://www.aclweb.org/anthology/N/N16/N16-1024.pdf" target="_blank" rel="noopener">Recurrent Neural Network Grammars</a><br><strong>代码地址：</strong><a href="https://github.com/clab/rnng" target="_blank" rel="noopener">github</a></p><p>今天要介绍的这篇论文是来自NAACL16的Recurrent Neural Network Grammars，主要贡献点就是提出了一种新的文法RNNG，不同于传统的PCFG之类的文法，RNNG使用RNN来对句子和它的句法树的联合概率进行建模，因此它是一个生成模型。但是稍稍修改就可以改为判别模型，也就是大家熟悉的基于转移的成分句法分析系统，并且转移系统是采用top-down方法的，也就是利用了句法树的前序遍历。</p><p>RNNG在语言模型任务上取得了当时的state-of-the-art结果，成分句法分析任务上，生成模型取得了媲美最好结果的F1值，而判别模型就差了点。本文最大的贡献点就是提出了生成式模型RNNG，说明了在数据量不是很大的时候，利用生成式模型可以提高成分句法分析的准确率。</p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>RNN在语言模型和其他许多NLP任务上面都已经取得了非常不错的效果，但是RNN只能捕捉到句子的序列特征，例如句子的句法结构等递归嵌套的结构信息无法用RNN捕捉到。</p><p>因此本文提出了一种利用RNN建模出来的全新文法RNNG，建立在句子的句法结构之上，消除了PCFG的上下文无关假设。并提出了两种变体，一种是生成模型，可以用来句法分析和训练语言模型，另一种是判别模型，可以用来句法分析。</p><p>RNNG建立在top-down转移系统之上，top-down转移系统相比于bottom-up转移系统有一个好处，就是不需要二叉化，因为如果bottom-up转移系统不二叉化的话，REDUCE的状态就会有很多种可能，不知道到底归约栈里的几个结点。而top-down转移系统就不存在这个问题，直接归约到第一个父结点为止就行了。本文应该也是第一个提出用RNN来实现top-down转移系统的，之前的方法都是用top-down的文法，或者是bottom-up的，例如Sochar2013的CVG，也是用二叉化后的RNN学习结点的语义表示。</p><h1 id="RNN文法"><a href="#RNN文法" class="headerlink" title="RNN文法"></a>RNN文法</h1><hr><p>RNNG定义为三元组$(N, \Sigma, \Theta)$，其中$N$是非终结符集合，$\Sigma$是终结符集合，并且$N \cap \Sigma = \emptyset$，$\Theta$就是神经网络的参数集合。RNNG和传统的PCFG的一个明显区别就是它没有显式地指出语法规则是什么，而是蕴含在了神经网络中，在句法转移的时候动态的生成。</p><h1 id="Top-down句法分析和生成"><a href="#Top-down句法分析和生成" class="headerlink" title="Top-down句法分析和生成"></a>Top-down句法分析和生成</h1><p>这部分主要介绍RNNG的两个变体，一个是top-down的句法分析系统，还有一个是稍稍修改后的生成系统。</p><h2 id="判别式系统"><a href="#判别式系统" class="headerlink" title="判别式系统"></a>判别式系统</h2><p>这个判别式模型之前也已经介绍过很多次了，和普通的基于转移的句法分析系统一样，输入是一个句子$x$，输出是它的句法分析树$y$。主要组成部分有句法树栈、句子单词buffer、动作集合，每一步的动作有三种：</p><ul><li>NT(X)： 将一个父结点X移进栈里。</li><li>SHIFT： 从buffer中移一个单词到栈里。</li><li>REDUCE： 将栈顶的若干个结点归约为它们的父结点，并且出栈。</li></ul><p>图1就是每个动作的状态变化过程，图2是判别式模型进行句法分析的示例：<br><img src="1.jpg" alt></p><p>当然得给动作添加一些限制，首先记当前状态为三元组$(B, S, n)$，分别表示buffer、栈、当前栈里未归约的父结点数量，这个之前的博客没有提及过：</p><ul><li>NT(X)动作只有当buffer不为空并且$n &lt; 100$的时候才能进行。因为buffer空了的话就没有单词了，此时不可能移进新的非终结符了，并且要限制$n &lt; 100$防止一元产生式无限生成下去。</li><li>SHIFT动作只有当buffer不为空并且$n \ge 1$时才能进行。前者不用解释了，后者的话因为是top-down的，所以栈里至少要有一个父结点才能移进新的单词。</li><li>REDUCE只有当栈顶不是没有归约的父结点才能进行。</li><li>REDUCE只有当$n \ge 2$或者buffer为空时才能进行。<strong>这里要解释一下为什么$n \ge 2$，因为如果buffer不为空同时$n = 1$，那么这时候如果REDUCE的话，栈里就只剩一个非终结符了，只可能是根节点S，而buffer里还有单词，所以这是不可能的。</strong></li></ul><p>记当前状态的可能动作集合为$\mathcal{A}_D(B, S, n)$。</p><h2 id="生成式系统"><a href="#生成式系统" class="headerlink" title="生成式系统"></a>生成式系统</h2><p>将上面的top-down转移系统稍稍修改即可得到生成式系统。区别有两点：</p><ul><li>首先没有了输入的buffer，取而代之的是输出的buffer $T$。</li><li>其次因为没有输入单词了，所以在需要输入单词的时候采用GEN(x)动作来产生一个新的单词$x$，然后移进栈里，取代SHIFT动作。</li></ul><p>图3就是每个动作的状态变化过程，图4是生成式模型进行句法分析的示例：<br><img src="2.jpg" alt></p><p>同样也要对其采取一些限制：</p><ul><li>GEN(x)动作只有当$n \ge 1$时才能进行，上面SHIFT限制已经解释过了。</li><li>REDUCE只有当$n \ge 1$或者buffer为空时才能进行。<strong>这里再次解释一下，上面判别式模型限制条件是$n \ge 2$，为什么这里就变成了$n \ge 1$？因为生成模型没有输入buffer，所以即使$n = 1$时REDUCE了，以后不要再GEN(x)即可，直接结束分析</strong></li></ul><p>记当前状态的可能动作集合为$\mathcal{A}_G(T, S, n)$。</p><h2 id="转移序列"><a href="#转移序列" class="headerlink" title="转移序列"></a>转移序列</h2><p>因为一棵句法树的前序遍历是唯一的，所以不管用判别式模型还是生成式模型，得到的动作序列也都是唯一的。对于句子$x$和句法树$y$，记生成式模型动作序列为$a(x, y)$，判别式模型动作序列为$b(x, y)$。</p><h1 id="生成式模型"><a href="#生成式模型" class="headerlink" title="生成式模型"></a>生成式模型</h1><hr><p>本文最重要的就是上面提到的生成式模型，因为GEN(x)动作的存在，所以模型同时对句子$x$和句法树$y$的联合分布进行了建模。记当前状态的向量表示为$u_t$，那么联合分布可以表示为：<br>\[p(x,y) = \prod\limits_{t = 1}^{\left| {a(x,y)} \right|} {p({a_t}|{a_{ &lt; t}})}  = \prod\limits_{t = 1}^{\left| {a(x,y)} \right|} {\frac{ {\exp r_{ {a_t}}^T{u_t} + {b_{ {a_t}}}}}{ {\sum\nolimits_{a’ \in {\mathcal{A}_G}({T_t},{S_t},{n_t})} {\exp r_{a’}^T{u_t} + {b_{a’}}} }}} \]<br>其中$r_a$表示动作$a$的向量表示，$b$表示偏移向量，都包含在了RNNG参数集合$\Theta$里面，通过训练得到。</p><p>而当前状态的向量表示$u_t$由三部分得到，输出buffer的LSTM输出$o_t$、栈的LSTM输出$s_t$、历史动作序列的LSTM输出$h_t$，然后经过一个前馈神经网络得到：<br>\[u_t = \tanh (W[o_t; s_t; h_t] + c)\]<br>$W$和$c$同样也包含在了RNNG参数集合$\Theta$里面，下图是三个LSTM的示例图：<br><img src="3.jpg" alt></p><h2 id="句法成分组合"><a href="#句法成分组合" class="headerlink" title="句法成分组合"></a>句法成分组合</h2><p>在REDUCE操作时，需要将若干个子结点归约为一个父结点，为了得到父结点的向量表示，再次利用一个LSTM对子结点序列进行编码，同时在首尾加上父结点，结构图如下所示：<br><img src="4.jpg" alt></p><h2 id="单词生成"><a href="#单词生成" class="headerlink" title="单词生成"></a>单词生成</h2><p>单词生成采用softmax寻找概率最大的单词，但是单词数量可能十分巨大，所以采用分层softmax的思想，首先预测当前动作是不是GEN，如果是GEN，记单词总数为${\left| { \sum  } \right|}$，再将单词平均分成${\sqrt {\left| \sum  \right|} }$个类别，用softmax预测属于哪个类别，然后在那个类别里再用softmax预测输出哪个单词。这样时间复杂度就从$O\left( {\left| \sum  \right|} \right)$降到了$O\left( {\sqrt {\left| \sum  \right|} } \right)$。</p><h2 id="参数训练和判别式模型"><a href="#参数训练和判别式模型" class="headerlink" title="参数训练和判别式模型"></a>参数训练和判别式模型</h2><p>模型最终训练目的就是使得联合概率最大。</p><p>而只需要将输出buffer改为输入buffer，GEN动作改为SHIFT动作，然后重新训练，就可以将模型变为判别式模型了，输出给定输入句子下概率最大的句法树。</p><h1 id="通过重要性采样进行推理"><a href="#通过重要性采样进行推理" class="headerlink" title="通过重要性采样进行推理"></a>通过重要性采样进行推理</h1><hr><p>本文的生成式模型另一大作用是训练语言模型$p(x)$，根据边际分布公式<br>\[p(x) = \sum\nolimits_{y \in \mathcal{Y}(x)} {p(x,y)} \]<br>可以直接得到$p(x)$，但是一句话的句法树可能性是指数级别的，不可能一一枚举，这时候就要用到重要性采样算法。</p><p>首先定义一个比较容易得到的条件分布$q(y | x)$，它满足如下性质：</p><ul><li>$p(y | x) &gt; 0$可以推出$q(y | x) &gt; 0$。</li><li>服从分布的样本很容易得到。</li><li>$q(y | x)$可以直接计算得到。</li></ul><p>可以发现，上面的判别式模型得到的条件分布符合上面的性质，所以这里直接用判别式模型来进行采样。</p><p>这样$p(x)$就变为了：<br>\[p(x) = \sum\nolimits_{y \in \mathcal{Y}(x)} {p(x,y)}  = \sum\nolimits_{y \in \mathcal{Y}(x)} {q(y|x)w(x,y)}  = {E_{q(y|x)}}w(x,y)\]<br>其中重要性权重$w(x,y) = p(x,y)/q(y|x)$。</p><p>最后如果根据分布$q(y | x)$采样得到了$N$个句法树样本，那么用蒙特卡罗方法就可以估计出$p(x)$了：<br>\[{E_{q(y|x)}}w(x,y) \approx \frac{1}{N}\sum\limits_{i = 1}^N {w(x,{y_i})} \]</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><p>实验部分主要说一下PTB上的句法分析和语言模型吧，下面两张图分别是句法分析和语言模型的结果：<br><img src="5.jpg" alt><br><img src="6.jpg" alt><br>句法分析方面可以看出，生成模型效果要远远好于判别模型，生成模型效果也接近了当时的最好结果。一个合理的解释是在小数据集上面，生成模型效果要更好，而在大数据集上，判别模型效果可以赶上生成模型。</p><p><strong>这里要提到的一点是，判别式模型就是每一个状态直接贪心argmax找到概率最大的动作，然后生成句法树。而生成式模型是利用判别式模型采样出100个概率比较高的句法树，然后用生成式模型计算它们的联合概率，重排序选择概率最高的句法树。</strong></p><p>语言模型方面，结果要比最好结果高了一点。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>RNNG这个文法是个生成式模型，建模了句子和句法树的联合分布，稍稍修改即可应用到句法分析和语言模型中，效果也非常的好。</p><p>最后，我再简要梳理一遍RNNG的主要训练过程，因为这篇论文也看了整整两天，还是看的头大，一些细节可能还是没完全搞清。</p><p>首先利用生成式模型对每句话进行训练，在每个状态计算正确的动作的概率，然后训练使得概率之积最大。</p><p>然后应用到句法分析中，只需要修改为判别式模型即可。</p><p>最后应用到语言模型中，由于需要用到重要性采样，所以直接利用判别式模型生成若干样本，然后根据算得的条件概率计算语言模型句子的概率。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
            <tag> NAACL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>In-Order Transition-based Constituent Parsing</title>
      <link href="/2018/08/28/tacl17-conparsing/"/>
      <url>/2018/08/28/tacl17-conparsing/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=366752&auto=1&height=66"></iframe></div><p><strong>论文地址：</strong><a href="https://www.aclweb.org/anthology/Q/Q17/Q17-1029.pdf" target="_blank" rel="noopener">In-Order Transition-based Constituent Parsing</a><br><strong>代码地址：</strong><a href="https://github.com/LeonCrashCode/InOrderParser" target="_blank" rel="noopener">github</a></p><p>今天要介绍的这篇论文是成分句法分析领域目前的第三名，结果最高的几篇paper可以参见ruder在github整理的列表：<a href="https://github.com/sebastianruder/NLP-progress/blob/master/constituency_parsing.md" target="_blank" rel="noopener">github</a>。<br>下面就是成分句法分析目前排名：</p><table><thead><tr><th>Model</th><th align="center">F1 score</th><th>Paper / Source</th></tr></thead><tbody><tr><td>Self-attentive encoder + ELMo (Kitaev and Klein, 2018)</td><td align="center">95.13</td><td><a href="https://arxiv.org/abs/1805.01052" target="_blank" rel="noopener">Constituency Parsing with a Self-Attentive Encoder</a></td></tr><tr><td>Model combination (Fried et al., 2017)</td><td align="center">94.66</td><td><a href="https://arxiv.org/abs/1707.03058" target="_blank" rel="noopener">Improving Neural Parsing by Disentangling Model Combination and Reranking Effects</a></td></tr><tr><td>In-order (Liu and Zhang, 2017)</td><td align="center">94.2</td><td><a href="http://aclweb.org/anthology/Q17-1029" target="_blank" rel="noopener">In-Order Transition-based Constituent Parsing</a></td></tr><tr><td>Semi-supervised LSTM-LM (Choe and Charniak, 2016)</td><td align="center">93.8</td><td><a href="http://www.aclweb.org/anthology/D16-1257" target="_blank" rel="noopener">Parsing as Language Modeling</a></td></tr><tr><td>Stack-only RNNG (Kuncoro et al., 2017)</td><td align="center">93.6</td><td><a href="https://arxiv.org/abs/1611.05774" target="_blank" rel="noopener">What Do Recurrent Neural Network Grammars Learn About Syntax?</a></td></tr><tr><td>RNN Grammar (Dyer et al., 2016)</td><td align="center">93.3</td><td><a href="https://www.aclweb.org/anthology/N16-1024" target="_blank" rel="noopener">Recurrent Neural Network Grammars</a></td></tr><tr><td>Transformer (Vaswani et al., 2017)</td><td align="center">92.7</td><td><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a></td></tr><tr><td>Semi-supervised LSTM (Vinyals et al., 2015)</td><td align="center">92.1</td><td><a href="https://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf" target="_blank" rel="noopener">Grammar as a Foreign Language</a></td></tr><tr><td>Self-trained parser (McClosky et al., 2006)</td><td align="center">92.1</td><td><a href="https://pdfs.semanticscholar.org/6f0f/64f0dab74295e5eb139c160ed79ff262558a.pdf" target="_blank" rel="noopener">Effective Self-Training for Parsing</a></td></tr></tbody></table><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>基于转移的成分句法分析主要分为两种：</p><ul><li>一种是自顶向下（top-down）的方法，按照前序遍历（pre-order）的顺序生成句法树。这种方法可以更好地利用全局信息，但是需要一个强大的编码器来对每个短语成分进行编码。</li><li>一种是自底向上（bottom-up）的方法，按照后序遍历（post-order）的顺序生成句法树。这种方法可以充分利用子树的特征来进行分析，但是却无法利用全局信息。</li></ul><p>本文的模型就对这两种方法进行了改进，采用中序遍历（in-order）的顺序来生成句法树。单模型最终取得了91.8的F1值（貌似也不是特别高？），采用监督重排序之后F1值提升到了93.6，采用半监督重排序之后F1值提升到了94.2。所以看起来还是重排序起了很大的作用。</p><h1 id="基于转移的成分句法分析"><a href="#基于转移的成分句法分析" class="headerlink" title="基于转移的成分句法分析"></a>基于转移的成分句法分析</h1><hr><p>首先简要介绍一下这三种基于转移的句法分析方法。</p><h2 id="自底向上的转移系统"><a href="#自底向上的转移系统" class="headerlink" title="自底向上的转移系统"></a>自底向上的转移系统</h2><p>自底向上的转移系统是基于后序遍历的，例如对于下图这棵句法树，算法产生结点的顺序为3、4、5、2、7、9、10、8、6、11、1。<br><img src="1.jpg" alt><br>a图是未经二叉化的句法树，b图是二叉化之后的句法树，二叉化之后的结点要用l和r来区分头结点。其实不二叉化也是可以的，伯克利一帮人的做法就是用$\emptyset $来作为临时结点，构造树的时候去掉就行了。</p><p>句法分析系统如下：<br><img src="2.jpg" alt><br>每个时刻的状态用三元组$[\sigma ,i,f]$来表示，分别表示栈中元素、buffer的第一个元素在句子中的下标、句法分析结束标记。系统一共有四个操作：</p><ul><li>SHIFT： 从buffer中移进一个单词到栈里。</li><li>REDUCE-L/R-X：将栈顶两个结点归约为一个父结点X。</li><li>UNARY-X：将栈顶元素归约为一元结点X。</li><li>FINISH：句法分析结束。</li></ul><p>上面那个句法树按照该模型分析的话过程如下：<br><img src="3.jpg" alt><br>优缺点很显然，可以充分利用已生成的子树来对父结点的预测进行分析，但是不能利用全局信息（也就是其他子树、父结点等信息），并且需要提前进行二叉化（这点可以用临时结点标记来规避）。</p><h2 id="自顶向下的转移系统"><a href="#自顶向下的转移系统" class="headerlink" title="自顶向下的转移系统"></a>自顶向下的转移系统</h2><p>自顶向下的转移系统是基于前序遍历的，例如对于之前那棵句法树，算法产生结点的顺序为1、2、3、4、5、6、7、8、9、10、11。</p><p>句法分析系统如下：<br><img src="4.jpg" alt><br>系统一共有三个操作：</p><ul><li>SHIFT： 从buffer中移进一个单词到栈里。</li><li>NT-X：对一个父结点生成出它的一个子结点X。</li><li>REDUCE：将栈顶的若干个结点归约为一个结点，并且全部出栈，注意它们的父结点这时已经在栈顶了。</li></ul><p>上面那个句法树按照该模型分析的话过程如下：<br><img src="5.jpg" alt><br>优缺点也很显然，可以充分利用全局信息，但是因为预测子树的时候，子树还没有生成，所以无法利用子树的特征来进行分析，所以需要提前对句子的每个短语进行编码。</p><h2 id="采用中序遍历的转移系统"><a href="#采用中序遍历的转移系统" class="headerlink" title="采用中序遍历的转移系统"></a>采用中序遍历的转移系统</h2><p>为了协调上面的两种问题，本文提出了一种基于中序遍历的转移系统。</p><p>其实采用中序遍历也符合人们的直觉判断，比如你读到一个单词“like”，脑子里首先就会想到，这个可能和下面短语共同组成了动词短语VP，然后接着往下看，果然印证了你的猜想。</p><p>中序遍历就是采用这种思想的，例如对于之前那棵句法树，算法产生结点的顺序为3、2、4、5、1、7、6、9、8、10。</p><p>句法分析系统如下：<br><img src="6.jpg" alt><br>系统一共有四个操作：</p><ul><li>SHIFT： 从buffer中移进一个单词到栈里。</li><li>PJ-X：向栈里移进父结点X，来作为栈顶结点的父结点。</li><li>REDUCE：将栈顶的若干个结点归约为一个结点，并且全部出栈，注意它们的父结点在出栈元素的倒数第二个。然后再将父结点入栈。</li><li>FINISH：句法分析结束。</li></ul><p>上面那个句法树按照该模型分析的话过程如下：<br><img src="7.jpg" alt></p><p>该转移系统还有很多变体。对于短语(S, a, b, c, d)，可以令它在栈中S结点之前的子结点个数为$k$，例如对于上面的中序转移系统，栈里存放顺序是“a S b c d”，那么$k = 1$，如果栈里存放顺序是“a b S c d”，那么$k = 2$。而对于自底向上的转移系统，$k$就是正无穷，对于自顶向下的转移系统，$k$就是0。</p><h1 id="句法分析模型"><a href="#句法分析模型" class="headerlink" title="句法分析模型"></a>句法分析模型</h1><hr><p>对于每一个状态，模型采用三个LSTM来预测当前步动作，结构如下图所示：<br><img src="8.jpg" alt><br>一个LSTM用来对栈顶元素进行编码，一个LSTM用来对buffer中所有元素进行编码，一个LSTM用来对之前预测完毕的所有动作进行编码。</p><h2 id="单词表示"><a href="#单词表示" class="headerlink" title="单词表示"></a>单词表示</h2><p>对于每个单词，用预训练词向量、随机初始化词向量、POS向量拼接起来，然后经过一个前馈神经网络来作为最终的单词表示：<br>\[{x_i} = f({W_{input}}[{e_{ {p_i}}};{\bar e_{ {w_i}}};{e_{ {w_i}}}] + {b_{input}})\]<br>其中${e_{ {p_i}}}$表示POS为$p_i$的向量，${\bar e_{ {w_i}}}$表示单词$w_i$的预训练词向量，${e_{ {w_i}}}$表示单词$w_i$的随机初始化词向量。$f$函数通常取ReLU。</p><h2 id="栈里的短语表示"><a href="#栈里的短语表示" class="headerlink" title="栈里的短语表示"></a>栈里的短语表示</h2><p>对于自顶向下和in-order的转移系统，由于不需要二叉化，所以采用如下图所示的LSTM来对栈里的短语进行编码：<br><img src="9.jpg" alt><br>具体的短语表示为：<br>\[{s_{comp}} = (LST{M_f}[{e_{nt}},{s_0}, \ldots ,{s_m}];LST{M_b}[{e_{nt}},{s_m}, \ldots ,{s_0}])\]<br>其中${e_{nt}}$是父结点的向量表示，其他都是子结点的短语表示。</p><p>而作为对比实验，自底向上的转移系统因为是二叉树，所以LSTM略有不同，结构图如下所示：<br><img src="12.jpg" alt><br>唯一的区别就是不管你短语的单词顺序如何，都要把中心词也就是头结点放在前面。</p><h2 id="贪心动作预测"><a href="#贪心动作预测" class="headerlink" title="贪心动作预测"></a>贪心动作预测</h2><p>上面的两个小节将buffer和栈里的元素都进行了编码，最后就要对当前状态进行动作预测了。</p><p>假设第$k$个状态为$[{s_j}, \ldots ,{s_0},i,false]$，那么当前状态每个动作的概率为：<br>\[p = SOFTMAX(W[{h_{stk}};{h_{buf}};{h_{ah}}] + b)\]<br>其中${h_{stk}}$是栈里的LSTM编码结果：<br>\[{h_{stk}} = LSTM[{s_0}, \ldots ,{s_j}]\]<br>${h_{buf}}$是buffer里的LSTM编码结果：<br>\[{h_{buf}} = LSTM[{x_i}, \ldots ,{x_n}]\]<br>${h_{ah}}$是之前动作序列的LSTM编码结果：<br>\[{h_{ah}} = LSTM[{e_{ac{t_k}}}, \ldots ,{e_{ac{t_0}}}]\]</p><p>最终的损失函数采用交叉熵：<br>\[L(\theta ) =  - \sum\limits_i {\sum\limits_j {\log {p_{ {a_{ij}}}}} }  + \frac{\lambda }{2}{\Vert \theta  \Vert ^2}\]</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><p>实验的超参数设置如下：<br><img src="10.jpg" alt><br>经过对比实验可以发现，结果比自底向上和自顶向下的转移系统都要略高一点，就算加上了重排序，还是略高一点。单模型的话，和2017年之前的结果相比的确是最高的，但是91.8的F1值现在看来不是特别高了，毕竟伯克利基于CKY算法的chart-parser都已经到了92多甚至93了。</p><p>详细结果如下表：<br><img src="11.jpg" alt><br>模型在依存句法分析和CTB上的表现也都很不错。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>本文提出了一种基于中序遍历转移系统的成分句法分析模型，主要的动机还是基于人类阅读时的直觉，该模型协调了自底向上和自顶向下转移系统的优缺点，在采用重排序之后，结果达到了非常高的水准。</p><p>当然我个人认为模型也存在一些改进的地方：</p><ul><li>单词的表示可以加上Char-LSTM。</li><li>预测阶段可以采用之前文章提到的Dynamic Oracle技术，来减少预测错误的发生。详见之前的文章：<a href="https://godweiyang.com/2018/08/03/dynamic-oracles/">地址</a>。不过这里的Dynamic Oracle要重新设计了，设计好了说不定又可以发一篇论文了？（手动滑稽）</li></ul>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
            <tag> TACL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K-bandit Algorithm</title>
      <link href="/2018/08/05/k-bandit/"/>
      <url>/2018/08/05/k-bandit/</url>
      
        <content type="html"><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><hr><p>有$K$个赌博机，每个赌博机有一定概率$P$吐出硬币，但是我们不知道这个概率是多少，每个赌博机吐出的硬币价值$V$也是不一样的，现在有$T$次机会选择赌博机，怎么选才能使得到的硬币总价值最大？</p><p>在下面的不同算法实现中，统一设定<br>\[\begin{array}{l}K = 5 \\ P = [0.1,0.9,0.3,0.2,0.7] \\ V = [5,3,1,7,4] \\ T = 1000000\end{array}\]<br>可以计算出，这种情况下：</p><ol><li>如果每次都选期望价值最高的4号赌博机，可以获得的最高总价值为2800000。</li><li>如果每次都选期望价值最低的2号赌博机，可以获得的最低总价值为300000。</li><li>如果随机选取赌博机，可以获得的期望总价值为1540000。</li></ol><h1 id="探索与利用算法"><a href="#探索与利用算法" class="headerlink" title="探索与利用算法"></a>探索与利用算法</h1><hr><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>“仅探索”（exploration-only）算法就是将机会平均分配给每一个赌博机，随机挑选赌博机。<br>“仅利用”（exploitation-only）算法就是选取当前平均价值最高的那台赌博机。</p><h2 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/python</span><span class="token comment" spellcheck="true"># -*- coding: UTF-8 -*-</span><span class="token keyword">import</span> random<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">def</span> <span class="token function">R</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> P<span class="token punctuation">,</span> V<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> P<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> V<span class="token punctuation">[</span>k<span class="token punctuation">]</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token number">0</span><span class="token keyword">def</span> <span class="token function">exploration_bandit</span><span class="token punctuation">(</span>K<span class="token punctuation">,</span> P<span class="token punctuation">,</span> V<span class="token punctuation">,</span> R<span class="token punctuation">,</span> T<span class="token punctuation">)</span><span class="token punctuation">:</span>    r <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">:</span>        k <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> K <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>        v <span class="token operator">=</span> R<span class="token punctuation">(</span>k<span class="token punctuation">,</span> P<span class="token punctuation">,</span> V<span class="token punctuation">)</span>        r <span class="token operator">+=</span> v    <span class="token keyword">return</span> r<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    K <span class="token operator">=</span> <span class="token number">5</span>    P <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    V <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    T <span class="token operator">=</span> <span class="token number">1000000</span>    <span class="token keyword">print</span> exploration_bandit<span class="token punctuation">(</span>K<span class="token punctuation">,</span> P<span class="token punctuation">,</span> V<span class="token punctuation">,</span> R<span class="token punctuation">,</span> T<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    main<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>代码运行结果为：获得总价值1538893。</p><h1 id="varepsilon-贪心算法"><a href="#varepsilon-贪心算法" class="headerlink" title="$\varepsilon $贪心算法"></a>$\varepsilon $贪心算法</h1><hr><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><h2 id="实现代码-1"><a href="#实现代码-1" class="headerlink" title="实现代码"></a>实现代码</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/python</span><span class="token comment" spellcheck="true"># -*- coding: UTF-8 -*-</span><span class="token keyword">import</span> random<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">def</span> <span class="token function">R</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> P<span class="token punctuation">,</span> V<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> P<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> V<span class="token punctuation">[</span>k<span class="token punctuation">]</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token number">0</span><span class="token keyword">def</span> <span class="token function">eplison_bandit</span><span class="token punctuation">(</span>K<span class="token punctuation">,</span> P<span class="token punctuation">,</span> V<span class="token punctuation">,</span> R<span class="token punctuation">,</span> T<span class="token punctuation">)</span><span class="token punctuation">:</span>    r <span class="token operator">=</span> <span class="token number">0</span>    Q <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>K<span class="token punctuation">)</span>    count <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>K<span class="token punctuation">)</span>    <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">:</span>        eplison <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> eplison<span class="token punctuation">:</span>            k <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> K <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            k <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>Q<span class="token punctuation">)</span>        v <span class="token operator">=</span> R<span class="token punctuation">(</span>k<span class="token punctuation">,</span> P<span class="token punctuation">,</span> V<span class="token punctuation">)</span>        r <span class="token operator">+=</span> v        Q<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token punctuation">(</span>v <span class="token operator">-</span> Q<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>count<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>        count<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">return</span> r<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    K <span class="token operator">=</span> <span class="token number">5</span>    P <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    V <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    T <span class="token operator">=</span> <span class="token number">1000000</span>    <span class="token keyword">print</span> eplison_bandit<span class="token punctuation">(</span>K<span class="token punctuation">,</span> P<span class="token punctuation">,</span> V<span class="token punctuation">,</span> R<span class="token punctuation">,</span> T<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    main<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>代码运行结果为：获得总价值2795546。</p><h1 id="Softmax算法"><a href="#Softmax算法" class="headerlink" title="Softmax算法"></a>Softmax算法</h1><hr><h2 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h2><h2 id="实现代码-2"><a href="#实现代码-2" class="headerlink" title="实现代码"></a>实现代码</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/python</span><span class="token comment" spellcheck="true"># -*- coding: UTF-8 -*-</span><span class="token keyword">import</span> random<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">R</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> P<span class="token punctuation">,</span> V<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> P<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> V<span class="token punctuation">[</span>k<span class="token punctuation">]</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token number">0</span><span class="token keyword">def</span> <span class="token function">eplison_bandit</span><span class="token punctuation">(</span>K<span class="token punctuation">,</span> P<span class="token punctuation">,</span> V<span class="token punctuation">,</span> R<span class="token punctuation">,</span> T<span class="token punctuation">,</span> tau<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    r <span class="token operator">=</span> <span class="token number">0</span>    Q <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>K<span class="token punctuation">)</span>    count <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>K<span class="token punctuation">)</span>    <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">:</span>        p <span class="token operator">=</span> softmax<span class="token punctuation">(</span>Q <span class="token operator">/</span> tau<span class="token punctuation">)</span>        rand <span class="token operator">=</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span>        total <span class="token operator">=</span> <span class="token number">0.0</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>K<span class="token punctuation">)</span><span class="token punctuation">:</span>            total <span class="token operator">+=</span> p<span class="token punctuation">[</span>i<span class="token punctuation">]</span>            <span class="token keyword">if</span> total <span class="token operator">>=</span> rand<span class="token punctuation">:</span>                k <span class="token operator">=</span> i                <span class="token keyword">break</span>        v <span class="token operator">=</span> R<span class="token punctuation">(</span>k<span class="token punctuation">,</span> P<span class="token punctuation">,</span> V<span class="token punctuation">)</span>        r <span class="token operator">+=</span> v        Q<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token punctuation">(</span>v <span class="token operator">-</span> Q<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>count<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>        count<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">return</span> r<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    K <span class="token operator">=</span> <span class="token number">5</span>    P <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    V <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    T <span class="token operator">=</span> <span class="token number">1000000</span>    tau <span class="token operator">=</span> <span class="token number">0.1</span>    <span class="token keyword">print</span> eplison_bandit<span class="token punctuation">(</span>K<span class="token punctuation">,</span> P<span class="token punctuation">,</span> V<span class="token punctuation">,</span> R<span class="token punctuation">,</span> T<span class="token punctuation">,</span> tau<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    main<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>代码运行结果为：$tau=0.01$时，获得总价值1397795。$tau=0.1$时，获得总价值2798372。当然随机性很大，每次运行结果都会不同</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Understanding of Dynamic Oracle in Constituent Parsing</title>
      <link href="/2018/08/03/dynamic-oracles/"/>
      <url>/2018/08/03/dynamic-oracles/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=4875075&auto=1&height=66"></iframe></div><p>本文将从定义到证明，一步步理清成分句法分析中用到的Dynamic Oracle函数。参考了James Cross在2016年发表在EMNLP上面的论文：<a href="https://www.aclweb.org/anthology/D/D16/D16-1001.pdf" target="_blank" rel="noopener">论文地址</a>，该论文还是当年的best paper。</p><h1 id="成分句法分析系统"><a href="#成分句法分析系统" class="headerlink" title="成分句法分析系统"></a>成分句法分析系统</h1><hr><p>首先本文用到的成分句法分析系统是基于span-based的转移系统，在这里只做简略介绍，详见<a href="http://ir.library.oregonstate.edu/downloads/0g354j52q" target="_blank" rel="noopener">Parsing with Recurrent Neural Networks</a>。<br><img src="1.png" alt><br>上图展示了该转移系统的转移过程，其中结构化预测只用到了shift(sh)和combine(comb)两种动作，因为stack中存放的是span的左右边界下标，所以comb动作不需要区分左右，这与另一种转移系统的reduce动作不同。而对于label的预测，如果栈首的span不构成短语结点，那么就预测为nolabel，否则就预测为$_i{X_j}$。</p><p>每个时刻的状态用三元组$\left\langle {z,\sigma ,t} \right\rangle $表示，分别表示第几个动作、栈（span的split序列）、当前已生成的结点$_i{X_j}$集合。注意到对于长度为$n$的句子，只需要用$4n-2$个动作就可以分析出句法树了，并且第偶数个动作做结构预测（sh和comb），第奇数个动作做label预测。<br><img src="2.png" alt><br>上图是一个转移的具体例子，下面将全部以这个句子为例进行介绍。注意到多叉树隐式的转化为了二叉树，临时结点预测为nolabel。</p><h1 id="Dynamic-Oracle"><a href="#Dynamic-Oracle" class="headerlink" title="Dynamic Oracle"></a>Dynamic Oracle</h1><hr><p>Dynamic Oracle是Goldberg和Nivre在2013年总结出来的，发表在TACL上面：<a href="https://www.aclweb.org/anthology/Q/Q13/Q13-1033.pdf" target="_blank" rel="noopener">Training Deterministic Parsers with Non-Deterministic Oracles</a>。</p><p>提出的动机就是为了解决测试阶段贪心预测错误导致误差越来越大的问题。在训练的时候，原来的静态Oracle方法就是每一步都严格按照标准树的动作来进行预测，最终拟合得和标准树动作序列相同，但是测试的时候没有标准树了，如果某一步预测错误，可能会到达一个训练中没有出现过的状态，那就会导致之后的预测越来越错。所以就提出了Dynamic Oracle的技巧，在训练过程中的每一步预测，不再局限于标准树中的一个动作，而扩展为一个动作集合，只要采取集合中的动作，那么最终得到的动作序列一定也是最优的。</p><p>这种方法主要用于贪心的预测方法，例如本文的转移系统就是在每一步贪心的预测当前动作，再如之前介绍过的成分句法分析top-down模型<a href="https://www.aclweb.org/anthology/P/P17/P17-1076.pdf" target="_blank" rel="noopener">A Minimal Span-Based Neural Constituency Parser</a>中，自顶向下贪心的选择每一个span的最佳split，也要用到Dynamic Oracle来防止错误扩大。之前的博客有过专门介绍，可以去翻看一下：<a href="https://godweiyang.com/2018/06/28/ACL17-ConParsing/#top-down解码模型">Dynamic Oracle</a></p><p>下面就将从定义、证明等方面来详细阐述Dynamic Oracle。</p><h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><hr><p><strong>定义1：</strong> 定义$c{ \vdash _\tau }c’$为状态$c$经过动作$\tau $之后转移到状态${c’}$，写成函数的形式就是$c’ = \tau ({c})$。另外定义$ \vdash $为所有动作$\tau $的并集，也就是状态$c$经过任意动作之后转移到状态${c’}$。定义${ \vdash ^*}$为$ \vdash $的自反和传递闭包。</p><p><strong>定义2（派生树/可到达树）：</strong> 定义$D({c})$为从状态$c$出发，最终可以产生的句法树的集合，即<br>\[D({c}) = \left\{ {t|c{ \vdash ^*}\left\langle {z,\sigma ,t} \right\rangle } \right\}\]<br>也可以称作“派生树”或者“可到达树”。</p><p><strong>定义3（$F_1$值）：</strong> 定义预测树$t$关于标准树$t_G$的$F_1$值为<br>\[{F_1}(t) = \frac{ {rp}}{ {r + p}}\]<br>其中$r = \frac{ {\left| {t \cap {t_G}} \right|}}{ {\left| { {t_G}} \right|}},p = \frac{ {\left| {t \cap {t_G}} \right|}}{ {\left| t \right|}}$。</p><p><strong>定义4：</strong> 将$F_1$扩展为状态$c$的函数，定义${F_1}({c})$为从状态$c$出发可以产生的$F_1$值最高的句法树的$F_1$值，即<br>\[{F_1}(c) = {\max _{t \in D(c)}}{F_1}(t)\]</p><p><strong>定义5（oracle）：</strong> 定义状态$c$的oracle为使状态$c$转移过后最优$F_1$值不变的动作集合，即<br>\[oracle(c) = \left\{ {\tau |{F_1}(\tau (c)) = {F_1}(c)} \right\}\]<br>至于这个集合该怎么求解，下面将会讲到。</p><p><strong>定义6（span包含）：</strong> span$(i,j)$被span$(p,q)$包含，当且仅当$p \le i &lt; j \le q$，记为<br>\[(i,j) \preceq (p,q)\]</p><p><strong>定义7（严格包含）：</strong> span$(i,j)$被span$(p,q)$严格包含，当且仅当$(i,j) \preceq (p,q)$，并且$(i,j) \ne (p,q)$，记为<br>\[(i,j) \prec (p,q)\]<br>同样可以将偏序关系从span扩展到类别，即$_i{X_j}{ \prec _p}{Y_q}$，当且仅当$(i,j) \prec (p,q)$。</p><p><strong>定义8（可到达类别）：</strong> 对于任意状态$c = \left\langle {z,\sigma |i|j,t} \right\rangle $，定义它的可到达类别集合为<br>\[reach(c) = left(c) \cup right(c)\]<br>其中左右可到达类别集合又分别定义为<br>\[\begin{array}{l}left(c) = \left\{ {_p{X_q} \in {t_G}|(i,j) \prec (p,q),p \in \sigma |i} \right\}\\right(c) = \left\{ {_p{X_q} \in {t_G}|p \ge j} \right\}\end{array}\]<br>光看定义可能有点生涩，通俗理解就是，$left(c)$为标准树中包含span$(i,j)$的类别集合，并且类别的左端点与栈中的span没有交叉，也就是说类别的左端点就是栈中除了$j$以外的其余split中的某一个。而$right(c)$为标准树中还处于队列中没有进栈的类别集合。<br><img src="3.png" alt><br>如上图所示，还以之前的句法树为例，现在的状态为$\left\langle {10,[0,1,2,4],\left\{ {_0N{P_1}} \right\}} \right\rangle $，此时的栈顶span$(i,j)=(2,4)$，也就是红色梯形部分，那么$left(c)$就是深蓝色类别，$right(c)$就是天蓝色类别。而灰色类别因为与红色类别交叉了，所以属于不可到达类别，而标准树中还有一个类别$_0N{P_1}$由于已经被识别出来了，所以也属于不可到达类别。</p><p>上面定义是基于动作序号为偶数的情况，而对于动作序号为奇数的情况，也就是预测label的动作，只需要将偏序$\prec$修改为$\preceq$即可，因为转移过后span依然是本身，所以不是严格包含关系。</p><p>特殊情况（初始值）：<br>\[reach(\left\langle {0,[0],\emptyset } \right\rangle ) = {t_G}\]<br>很显然，初始时$t_G$中所有类别都属于$right(c)$。</p><p>最后需要注意的一点是，根据以上定义有<br>\[reach(c) \cap t = \emptyset ,reach(c) \subseteq {t_G} - t\]<br>这一点也是很显然的，$left(c)$都是严格包含span$(i,j)$的，所以与$t$不存在交集，而$right(c)$在队列里，更不可能存在交集，观察上面的例子会更加好理解。</p><p><strong>定义9（next类别）：</strong> 对于任意状态$c = \left\langle {z,\sigma |i|j,t} \right\rangle $，上面已经定义了它的可到达类别集合，最后再定义它的下一个可到达类别为严格包含span$(i,j)$的可到达类别集合（即$left(c)$）中偏序关系最小的类别<br>\[next(c) = {\min _ \prec }left(c)\]</p><h1 id="结构化和label-Oracles"><a href="#结构化和label-Oracles" class="headerlink" title="结构化和label Oracles"></a>结构化和label Oracles</h1><hr><p>对于任意动作序号为偶数的状态$c = \left\langle {z,\sigma |i|j,t} \right\rangle $，记$next(c){ = _p}{X_q}$，那么定义它的结构化Dynamic Oracle为<br><img src="4.png" alt><br>也就是使当前状态向着标准树中最接近它的状态$next(c)$转移，如果$p = i,q &gt; j$，那么应该在移进栈里一些单词；如果$p &lt; i,q = j$，那么不能再移进了，而应该在栈里combine两个span；如果$p &lt; i,q &gt; j$，那么移进或者归约都可以，反正总能达到前两种状态。</p><p>特殊情况（初始值）：<br>\[dyna(\left\langle {0,[0],\emptyset } \right\rangle ) = \{ sh\} \]<br>即使当前预测的span是错的，也可以经过Dynamic Oracle指导，几步之后预测到正确的$next(c)$。而如果没有Dynamic Oracle，可能就一直错下去了。<br><img src="5.png" alt><br>上图是几种任意状态的Dynamic Oracle示例，除了第一种之外，其余三个都是预测错误的，如果没有Dynamic Oracle，甚至都不知道下一步转移的动作是什么。</p><p><strong>引理1：</strong> 对于任意状态$c$，任意动作$\tau  \in dyna(c)$，有<br>\[reach(\tau (c)) = reach(c)\]<br>而对于任意动作$\tau  \notin dyna(c)$，有<br>\[reach(\tau (c)) \not\subset reach(c)\]</p><p>最后是label Dynamic Oracle，这个就很简单了，如果span$(i,j)$出现在了标准树中，那么预测类别就行了，否则的话预测为nolabel：<br><img src="6.png" alt></p><h1 id="正确性证明"><a href="#正确性证明" class="headerlink" title="正确性证明"></a>正确性证明</h1><hr><p>主要证明两点内容：</p><ul><li>首先定义一个特殊的树$t^*{(c)}$，下面会证明它是从状态$c$开始可以得到的得分最高的树。</li><li>然后证明从状态$c$开始按照Dynamic Oracle策略，确实可以得到最优树$t^*{(c)}$。</li></ul><p><strong>定义10（$t^*{(c)}$）：</strong> 对于任意状态$c = \left\langle {z,\sigma,t} \right\rangle $，定义最优树$t^*{(c)}$为$c$中的子树$t$并上当前状态可到达的类别集合，也就是<br>\[{t^*}(c) = t \cup reach(c)\]<br>下面我们会证明，$t^*{(c)}$的确是当前状态可以得到的得分最高的树。<br><img src="7.png" alt><br>上图形象的说明了几种树之间的关系。当前子树$t$与标准树$t_G$不一定完全重合，可能有预测错误的，所以是交叉的。那么接下来的预测如果全部预测为标准树中的$reach(c)$，那么得分一定是最高的。而剩余的白色部分就是与$t$的span产生交叉的类别，属于不可到达的。</p><p><strong>引理2：</strong> 对于任意状态$c$，最优树$t^*{(c)}$一定是$c$的派生树，也就是<br>\[{t^*}(c) \in D(c)\]</p><p><strong>定理1：</strong> 对于任意状态$c$，有<br>\[{F_1}({t^*}(c)) = {F_1}(c)\]</p><p>也就是说最优树$t^*{(c)}$的得分一定是当前状态可以得到的最高分数。</p><p>证明也很简单，根据召回率和准确率公式，最优树$t^*{(c)}$是在$t$的基础上加入了所有的标准树中的可到达类别$reach(c)$，所以召回率分子不会降下来，召回率不可能更高了；同时并没有加入任何不在标准树中的类别，所以准确率的分母也不可能减小，准确率也不会更高了。因此$t^*{(c)}$就是当前状态可以得到的最优树。</p><p><strong>推论1：</strong> 对于任意状态$c = \left\langle {z,\sigma,t} \right\rangle $，对任意$t’ \in D(c),t’ \ne {t^*}(c)$，都有<br>\[{F_1}(t’) &lt; {F_1}(c)\]<br>上面已经证明了$t^*{(c)}$是最优树，所以自然其余的树得分都比它低了。</p><p>最后需要证明的一点就是，按照Dynamic Oracle策略进行转移，一定能到达这个最优树吗？</p><p><strong>引理3：</strong> 对于任意状态$c = \left\langle {z,\sigma,t} \right\rangle $，对任意动作$\tau  \in dyna(c)$，都有<br>\[{t^*}(\tau (c)) = {t^*}(c)\]<br>反之如果$\tau  \notin dyna(c)$，那么有<br>\[{t^*}(\tau (c)) \ne {t^*}(c)\]<br>原文并没有给出证明，粗略理解的话，按照Dynamic Oracle策略，下面应该向着$next(c)$这个类别靠近，而在这个过程中，包含在$next(c)$内的$right(c)$都会被sh动作识别，而其余不在标准树中的类别都会被识别为nolabel，$next(c)$又是第一个$left(c)$，所以所有的$reach(c)$都可以被识别，所以这是符合$t^*{(c)}$定义的。</p><p>反之如果不按照Dynamic Oracle策略来转移，下一步产生的span一定会与$next(c)$产生交叉，因此$next(c)$再也无法被包括进最终的句法树中，所以第二点也成立。</p><p>最终综合引理3、定理1和推论1，得到了本文中最关键的结论：</p><p><strong>定理2：</strong> $dyna()$函数符合定义5中的oracle定义，即对于任意状态$c$，有<br>\[dyna(c) = oracle(c)\]</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>至此关于Dynamic Oracle已经全部介绍完了，在黄亮老师的个人主页上面，还有这篇论文的会议视频和ppt，还有github源码，大家可以去深入学习：<a href="http://web.engr.oregonstate.edu/~huanlian/" target="_blank" rel="noopener">Liang Huang</a>。</p><p>当然，在具体实现中，由于在训练集上过早的拟合，单纯使用Dynamic Oracle并没有得到任何效果提升，所以要加入exploration机制，也就是人为的干预动作分类，使模型故意预测错误的动作，这样就能学习到更多的情况了，事实证明这样的确得到了略微提升。PTB上的结果如下：<br><img src="8.png" alt></p><p>最后提一个小疑问，关于引理1，原文说之后定理的证明会用到它，但我没看出来哪里用到了。而且我对它的正确性也有所怀疑，按照Dynamic Oracle转移之后，$reach(c)$不可能一直不变啊，按理说会先不变，再变少，交替变化，最后生成句法树后变为空集。并且原文中引理1符号也出现了一个小错误，我在这里修改正确了。</p><p>关于这一点疑问，我已经发邮件请教了原作者James Cross，他也已经回复我了，更深入的解答不久应该就会告诉我了，到时候我再更新一下。如果大家有想法的话，也可以提出来。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Straight to the Tree： Constituency Parsing with Neural Syntactic Distance</title>
      <link href="/2018/07/19/conparsing-syntactic-distance/"/>
      <url>/2018/07/19/conparsing-syntactic-distance/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=451169473&auto=1&height=66"></iframe></div><p><strong>论文地址：</strong><a href="http://aclweb.org/anthology/P18-1108" target="_blank" rel="noopener">Straight to the Tree: Constituency Parsing with Neural Syntactic Distance</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>今天要讲的这篇论文发表在ACL18上面，一句话概括，本文就是<strong>将句法树序列化，通过预测序列进行句法分析。</strong></p><p>主要思想是通过预测一个实值向量来构造出成分句法树，该实值向量表示的就是成分句法树的所有split，并且按照中序遍历给出，具体细节之后会讲到。这个方法之前没有见过，很有新意，效果也很不错，虽然比不上之前讲的基于span的方法，但是该模型最大的优点就是可以并行，时间复杂度低。</p><p>近些年来，成分句法分析模型大多是通过学习出词和短语的表示，然后用基于转移的或者基于chart的方法进行句法分析，亦或者是上一篇笔记中提到的top-down方法。但是这一类方法都有一些不可避免的缺点，比如基于转移的方法，通过预测转移序列来生成句法分析树，但是一棵句法分析树可能对应着多棵不同的转移序列，所以训练的时候可能产生错误，可以通过动态Oracle技术解决。基于chart的模型缺点就是速度太慢。</p><p>本文提出了一种新的概念叫做“syntactic distance”，以下称作句法距离，这个概念首次提出是2017年一篇语言模型的论文中的，本文将其用在了句法分析中。主要思想是这样的：对于一棵二叉树，它的中序遍历的split序列和二叉树是唯一对应的，所以只需要预测这个split序列就行了，而每个split就是用句法距离来表示。下图就是一棵句法树对应的句法距离：<br><img src="1.png" alt><br>这棵树有两个split，第一个split的高度更高，所以对应的句法距离数值更大。</p><p>最后通过top-down顺序进行解码，解码时间复杂度为$O(n\log n)$。最后模型在PTB上取得了91.8的F1值，CTB上取得了86.5的F1值。</p><h1 id="Syntactic-Distances"><a href="#Syntactic-Distances" class="headerlink" title="Syntactic Distances"></a>Syntactic Distances</h1><hr><p>一棵句法树的句法距离如下定义：<br>对于句法分析树$T$，它的叶子结点也就是句子为$({w_0}, \ldots ,{w_n})$，记叶子结点$w_i,w_j$的最近公共祖先LCA为$\tilde d_j^i$，那么句法树$T$的句法距离定义为任意向量$d = ({d_1}, \ldots ,{d_n})$，并且满足<br>\[sign({d_i} - {d_j}) = sign(\tilde d_i^{i - 1} - \tilde d_j^{j - 1})\]<br>这个定义可能看起来比较难理解，通俗一点讲就是，$({d_1}, \ldots ,{d_n})$中任意一对元素的大小关系和$(\tilde d_1^0, \ldots ,\tilde d_n^{n - 1})$中下标相同的一对元素的大小关系是完全一样的，也就是说，句法距离大小反映的是一个句子两两相邻元素的LCA的高度大小。</p><p>还用上面那张图举个例子，$\tilde d_1^0 = 2,\tilde d_2^1 = 1$，那么它的句法距离$d=(d_1,d_2)$就是满足$d_1&gt;d_2$的任意向量。</p><p>这样就可以将一棵句法树唯一对应到一个句法距离的序列，只要预测这个序列就可以得到句法树了，这比预测span集合更加直接。</p><p>那么训练的时候如何将句法树转化为句法距离呢？这里只考虑二叉树，下面的算法1给出了伪代码，将句法树转化为三元组$(d,c,t)$。其中$d$是两两相邻的叶子结点的LCA的高度向量，可以证明，这和中序遍历得到的结点顺序完全相同。$c$是与之顺序相同的结点的label向量。$t$是叶子结点从左向右的tag标签向量。<br><img src="2.png" alt><br>从算法中可以看出，采用自顶向下递归的形式，叶子结点高度为0，不存在句法距离和label。而内结点的高度等于左右儿子高度较大的一个加1，句法距离为左儿子句法距离拼接上自身句法距离再拼接上右儿子句法距离，label也是如此。</p><p>那么如果得到了一棵句法树的三元组$(d,c,t)$，如何还原出这棵句法树呢？算法2给出了构造方法，其实类似于之前那篇论文的top-down方法。<br><img src="3.png" alt><br>原理很简单，只要在每一步寻找$d$中最大的元素，也就是寻找高度最大的内结点，该内结点对应的下标就是句法树的split，然后对左右子树递归解析就行了。时间复杂度只要$O(n\log n)$，而之前的top-down模型时间复杂度为$O(n^2)$。<br><img src="4.png" alt><br>上图是构造句法树的一个例子，和之前一样，通过$\emptyset$的label隐式的将句法树二叉化了，一元还是处理成新的label。图中的矩形高度就代表了句法距离的大小，可以看出，除了$d_0,d_5$这两个句子开始结束标记的句法距离以外，$d_1$最大，所以句法树的split就是$w_0$和$w_1$，然后对右子树递归分析。</p><p>在子树递归过程中，可以并行计算，理论上时间复杂度可以降到$O(\log n)$，但是句子长度过短的话，是否与cpu通讯时间都要大于这个数量级了呢？这个并行的意义还有待商榷。</p><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><hr><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>下面的问题就是给出一个句子，如何学习出它的三元组$(d,c,t)$呢？</p><p>首先将每个单词的词向量和tag向量拼接作为输入，然后送入一个Bi-LSTM，得到隐含层输出为<br>\[h_0^w, \ldots ,h_n^w = {\rm{BiLST}}{ {\rm{M}}_w}([e_0^w,e_0^t], \ldots ,[e_n^w,e_n^t])\]</p><p>对于每个单词，可能存在一元结点，也可能不存在，不存在的话就标记为$\emptyset$，用一个前馈神经网络和softmax预测每个单词的一元结点：<br>\[p(c_i^w|w) = {\rm{softmax(FF}}_c^w(h_i^w))\]</p><p>为了得到每个split的表示，对两两相邻单词进行卷积：<br>\[g_1^s, \ldots ,g_n^s = {\rm{CONV(}}h_0^w, \ldots ,h_n^w)\]<br>注意输出比输入少一个，因为split数量比单词少一个。</p><p>然后再将输出通过一层Bi-LSTM，得到最终的split表示：<br>\[h_1^s, \ldots ,h_n^s = {\rm{BiLST}}{ {\rm{M}}_s}(g_1^s, \ldots ,g_n^s)\]<br>当然这里也可以选择采用self-attention，详见我的上一篇博客。</p><p>最后将输出通过一个两层前馈神经网络，得到每个split的句法距离值：<br>\[{ {\hat d}_i} = {\rm{F}}{ {\rm{F}}_d}(h_i^s)\]</p><p>每个内结点的label同样用一个前馈神经网络和softmax预测：<br>\[p(c_i^s|w) = {\rm{softmax(FF}}_c^s(h_i^s))\]</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>对于训练样例$\left\{ {\left\langle { {d_k},{c_k},{t_k},{w_k}} \right\rangle } \right\}_{k = 1}^K$，它的损失函数就是$d$和$c$的损失函数之和。</p><p>对于$c$，因为用的是softmax预测的，所以直接用交叉熵即可得到损失${L_{ {\rm{label}}}}$。</p><p>对于$d$，可以用均方误差：<br>\[L_{dist}^{mse} = \sum\limits_i { { {({d_i} - { {\hat d}_i})}^2}} \]<br>然而我们并不在意句法距离的绝对值大小，我们只要它的相对大小是正确的即可，所以均方误差在这里不是很合适，可以换成如下损失函数：<br>\[L_{dist}^{rank} = \sum\limits_{i,j &gt; i} {\max (0,1 - sign({d_i} - {d_j})({ {\hat d}_i} - { {\hat d}_j}))} \]</p><p>最后总的损失函数为：<br>\[L = {L_{ {\rm{label}}}} + L_{dist}^{rank}\]</p><p>下面这张图形象的说明了模型的结构，由下往上，第一层圆圈是单词之间的LSTM，然后每个单词上面的五边形是前馈神经网络用来预测一元label，两个相邻单词之间的三角形是卷积，卷积得到的结果再通过一个LSTM得到split表示，最后每个split上面有两个五边形，一个是前馈神经网络用来预测label，另一个是前馈神经网络用来预测句法距离。<br><img src="5.png" alt></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><p>在PTB数据集上的实验结果如下：<br><img src="6.png" alt><br>可以看出，结果还是很有竞争力的，虽然有很多比本文高的模型，但本文解释了，那是因为他们用了Char-LSTM，用了外部数据，用了半监督方法或者重排序方法。。。目前单模型最高的方法依然是上一篇博客讲的span-based模型。</p><p>最后值得一提的是模型的运行速度，之前理论分析时间复杂度非常低，而实际上运行速度的确快了许多，结果如下：<br><img src="7.png" alt></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>本文最大的创新点就是应用句法距离进行句法分析，并且时间复杂度很低。至于文章题目为什么叫“Straight to the Tree”，文章最后说因为只通过一个最普通的LSTM和卷积就预测出了句法树，所以是很直接的。。。</p><p>本文的模型还比较粗糙，我觉得仍然有许多改进之处：</p><ul><li>可以将LSTM替换为self-attention，因为之前博客讲到了，伯克利的self-attention编码器比LSTM编码器准确率高了1个多的百分点。</li><li>可以尝试最近新出的词向量模型ELMo，也许会有特别大的提升。</li><li>加入词级别的Char-LSTM，可能会有一定提升。</li><li>我觉得split的表示可以更加复杂化一点，而不仅仅是一层卷积+一层LSTM这么简单。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACL </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Constituency Parsing with a Self-Attentive Encoder</title>
      <link href="/2018/07/04/acl18-attconparsing/"/>
      <url>/2018/07/04/acl18-attconparsing/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=493316158&auto=1&height=66"></iframe></div><p><strong>论文地址：</strong><a href="https://arxiv.org/pdf/1805.01052.pdf" target="_blank" rel="noopener">Constituency Parsing with a Self-Attentive Encoder</a><br><strong>代码地址：</strong><a href="https://github.com/nikitakit/self-attentive-parser" target="_blank" rel="noopener">github</a></p><p>今天要介绍的这篇论文是成分句法分析领域目前的state-of-the-art，结果最高的几篇paper可以参见ruder在github整理的列表：<a href="https://github.com/sebastianruder/NLP-progress/blob/master/constituency_parsing.md" target="_blank" rel="noopener">github</a>。<br>下面就是成分句法分析目前排名：</p><table><thead><tr><th>Model</th><th align="center">F1 score</th><th>Paper / Source</th></tr></thead><tbody><tr><td>Self-attentive encoder + ELMo (Kitaev and Klein, 2018)</td><td align="center">95.13</td><td><a href="https://arxiv.org/abs/1805.01052" target="_blank" rel="noopener">Constituency Parsing with a Self-Attentive Encoder</a></td></tr><tr><td>Model combination (Fried et al., 2017)</td><td align="center">94.66</td><td><a href="https://arxiv.org/abs/1707.03058" target="_blank" rel="noopener">Improving Neural Parsing by Disentangling Model Combination and Reranking Effects</a></td></tr><tr><td>In-order (Liu and Zhang, 2017)</td><td align="center">94.2</td><td><a href="http://aclweb.org/anthology/Q17-1029" target="_blank" rel="noopener">In-Order Transition-based Constituent Parsing</a></td></tr><tr><td>Semi-supervised LSTM-LM (Choe and Charniak, 2016)</td><td align="center">93.8</td><td><a href="http://www.aclweb.org/anthology/D16-1257" target="_blank" rel="noopener">Parsing as Language Modeling</a></td></tr><tr><td>Stack-only RNNG (Kuncoro et al., 2017)</td><td align="center">93.6</td><td><a href="https://arxiv.org/abs/1611.05774" target="_blank" rel="noopener">What Do Recurrent Neural Network Grammars Learn About Syntax?</a></td></tr><tr><td>RNN Grammar (Dyer et al., 2016)</td><td align="center">93.3</td><td><a href="https://www.aclweb.org/anthology/N16-1024" target="_blank" rel="noopener">Recurrent Neural Network Grammars</a></td></tr><tr><td>Transformer (Vaswani et al., 2017)</td><td align="center">92.7</td><td><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a></td></tr><tr><td>Semi-supervised LSTM (Vinyals et al., 2015)</td><td align="center">92.1</td><td><a href="https://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf" target="_blank" rel="noopener">Grammar as a Foreign Language</a></td></tr><tr><td>Self-trained parser (McClosky et al., 2006)</td><td align="center">92.1</td><td><a href="https://pdfs.semanticscholar.org/6f0f/64f0dab74295e5eb139c160ed79ff262558a.pdf" target="_blank" rel="noopener">Effective Self-Training for Parsing</a></td></tr></tbody></table><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>本篇论文将之前<a href="http://godweiyang.com/2018/06/28/ACL17-ConParsing/">A Minimal Span-Based Neural Constituency Parser</a>这篇论文中模型的编码器部分由LSTM替换为了Self-Attentive，来捕捉句子单词内容(content)与位置(position)之间的联系。实验结果可以达到93.55%的F1值，如果再加上预训练的词向量ELMo，那么F1值可以提升到95.13%。这是目前效果最好的一个模型了。</p><p>Attention的主要想法借鉴了谷歌的神作：<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is All You Need</a>，这篇论文网上也有很多讲解了，我挑选了一篇讲解比较好的，大家可以先看看：<a href="https://yq.aliyun.com/articles/342508?utm_content=m_39938" target="_blank" rel="noopener">Attention Is All You Need</a>。</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><hr><h2 id="基本模型"><a href="#基本模型" class="headerlink" title="基本模型"></a>基本模型</h2><p>这部分详见我之前写的一篇笔记：<a href="http://godweiyang.com/2018/06/28/ACL17-ConParsing/">A Minimal Span-Based Neural Constituency Parser</a>，解码器部分和之前模型基本一致。本文主要探讨的是编码器的构造，也就是如何求出每个span的向量表示，从而得到span的得分$s(i,j,l)$，然后应用解码器进行解码，生成成分句法分析树。</p><h2 id="词向量表示"><a href="#词向量表示" class="headerlink" title="词向量表示"></a>词向量表示</h2><p>第$t$个单词的词向量由三个部分组成：</p><ul><li>word embdding：$w_t$，这部分可以用随机初始化的向量，也可以用CharLSTM，也可以用预训练的词向量。</li><li>tag embdding：$m_t$</li><li>位置向量：$p_t$</li></ul><p>最终词向量为三部分的加和：<br>\[z_t = w_t + m_t + p_t\]</p><h2 id="Self-Attentive"><a href="#Self-Attentive" class="headerlink" title="Self-Attentive"></a>Self-Attentive</h2><p>模型结构如下图所示：<br><img src="1.png" alt><br>这一部分是不同位置单词互相联系的唯一方式，采用谷歌<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is All You Need</a>中的Self-Attentive模型。</p><p>模型一共由8个SingleHead组成，每个SingleHead结构如下图：<br><img src="2.png" alt><br>计算方式如下：<br>\[{\rm{SingleHead}}(X) = \left[ { {\rm{Softmax}}\left( {\frac{ {Q{K^{\rm{T}}}}}{ {\sqrt { {d_k}} }}} \right)V} \right]{W_o}\]<br>其中$Q = X{W_Q};K = X{W_K};V = X{W_V}$，而$W_O$用来将输出映射到与输入相同的维度。</p><p>详细分析一下计算过程，首先输入矩阵$X = [{z_1},{z_2}, \ldots ,{z_T}]$是由一个句子中所有词向量组成的矩阵，拼接在一起是为了并行，加快计算速度，$X \in {\mathbb{R}^{T \times {d_{ {model}}}}}$。</p><p>然后将$X$映射为三个矩阵，query矩阵$Q$，key矩阵$K$，value矩阵$V$，其中$Q,K \in {\mathbb{R}^{ {d_{ {model}}} \times {d_k}}}$。</p><p>我们想要计算单词$i$和单词$j$之间的Attention大小，可以用两者query向量和key向量元素乘得到：<br>\[p(i \to j) \propto \exp \left( {\frac{ { {q_i} \cdot {k_j}}}{ {\sqrt { {d_k}} }}} \right)\]<br>所有单词的value向量乘以单词$i$对它的Attention值，加权求和之后得到的结果就是单词$i$最后的向量表示：<br>\[{ {\bar v}_i} = \sum\nolimits_j {p(i \to j){v_j}} \]<br>最后乘以$W_O$映射到与输入$X$相同的维度。</p><p>整个过程如果写成矩阵形式就是最开始的那个矩阵式子。注意到式子中$Q{K^{\rm{T}}} \in {\mathbb{R}^{ {\rm{T \times T}}}}$，矩阵中的每个元素恰好就是Attention值$p(i \to j)$。$\sqrt { {d_k}}$是归一化因子。</p><p>最后将8个SingleHead的结果求和得到MultiHead结果，注意这8个SingleHead参数不共享：<br>\[{\rm{MultiHead(}}X) = \sum\limits_{i = 1}^8 { {\rm{SingleHea}}{ {\rm{d}}^{(i)}}(X)} \]</p><p>注意到Attention模型有一个很严重的问题，就是无论单词的顺序是怎么样的，都不影响最终的结果。所以在输入中要添加位置向量$p_t$，否则之后会有实验表明，不加的话效果大大下降。</p><p>图一中还有一个前馈神经网络的部分，使用的是一个双层前馈神经网络：<br>\[{\rm{FeedForward}}(x) = {W_2}{\rm{relu}}({W_1}x + {b_1}) + {b_2}\]</p><h2 id="Span得分"><a href="#Span得分" class="headerlink" title="Span得分"></a>Span得分</h2><p>最终的span得分计算方式如下：<br>\[s(i,j, \cdot ) = {M_2}{\rm{relu}}({\rm{LayerNorm}}({M_1}v + {c_1})) + {c_2}\]<br>其中$v$就是短语的向量表示，由之前的每个单词的向量输出得到：<br>\[v = [{ {\vec y}_j} - { {\vec y}_i},{ {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over y} }_{j + 1}} - { {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over y} }_{i + 1}}]\]<br>在之前的LSTM模型中前向后向表示很容易得到，在这里只能通过将输出向量一分为二，一半作为前向表示，一半作为后向表示，实际实现中，偶数维度作为前向表示，奇数维度作为后向表示。</p><h1 id="模型分析"><a href="#模型分析" class="headerlink" title="模型分析"></a>模型分析</h1><hr><h2 id="Content-vs-Position-Attention"><a href="#Content-vs-Position-Attention" class="headerlink" title="Content vs Position Attention"></a>Content vs Position Attention</h2><p>之前的模型中，我们采用元素加将三个输入向量求和作为输入，期待模型自己训练出它们之间的权重，将它们很好地分开，但是实际上效果并不好。</p><p>下面论文做了许多实验来探讨content和position的重要性。</p><p>首先修改模型输入，令$Q=PW_Q,K=PW_K$，也就是丢弃了content信息，但是最后结果只下降了0.27个百分点，说明了content信息对模型影响不是很大。</p><p>然后为了验证是不是元素加导致content和position信息混合在一起模型无法分开，实验将输入向量显示分开，输入改为<br>\[z_t=[w_t+m_t;p_t]\]<br>但是实验结果只下降了0.07个百分点，说明不是这个因素导致的。事实上元素加和拼接操作在高维度上面是相似的，特别是之后立即乘上了一个矩阵，这就会混合里面的信息。</p><p>所以最好的解决办法就是将content和position向量分开计算attention，最后求和。这样attention矩阵就可以表示为：<br>\[QK^{\rm{T}} = {Q_c}K_c^{\rm{T}} + {Q_p}K_p^{\rm{T}}\]<br>这时的权重矩阵$W$就可以写为<br>\[W = \left[ {\begin{array}{*{20}{c}}{ {W_c}}&amp;0\\0&amp;{ {W_p}}\end{array}} \right]\]</p><p>通过将content和position信息分开，模型的效果从92.67%提升到了93.15%，模型示意图如下：<br><img src="3.png" alt></p><p>最后的实验在测试阶段的8层模型中，每一层手动选择采不采用content或者position attention，实验结果如下：<br><img src="4.png" alt><br>可以发现，不用position信息的话结果大大下降，接近传统的CKY算法，这也说明了普通的CKY算法是无法捕捉到全局的信息的。还有就是content信息主要作用在最后几层，这也说明了前面几层有点类似于扩张卷积网络。</p><h2 id="窗口Attention"><a href="#窗口Attention" class="headerlink" title="窗口Attention"></a>窗口Attention</h2><p>这一部分也不是什么新鲜玩意了，谷歌的论文中也有提到，主要思想就是限制attention的范围，每个单词只与周围窗口大小内的单词进行计算。在本文中还提出一个relaxed变体，就是除了窗口大小范围外，再加入首尾各两个单词进行attention操作。</p><p>如果只在测试阶段进行窗口attention的话，实验结果如下：<br><img src="5.png" alt><br>可以看出，首尾的4个单词对模型效果有很大的影响，如果加上的话，即使窗口很小，效果下降也不会很多。</p><p>然后如果训练和测试阶段都采用窗口attention，结果如下：<br><img src="6.png" alt><br>这时结果下降反而不是很明显了，其实模型的8层就类似于卷积操作，假设窗口大小为10，那么经过8层计算之后，窗口其实可以覆盖到长度为80的句子，这已经足够了，所以性能没有下降太多不足为奇。</p><h1 id="模型的一些改进"><a href="#模型的一些改进" class="headerlink" title="模型的一些改进"></a>模型的一些改进</h1><hr><h2 id="Subword-Features"><a href="#Subword-Features" class="headerlink" title="Subword Features"></a>Subword Features</h2><p>可以尝试加入CharLSTM来代替随机初始化的tag embdding，并且可以直接删除随机初始化的word embdding，只保留tag embdding和位置向量，效果反而能有提升。</p><p>受到其他工作的启发，还可以将每个单词的前后各8个字符向量拼接起来作为输入，但是实验结果不如CharLSTM。</p><p>上面的实验结果如下图所示：<br><img src="7.png" alt><br>可以看出，采用CharLSTM输出作为tag embdding，并且不采用word embdding的效果最好。</p><h2 id="外部词向量"><a href="#外部词向量" class="headerlink" title="外部词向量"></a>外部词向量</h2><hr><p>如果采用另一项关于词向量的工作成果ELMo，将其预训练的结果作为word embdding，实验结果可以更高。因为该词向量已经很好的学习到了全局的信息，所以模型可以减少到4层，效果比8层更好，结果如下：<br><img src="8.png" alt><br>F1值大大提高，达到了惊人的95.21%。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><hr><p>在PTB数据集上，无论是单模型还是多模型或者加入外部词向量，本文的模型结果都是近来最好的，单模型93.55%，多模型95.13%，对比结果如下：<br><img src="9.png" alt><br>本文开头已经提到了最高的几个排名，大家可以去看看其他的方法学习学习。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>之前的工作大多数围绕解码器的算法优化，但是本文对编码器进行了改进，使其能更好的捕捉全局信息。</p><p>同时提出了几点重要的改进：</p><ul><li>subword的信息（CharLSTM）和预训练的词向量非常重要。</li><li>将content和position信息分开可以提升实验结果。</li></ul><p>同时在阅读<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is All You Need</a>的同时，我也发现了谷歌的attention模型其实还是有很多问题的，例如模型本身无法捕捉位置信息，需要加入position embdding来表示位置信息，但这只是临时应付措施，今后工作可以探讨更好的解决方案。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACL </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What&#39;s Going On in Neural Constituency Parsers? An Analysis</title>
      <link href="/2018/07/03/naacl18-conparsing/"/>
      <url>/2018/07/03/naacl18-conparsing/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=27890306&auto=1&height=66"></iframe></div><p><strong>论文地址：</strong><a href="https://arxiv.org/pdf/1804.07853.pdf" target="_blank" rel="noopener">What’s Going On in Neural Constituency Parsers? An Analysis</a><br><strong>代码地址：</strong><a href="https://github.com/dgaddy/parser-analysis" target="_blank" rel="noopener">github</a></p><p><img src="1.jpg" alt></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>最近几年，成分句法分析的方法发生了巨大的变化。传统的有基于PCFG的CKY算法，最近几年随着神经网络的兴起又产生了基于转移的方法、CRF句法分析、重排序方法等等。</p><p>本文是伯克利大学在NAACL18提出的一种基于神经网络的句法分析方法，和传统的方法不同的是，完全不使用语法规则和词汇特征，只需要用神经网络学习出短语的表示即可。通过实验可以发现，神经网络的确也很好的隐式学习出了传统的方法显式用到的语法规则和其他一些特征。在PTB数据集上，该方法达到了92.08%的F1值，这也直接超过了传统的大多数方法。而在下一篇伯克利的ACL18论文中，他们提出的基于Multi-Head Attention的成分句法分析方法更是达到了95.13%的F1值。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>在过去几年，成分句法分析方法发生了巨大的变化。最大的变化就是语法规则和人工的词汇特征变得越来越不那么重要，取而代之的用循环神经网络学习短语的表示，但是还没有什么人对这种神经网络表示的有效性做研究分析。本文提出了一种用RNN学习表示用来句法分析的模型，并对模型的各个方面进行了分析。</p><h1 id="句法分析模型"><a href="#句法分析模型" class="headerlink" title="句法分析模型"></a>句法分析模型</h1><hr><h2 id="模型总览"><a href="#模型总览" class="headerlink" title="模型总览"></a>模型总览</h2><p>和传统的CKY算法一样，该模型还是采用动态规划的算法求解每个短语的最大得分对应的类别以及分割点。只不过这里的短语得分不再是产生式概率之积，而是用RNN训练出来的短语得分。</p><p>用$s(i,j,l)$表示短语$x_{ij}$类别为$l$的得分，句法树$T$的得分表示为所有产生式的得分总和：<br>\[s(T) = \sum\limits_{(i,j,l) \in T} {s(i,j,l)} \]</p><p><strong>值得一提的是，该句法分析模型不需要句法树是二叉树，所以可以直接分析出$n$叉树结构。</strong></p><p>模型的任务依然是寻找得分最高的句法分析树：<br>\[\hat T = \mathop {\arg \max }\limits_T s(T)\]</p><p>短语得分$s(i,j,l)$的计算可以通过三个部分实现：单词表示、短语表示、短语类别得分。</p><h2 id="单词表示"><a href="#单词表示" class="headerlink" title="单词表示"></a>单词表示</h2><p>首先采用Char-BiLSTM训练出每个单词的字符级别的表示，这种表示方法已经很常见了，优点是可以捕捉到单词的前缀后缀等信息，还可以解决未登录词的表示问题。</p><p>最后将字符级别的词表示和预训练的词向量拼接起来作为最终的单词表示。也可以用外部序列标注器预测出单词的词性，并拼接上去作为输入，但是在这里只要Char-BiLSTM训练的足够好，就不需要词性标注了。</p><h2 id="短语表示"><a href="#短语表示" class="headerlink" title="短语表示"></a>短语表示</h2><p>对句子单词序列跑一遍双向LSTM，得到每个单词的前向后向上下文表示${ {\bf{f}}_i}$和${ {\bf{b}}_i}$，然后对于短语$x_{ij}$，用两者的上下文表示的差值拼接起来作为该短语的向量表示：<br>\[{ {\bf{r}}_{ij}} = [{ {\bf{f}}_j} - { {\bf{f}}_i},{ {\bf{b}}_i} - { {\bf{b}}_j}]\]<br>图1是一个具体的例子：<br><img src="2.jpg" alt></p><h2 id="短语类别得分"><a href="#短语类别得分" class="headerlink" title="短语类别得分"></a>短语类别得分</h2><p>最后将短语表示输入到一个单层前馈神经网络中，输出得分向量，其中得分向量每个维度就对应了每个类别的概率。具体计算公式如下：<br>\[s(i,j,l) = {[{ {\bf{W}}_2}ReLU({ {\bf{W}}_1}{ {\bf{r}}_{ij}} + { {\bf{z}}_1}) + { {\bf{z}}_2}]_l}\]</p><h2 id="句法分析"><a href="#句法分析" class="headerlink" title="句法分析"></a>句法分析</h2><p>虽然现在是$n$叉树，但是依然可以用CKY算法来进行句法分析。这时候就需要定义一个虚拟短语类别$\emptyset$，表示实际上不能构成短语的临时短语。例如对于产生式$A \to BCD$，分析的时候可以将其分析为$A \to \emptyset D,\emptyset  \to BC$。定义虚拟短语类别的得分为0，即$s(i,j,\emptyset ) = 0$。</p><p>定义${s_{best}}(i,j)$为短语$x_{ij}$得分最高的句法分析树的得分。首先对长度为1的短语即单词进行初始化：<br>\[{s_{best}}(i,i + 1) = \mathop {\max }\limits_l s(i,i + 1,l)\]<br>然后对于短语$x_{ij}$，运用动态规划算法计算得分最高的句法分析树：<br>\[{s_{best}}(i,j) = \mathop {\max }\limits_l s(i,j,l) + \mathop {\max }\limits_k [{s_{best}}(i,k) + {s_{best}}(k,j)]\]<br>注意到这里对类别的预测和分割点的预测是分开的，短语的最高分数就是最优子树的每个结点的类别得分之和。</p><p>自底向上进行计算，最终整个句子的最高得分就是${s_{best}}(0,n)$。然后自顶向下回溯得到整个句子的句法分析树，注意如果回溯遇到了虚拟短语类别$\emptyset$，就直接忽略它，继续往下回溯。</p><p>整个算法的时间复杂度为$O({n^3} + L{n^2})$，因为没有用到语法规则，所以$n^3$的常数系数$\left| G \right|$就没有了，复杂度大大降低！</p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>损失函数依然使用的是max-margin，即<br>\[\mathcal L = \max \left( {0,\mathop {\max }\limits_T [s(T) + \Delta (T,{T^*})] - s({T^*})} \right)\]<br>这里提到了一点加快计算的方法，${\Delta (T,{T^*})}$可以融入到动态规划算法中一起计算，即将短语类别得分$s(i,j,l)$替换为$s(i,j,l) + 1[l \ne l_{ij}^*]$，其中$l_{ij}^*$是标准树中短语$x_{ij}$的类别。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>经过10轮训练之后，模型在验证集上的F1值达到了92.22%，在测试集上的F1值为92.08%。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
            <tag> NAACL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Minimal Span-Based Neural Constituency Parser</title>
      <link href="/2018/06/28/acl17-conparsing/"/>
      <url>/2018/06/28/acl17-conparsing/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=41500990&auto=1&height=66"></iframe></div><p>论文地址：<a href="https://www.aclweb.org/anthology/P/P17/P17-1076.pdf" target="_blank" rel="noopener">ACL17</a><br>代码地址：<a href="https://github.com/mitchellstern/minimal-span-parser" target="_blank" rel="noopener">github</a></p><p>今天要分享的是伯克利2017年发表在ACL的一篇成分句法分析论文，论文和代码地址都已经放在上面了，代码里还给出了处理过的PTB数据集，使用起来非常方便。</p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>本文提出了一种不同于传统方法的成分句法分析方法。</p><p>传统的句法分析器需要预处理出语法规则集合，然后利用语法规则来进行各种句法分析，这类方法的弊端有很多，我列举了主要的三点：</p><ul><li>语法规则集合构造的好坏直接影响到分析效果的好坏。</li><li>不仅如此，利用语法规则的方法时间复杂度高，因为每次都要遍历一遍语法规则集合来决定每个短语采用哪一条语法规则。</li><li>还有一种弊端就是利用语法规则的方法无法产生新的产生式，也就是说如果测试集中的语法规则没有在训练集中出现过，那么是无法预测出来的。</li></ul><p>而本文提出的模型不需要预先构造出语法规则集合，只需要预测出每个短语的label和split就行了，这样就能构造出一棵完整的句法树。</p><p>该模型分为编码与解码两部分，其中编码部分就是利用双向LSTM将每个词和短语表示成向量，解码部分提出了两种模型，一种是chart模型，类似于CKY算法，另一种是top-down模型，就是自顶向下的贪心算法，具体模型之后介绍。</p><h1 id="编码模型"><a href="#编码模型" class="headerlink" title="编码模型"></a>编码模型</h1><hr><p>一棵句法分析树可以看做是$(label, span)$的集合，也就是句法树中的每一个结点的类别是label，该结点对应的短语在句子中的下标范围就是span。所以模型的任务就是要预测这个集合，给每一个label和span一个得分，找出使得一个句子得分最高的$(label, span)$集合即可。</p><p>那么我们的编码模型任务就是要得出每一个短语的表示，并将其转换为短语的label得分和span得分。这里用到的就是最简单的双向LSTM，对于句子的第$i$个位置，得到它的双向表示$f_i$和$b_i$，那么$span(i,j)$就可以表示为$[f_j-f_i,b_i-b_j]$。</p><p>然后将短语表示输入到两个单独的单层前馈神经网络中，就能分别得到label得分和span得分了。假设用$s_{ij}$表示$span(i,j)$的短语表示，那么label得分和span得分可以分别表示为：<br>\[\begin{array}{*{20}{l}}{ {s_{labels}}(i,j) = {V_l}g({W_l}{s_{ij}} + {b_l})}\\\ { {s_{span}}(i,j) = {v_s}^{\rm{T}}g({W_s}{s_{ij}} + {b_s})}\end{array}\]<br>注意到这里计算出来的label得分是一个向量，维数为label的类别数，而span得分计算出来就是一个标量了。而对于某一个特定的类别，它的得分就可以直接从label得分向量中取出对应的那一维就行了：</p><p>\[{s_{label}}(i,j,l) = {[{s_{labels}}(i,j)]_l}\]</p><p>还有个重要的问题就是一元和$n$元的产生式怎么处理，对于一元产生式，可以将所有的类别合并为一个新的类别，然后加入类别集合中共同预测就行了，在实现代码中，将一元的产生式链上面的类别合并成了一个元组作为这棵子树的label。对于$n$元的产生式，可以添加一个临时类别$\emptyset $，相当于进行了二叉化，所有的新增节点全部预测为$\emptyset $。</p><h1 id="chart解码模型"><a href="#chart解码模型" class="headerlink" title="chart解码模型"></a>chart解码模型</h1><hr><p>chart模型本质上就是一个动态规划算法，类似于CKY算法。</p><p>首先一棵句法树的总得分可以表示为组成它的$(label, span)$集合的label得分与span得分之和：<br>\[{s_{tree}}(T) = \sum\limits_{(l,(i,j)) \in T} {[{s_{label}}(i,j,l) + {s_{span}}(i,j)]} \]<br>我们目的就是寻找使得该式最大的集合$T$，利用动态规划可以将时间复杂度降到$O(n^3)$。</p><p>对于叶子结点的情况，因为没有split，所以我们只需要预测最大得分的label就行了：<br>\[{s_{best}}(i,i + 1) = \mathop {\max }\limits_l [{s_{label}}(i,i + 1,l)]\]<br>而对于一般的$span(i,j)$，我们不仅要预测label，还得预测split。对于split$k$，我们可以将split得分表示为：<br>\[{s_{split}}(i,k,j) = {s_{span}}(i,k) + {s_{span}}(k,j)\]<br>那么最大得分可以表示为：<br>\[{s_{best}}(i,j) = \mathop {\max }\limits_l [{s_{label}}(i,j,l)] + \mathop {\max }\limits_k [{s_{split}}(i,k,j) + {s_{best}}(i,k) + {s_{best}}(k,j)]\]<br>这样就可以对label和split单独预测，在实际代码实现中，去掉了${s_{split}}(i,k,j)$这一部分，也就是只预测label得分之和最高的split。这样做的一个好处就是防止了二叉化过程中，从左边开始合并和从右边开始合并得到的分数不一样，从而导致偏差，另外加上这部分效果提升也不大，所以为了简便就删掉了。</p><p>训练的话采用的还是Max-Margin：<br>\[max\left( {0,\Delta (\hat T,{T^*}) - {s_{tree}}({T^*}) + {s_{tree}}(\hat T)} \right)\]<br>至于句法树差异${\Delta (\hat T,{T^*})}$，可以方便的将${s_{label}}(i,j,l)$替换为${s_{label}}(i,j,l) + {\bf{1}}(l \ne l_{ij}^*)$，其中$l_{ij}^*$就是$span(i,j)$在标准树中的label。</p><h1 id="top-down解码模型"><a href="#top-down解码模型" class="headerlink" title="top-down解码模型"></a>top-down解码模型</h1><hr><p>top-down模型其实就是自顶向下贪心的选择每一个短语的最大label和split。</p><p>其中叶子结点处依然还是直接找得分最高的那一维：<br>\[\hat l = \mathop {arg\max }\limits_l [{s_{label}}(i,i + 1,l)]\]<br>对于一般的$span(i,j)$，直接贪心的寻找得分最高的label和split就行了：<br>\[\begin{array}{l}\hat l = \mathop {\arg \max }\limits_l [{s_{label}}(i,j,l)]\\\hat k = \mathop {\arg \max }\limits_k [{s_{split}}(i,k,j)]\end{array}\]<br>虽然这种贪心的方法看上去并不十分科学，但是实际效果却比动态规划算法还要好一点，并且它的时间复杂度只有$O(n^2)$。</p><p>下面是top-down模型进行解析的一个例子：<br><img src="1.png" alt><br>其中$\emptyset $在构造句法树的时候就直接忽略，最后可以还原成$n$元的产生式。并且一元产生式$S \to VP$被直接替换为了新的类别$S-VP$。</p><p>训练过程类似，对标准树中的每一个$span(i,j)$，分别计算label和split的loss就行了：<br>\[\begin{array}{l}\max \left( {0,1 - {s_{label}}(i,j,{l^*}) + {s_{label}}(i,j,\hat l)} \right)\\\max \left( {0,1 - {s_{split}}(i,{k^*},j) + {s_{split}}(i,\hat k,j)} \right)\end{array}\]<br>最后累加求出总的loss即可。</p><p><strong>动态Oracle</strong><br>top-down模型在每一个$span(i,j)$处都计算出得分最高的label和split，然后与标准树对应的$span(i,j)$作比较，计算出loss。但是这样存在一个很严重的问题，就是如果这个预测出来的$span(i,j)$没有出现在标准树中，那么他在标准树中的label和split是什么呢？这时候就要用到这里提到的动态Oracle技术了。</p><p>对于label而言，如果$span(i,j)$出现在标准树中，那么label就是标准树中的label，否则的话就是$\emptyset $。</p><p>对于split而言，定义$b(i,j)$为$span(i,j)$的split集合，因为可能是$n$元的，所以split可能不止一个。如果$span(i,j)$在标准树中，那么$b(i,j)$显然就是标准树中$span(i,j)$的split集合。如果$span(i,j)$不在标准树中，那么就寻找一个标准树中包含$span(i,j)$的最小span，该span的split集合中位于$i,j$之间的split就构成了$b(i,j)$。</p><p>形式化定义为，寻找：<br>\[({i^*},{j^*}) = \min \left\{ {(i’,j’) \in T:i’ \le i &lt; j \le j’} \right\}\]<br>其中这里的最小是定义在区间长度上的偏序关系。所以$b(i,j)$就可以定义为：<br>\[b(i,j) = \left\{ {k \in b({i^*},{j^*}):i &lt; k &lt; j} \right\}\]</p><p>这样对于任意的$span(i,j)$，都能在标准树中找到对应的split集合，然后计算出loss。这样也能解决因为$n$叉树不同的二叉化导致的不同的split产生的问题。在实际的代码中，直接采用了$b(i,j)$集合中最左边的split作为标准树中的split，当然也可以选择得分最高的一个split，不过提升不大没有必要。</p><p>采用动态Oracle有两个好处：</p><ul><li>一个就是上面说到的，训练的时候不需要每次都预测的和标准树一样了，就算不一样也能给出评判标准。</li><li>另一个就是在预测不准的时候，可以给出在该span里的标准树中的split，这样可以将贪心预测从错误中逐渐“拉回正轨”。</li></ul><h1 id="其他的得分计算方法"><a href="#其他的得分计算方法" class="headerlink" title="其他的得分计算方法"></a>其他的得分计算方法</h1><hr><p><strong>Top-Middle-Bottom label得分</strong><br>其实就是将每个span的label拆分为三元组$(top,middle,bottom)$，主要用来应对一元产生式的：</p><ul><li>如果不是一元产生式，那么父结点label就可以写为$(X,\emptyset,\emptyset)$。</li><li>如果产生式为$X \to Y$，那么label可以合并写为$(X,\emptyset,Y)$。</li><li>如果产生式为$X \to {Z_1} \to  \cdots  \to {Z_k} \to Y$，那么label可以合并写为$(X,{Z_1} -  \cdots  - {Z_k},Y)$。</li></ul><p>label的得分也由三部分求和得到：<br>\[{s_{label}}(i,j,({l_t},{l_m},{l_b})) = {s_{top}}(i,j,{l_t}) + {s_{middle}}(i,j,{l_m}) + {s_{bottom}}(i,j,{l_b})\]<br>求最大得分的时候也可以三部分分开求。</p><p><strong>左右span得分</strong><br>其实就是在计算split得分时，将左右span的得分区别为left和right两部分：<br>\[{s_{split}}(i,k,j) = {s_{left}}(i,k) + {s_{right}}(k,j)\]</p><p><strong>span连接得分</strong><br>之前计算split得分都是将左右span得分直接相加，当然也可以将他们拼接起来，输入到单层前馈神经网络里，输出作为得分：<br>\[{s_{split}}(i,k,j) = {v^{\rm{T}}}g({W_s}[{s_{ik}};{s_{kj}}] + {b_s})\]</p><p><strong>深度双仿射span得分</strong><br>首先令${h_{ik}} = {f_{left}}({s_{ik}}),{h_{kj}} = {f_{right}}({s_{kj}})$，然后split得分可以计算为：<br>\[{s_{split}}(i,k,j) = h_{ik}^{\rm{T}}{W_s}{h_{kj}} + v_{left}^{\rm{T}}{h_{ik}} + v_{right}^{\rm{T}}{h_{kj}}\]</p><p><strong>结构化label损失</strong><br>对于两个label集合，定义它们之间的结构化Hamming损失为：<br>\[\left| { {l_1}\backslash {l_2}} \right| + \left| { {l_2}\backslash {l_1}} \right|\]<br>这个loss可以被用在之前的训练过程中。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><p>具体代码细节以及超参数设置请参看代码。</p><p>首先实验对不同的得分计算方式以及loss计算方式进行了对比，发现效果最好的chart模型用的是原始label，0-1标签损失，split得分用的是拼接得分，而top-down模型效果最好的是原始label，结构化label损失，split得分用的是左右span得分。</p><p>当然提升都不是很大，实验为了简便，用了最简单原始的设置：原始label，0-1标签损失，split得分用的是直接求和。</p><p>实验对比结果如下图所示：<br><img src="2.png" alt></p><p>在PTB数据集上，实验结果都要好于之前的所有parser，结果如下：<br><img src="3.png" alt></p><p>不仅结果更好，处理速度也有很大提升，chart模型一秒钟能处理20.3句话，top-down模型一秒钟能处理75.5句话。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>近些年来，效果最好的成分句法分析器基本都是基于转移系统的，还有诸如基于CRF之类的句法分析器。本文提出的基于span表示与得分，从而进行chart解析或者top-down解析的模型是当时结果最好的模型。而且该模型非常的简单，不再需要复杂的语法规则。模型仍然有很多改进之处，体现在span表示的计算方式，各种得分的计算方式。在下一篇博客中，我将为大家介绍一篇伯克利最新的成分句法分析论文，使用的是自注意力机制的编码器，F1值达到了惊人的95.15%。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACL </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EOJ3006. 计算多项式的系数II</title>
      <link href="/2018/06/05/eoj3006/"/>
      <url>/2018/06/05/eoj3006/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=109174&auto=1&height=66"></iframe></div><p><strong>题目链接：<a href="https://acm.ecnu.edu.cn/problem/3006/" target="_blank" rel="noopener">EOJ3006</a></strong></p><h1 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h1><hr><p>给定一个多项式${(ax + by)^k}$，计算多项式展开后${x^n}{y^m}$项的系数，结果对1000000007取模。</p><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><hr><p>由二项式定理可以得知，${x^n}{y^m}$项的系数就是<br>\[{a^n}{b^m}C_k^n\]<br>然后再对1000000007取模，其中${a^n}{b^m}$取模很方便，用快速幂就行了，剩下的问题就是如何求解<br>\[C_k^n\bmod p\]<br>这里由于$n,k$都不是很大，所以直接采用组合数计算公式求出答案，再进行取模就行了。</p><p>拓展一下，如果$n,k$都是小于$10^9$的，那么就不能直接计算了。</p><p>这时候要用到一个大组合数取模的定理，叫做lucas定理：</p><blockquote><p><strong>定理：</strong><br>对于组合数取模<br>\[C_n^m\bmod p\]<br>其中$p$是质数。<br>如果令$n=sp+q,m=tp+r. (q,r&lt;p)$<br>那么有<br>\[C_{sp + q}^{tp + r} \equiv C_s^tC_q^r\bmod p\]<br><strong>证明:</strong><br>\[\begin{array}{l}{(1 + x)^n} \equiv {(1 + x)^{sp + q}} \equiv {(1 + x)^{sp}}{(1 + x)^q}\\ \equiv {({(1 + x)^p})^s}{(1 + x)^q} \equiv {(1 + {x^p})^s}{(1 + x)^q}\\ \equiv \sum\limits_{i = 0}^s {C_s^i{x^{ip}}} \sum\limits_{j = 0}^q {C_q^j{x^j}} \bmod p\end{array}\]<br>其中$(1 + x)^n$中$x^{tp+r}$项的系数为$C_{sp + q}^{tp + r}$，而在同余号右边$x^{tp+r}$项的系数只能为$C_s^tC_q^r$。<br>因为假设<br>\[tp + r = ip + j\]<br>所以<br>\[(t - i)p = j - r\]<br>而<br>\[ - p &lt;  - r \le j - r \le q - r &lt; p - r \le p\]<br>所以只能是<br>\[t - i = 0,j - r = 0\]<br>所以<br>\[C_{sp + q}^{tp + r} \equiv C_s^tC_q^r\bmod p\]</p></blockquote><p>在代码实现中，应用lucas定理之后，将$C_n^m$替换为$C_s^t$继续调用lucas定理即可。递归终止条件是$t=0$。</p><p>最后计算$C_q^r\bmod p$时直接应用组合数定义即可：<br>\[C_q^r \equiv \frac{ {q!}}{ {r!(q - r)!}} \equiv q!{(r!(q - r)!)^{p - 2}} \equiv q!{(r!(q - r)!\bmod p)^{p - 2}}\bmod p\]<br>这里还用到了逆元：<br>\[{a^{ - 1}} \equiv {a^{p - 2}}\bmod p\]<br>证明详见我的另一篇博客：<a href="http://godweiyang.com/2018/05/14/concrete-math-12/">具体数学-第12课</a>中的费马小定理和欧拉定理。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><hr><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;bits/stdc++.h></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span class="token keyword">typedef</span> <span class="token keyword">long</span> <span class="token keyword">long</span> LL<span class="token punctuation">;</span><span class="token keyword">const</span> <span class="token keyword">int</span> MAXN <span class="token operator">=</span> <span class="token number">1000000</span> <span class="token operator">+</span> <span class="token number">10</span><span class="token punctuation">;</span><span class="token keyword">const</span> LL MOD <span class="token operator">=</span> <span class="token number">1e9</span> <span class="token operator">+</span> <span class="token number">7</span><span class="token punctuation">;</span>LL f<span class="token punctuation">[</span>MAXN<span class="token punctuation">]</span><span class="token punctuation">;</span>LL <span class="token function">QuickPow</span><span class="token punctuation">(</span>LL a<span class="token punctuation">,</span> LL n<span class="token punctuation">,</span> LL p<span class="token punctuation">)</span> <span class="token punctuation">{</span>    LL res <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>n <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>n <span class="token operator">&amp;</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            res <span class="token operator">=</span> <span class="token punctuation">(</span>res <span class="token operator">*</span> a<span class="token punctuation">)</span> <span class="token operator">%</span> p<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        a <span class="token operator">=</span> <span class="token punctuation">(</span>a <span class="token operator">*</span> a<span class="token punctuation">)</span> <span class="token operator">%</span> p<span class="token punctuation">;</span>        n <span class="token operator">>>=</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> res<span class="token punctuation">;</span><span class="token punctuation">}</span>LL <span class="token function">C</span><span class="token punctuation">(</span>LL n<span class="token punctuation">,</span> LL m<span class="token punctuation">,</span> LL p<span class="token punctuation">)</span> <span class="token punctuation">{</span>    LL res <span class="token operator">=</span> f<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token function">QuickPow</span><span class="token punctuation">(</span><span class="token punctuation">(</span>f<span class="token punctuation">[</span>m<span class="token punctuation">]</span> <span class="token operator">*</span> f<span class="token punctuation">[</span>n<span class="token operator">-</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">%</span> p<span class="token punctuation">,</span> p <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">,</span> p<span class="token punctuation">)</span> <span class="token operator">%</span> p<span class="token punctuation">;</span>    <span class="token keyword">return</span> res<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">init</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    f<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> MAXN<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        f<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>f<span class="token punctuation">[</span>i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> i<span class="token punctuation">)</span> <span class="token operator">%</span> MOD<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token function">init</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> T<span class="token punctuation">;</span>    <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>T<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> t <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> t <span class="token operator">&lt;</span> T<span class="token punctuation">;</span> <span class="token operator">++</span>t<span class="token punctuation">)</span> <span class="token punctuation">{</span>        LL a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> k<span class="token punctuation">,</span> n<span class="token punctuation">,</span> m<span class="token punctuation">;</span>        <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%lld%lld%lld%lld%lld"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>a<span class="token punctuation">,</span> <span class="token operator">&amp;</span>b<span class="token punctuation">,</span> <span class="token operator">&amp;</span>k<span class="token punctuation">,</span> <span class="token operator">&amp;</span>n<span class="token punctuation">,</span> <span class="token operator">&amp;</span>m<span class="token punctuation">)</span><span class="token punctuation">;</span>        LL res <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token function">QuickPow</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> n<span class="token punctuation">,</span> MOD<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token function">QuickPow</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> m<span class="token punctuation">,</span> MOD<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">%</span> MOD<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token function">C</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> n<span class="token punctuation">,</span> MOD<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">%</span> MOD<span class="token punctuation">;</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"case #%d:\n%lld\n"</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> res<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> EOJ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EOJ2854. 统计特定字串模式的个数</title>
      <link href="/2018/06/05/eoj2854/"/>
      <url>/2018/06/05/eoj2854/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=461525011&auto=1&height=66"></iframe></div><p><strong>题目链接：<a href="https://acm.ecnu.edu.cn/problem/2854/" target="_blank" rel="noopener">EOJ2854</a></strong></p><h1 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h1><hr><p>在0和1组成的长度为$n(1 \le n \le 31)$的字符串中，统计包含$m(1 \le m \le n)$个连续1子串的字符串的个数。</p><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><hr><p>这题要用到的算法思想是动态规划。</p><p>首先令$f(n, m)$表示长度为$n(1 \le n \le 31)$的字符串中，包含$m(1 \le m \le n)$个连续1子串的字符串的个数。考虑最后一位，也就是第$n$位的取值，可以分为两种情况：</p><ul><li>如果第$n$位为0，那么只能在前面的$n-1$位里取长度为$m$的连续1子串，那么答案就是<br>\[f(n-1,m)\]</li><li>如果第$n$位为1，那么考虑两种情况。<br>一种是最后$m$位全为1，那么前面$n-m$位就可以任意取值，答案为<br>\[2^{n-m}\]<br>另一种情况是最后$m$位不全为1，也就是存在某一位为0，枚举最后一位0出现的位置，可能出现在第$n-1$位、第$n-2$位，一直到第$n-m+1$位，不管最后一个0出现在哪里，都要在之前的字符串中重新出现长度为$m$的连续1子串，所以答案是<br>\[\sum\limits_{n - m \le i \le n - 2} {f(i,m)} \]</li></ul><p>所以最终的答案就是<br>\[f(n,m) = {2^{n - m}} + \sum\limits_{n - m \le i \le n - 1} {f(i,m)} \]<br>进一步化简这个式子，用$n-1$替换$n$可以得到<br>\[f(n - 1,m) = {2^{n - m - 1}} + \sum\limits_{n - m - 1 \le i \le n - 2} {f(i,m)} \]<br>两式相减可以得到<br>\[f(n,m) = {2^{n - m - 1}} + 2f(n - 1,m) - f(n - m - 1,m)\]<br>边界条件为：</p><ul><li>当$n &lt; m$时，$f(n,m)=0$。</li><li>当$n = m$时，$f(n,m)=1$。</li></ul><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><hr><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;bits/stdc++.h></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">f</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">int</span> m<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>n <span class="token operator">&lt;</span> m<span class="token punctuation">)</span> <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>n <span class="token operator">==</span> m<span class="token punctuation">)</span> <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> res <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> <span class="token function">f</span><span class="token punctuation">(</span>n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token function">f</span><span class="token punctuation">(</span>n <span class="token operator">-</span> m <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">&lt;&lt;</span> <span class="token punctuation">(</span>n <span class="token operator">-</span> m <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> res<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> n<span class="token punctuation">,</span> m<span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>n<span class="token punctuation">,</span> <span class="token operator">&amp;</span>m<span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> <span class="token punctuation">(</span>n <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span> <span class="token operator">||</span> m <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span> res <span class="token operator">=</span> <span class="token function">f</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d\n"</span><span class="token punctuation">,</span> res<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> EOJ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第五章作业解答</title>
      <link href="/2018/06/01/concrete-math-hw5/"/>
      <url>/2018/06/01/concrete-math-hw5/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=460578140&auto=1&height=66"></iframe></div><h1 id="4"><a href="#4" class="headerlink" title="4."></a>4.</h1><hr><p><strong>题目：</strong><br>通过上指标翻转计算出$\left( {\begin{array}{*{20}{c}}{ - 1}\\k\end{array}} \right)$。<br><strong>解答：</strong><br>如果$k \ge 0$，那么<br>\[<br>\left( {\begin{array}{*{20}{c}}{ - 1}\\k\end{array}} \right) = {( - 1)^k}\left( {\begin{array}{*{20}{c}}{k - ( - 1) - 1}\\k\end{array}} \right) = {( - 1)^k}\left( {\begin{array}{*{20}{c}}k\\k\end{array}} \right) = {( - 1)^k}<br>\]<br>如果$k&lt;0$，那么<br>\[<br>\left( {\begin{array}{*{20}{c}}{ - 1}\\k\end{array}} \right) = 0<br>\]</p><h1 id="46"><a href="#46" class="headerlink" title="46."></a>46.</h1><hr><p><strong>题目：</strong><br>求出下列和式的闭形式解，其中$n$是正整数。<br>\[<br>\sum\limits_k {\left( {\begin{array}{*{20}{c}}{2k - 1}\\k\end{array}} \right)\left( {\begin{array}{*{20}{c}}{4n - 2k - 1}\\{2n - k}\end{array}} \right)\frac{ { { {( - 1)}^{k - 1}}}}{ {(2k - 1)(4n - 2k - 1)}}}<br>\]<br><strong>解答：</strong><br>由公式$(5.69)$可得<br>\[<br>{\mathcal B_{ - 1}}(z) = \sum\limits_k {\left( {\begin{array}{*{20}{c}}{2k - 1}\\k\end{array}} \right)\frac{ { { {( - z)}^k}}}{ {1 - 2k}}}  = \sum\limits_k {\left( {\begin{array}{*{20}{c}}{2k - 1}\\k\end{array}} \right)\frac{ { { {( - 1)}^{k - 1}}}}{ {2k - 1}}{z^k}}<br>\]<br>\[<br>{\mathcal{B}_{ - 1}}( - z) = \sum\limits_k {\left( {\begin{array}{*{20}{c}}{2k - 1}\\k\end{array}} \right)\frac{ { {z^k}}}{ {1 - 2k}}}  = \sum\limits_k {\left( {\begin{array}{*{20}{c}}{2k - 1}\\k\end{array}} \right)\frac{ { - 1}}{ {2k - 1}}{z^k}}<br>\]<br>两式相乘得到${\mathcal{B}_{ - 1}}(z){\mathcal{B}_{ - 1}}( - z)$，其中$z^{2n}$项的系数恰好就是<br>\[<br>\begin{array}{l}\sum\limits_k {\left( {\begin{array}{*{20}{c}}{2k - 1}\\k\end{array}} \right)\frac{ { { {( - 1)}^{k - 1}}}}{ {2k - 1}} \cdot \left( {\begin{array}{*{20}{c}}{2(2n - k) - 1}\\{2n - k}\end{array}} \right)\frac{ { - 1}}{ {2(2n - k) - 1}}} \\ =  - \sum\limits_k {\left( {\begin{array}{*{20}{c}}{2k - 1}\\k\end{array}} \right)\left( {\begin{array}{*{20}{c}}{4n - 2k - 1}\\{2n - k}\end{array}} \right)\frac{ { { {( - 1)}^{k - 1}}}}{ {(2k - 1)(4n - 2k - 1)}}} \end{array}<br>\]<br>所以题目所求的和式的闭形式解就是${\mathcal{B}_{ - 1}}(z){\mathcal{B}_{ - 1}}( - z)$的$z^{2n}$项的系数的相反数。<br>由公式$(5.69)$还可以得到<br>\[<br>{\mathcal{B}_{ - 1}}(z) = \frac{ {1 + \sqrt {1 + 4z} }}{2}<br>\]<br>\[<br>{\mathcal{B}_{ - 1}}( - z) = \frac{ {1 + \sqrt {1 - 4z} }}{2}<br>\]<br>所以<br>\[<br>(2{\mathcal{B}_{ - 1}}(z) - 1)(2{\mathcal{B}_{ - 1}}( - z) - 1) = \sqrt {1 - 16{z^2}}<br>\]<br>展开化简可以得到<br>\[<br>{\mathcal{B}_{ - 1}}(z){\mathcal{B}_{ - 1}}( - z) = \frac{1}{4}\sqrt {1 - 16{z^2}}  + \frac{1}{2}{\mathcal{B}_{ - 1}}(z) + \frac{1}{2}{\mathcal{B}_{ - 1}}( - z) - 1<br>\]<br>而<br>\[<br>\begin{array}{l}{(1 - 16{z^2})^{1/2}} = \sum\limits_k {\left( {\begin{array}{*{20}{c}}{1/2}\\k\end{array}} \right){ {( - 16)}^k}{z^{2k}}} \\ = \sum\limits_k {\frac{1}{ {1 - 2k}}\left( {\begin{array}{*{20}{c}}{ - 1/2}\\k\end{array}} \right){ {( - 16)}^k}{z^{2k}}} \\ = \sum\limits_k {\frac{1}{ {1 - 2k}}\frac{ { { {( - 1)}^k}}}{ { {4^k}}}\left( {\begin{array}{*{20}{c}}{2k}\\k\end{array}} \right){ {( - 16)}^k}{z^{2k}}} \\ = \sum\limits_k {\frac{1}{ {1 - 2k}}\left( {\begin{array}{*{20}{c}}{2k}\\k\end{array}} \right){4^k}{z^{2k}}} \end{array}<br>\]<br>所以题目答案即${\mathcal{B}_{ - 1}}(z){\mathcal{B}_{ - 1}}( - z)$的$z^{2n}$项的系数的相反数为<br>\[<br>\left( {\begin{array}{*{20}{c}}{2n}\\n\end{array}} \right)\frac{ { {4^{n - 1}}}}{ {2n - 1}} + \left( {\begin{array}{*{20}{c}}{4n - 1}\\{2n}\end{array}} \right)\frac{1}{ {4n - 1}}<br>\]</p><h1 id="64"><a href="#64" class="headerlink" title="64."></a>64.</h1><hr><p><strong>题目：</strong><br>计算<br>\[<br>\sum\limits_{k = 0}^n {\left( {\begin{array}{*{20}{c}}n\\k\end{array}} \right)/\left\lceil {\frac{ {k + 1}}{2}} \right\rceil }<br>\]<br><strong>解答：</strong><br>\[<br>\begin{array}{l}\sum\limits_{k = 0}^n {\left( {\begin{array}{*{20}{c}}n\\k\end{array}} \right)/\left\lceil {\frac{ {k + 1}}{2}} \right\rceil } \\ = \sum\limits_{k = 0}^n {\left( {\left( {\begin{array}{*{20}{c}}n\\{2k}\end{array}} \right) + \left( {\begin{array}{*{20}{c}}n\\{2k + 1}\end{array}} \right)} \right)\frac{1}{ {k + 1}}} \\ = \sum\limits_{k = 0}^n {\left( {\begin{array}{*{20}{c}}{n + 1}\\{2k + 1}\end{array}} \right)\frac{1}{ {k + 1}}} \\ = \frac{2}{ {n + 2}}\sum\limits_{k = 0}^n {\left( {\begin{array}{*{20}{c}}{n + 2}\\{2k + 2}\end{array}} \right)} \\ = \frac{ { {2^{n + 2}} - 2}}{ {n + 2}}\end{array}<br>\]</p><h1 id="65"><a href="#65" class="headerlink" title="65."></a>65.</h1><hr><p><strong>题目：</strong><br>证明<br>\[<br>\sum\limits_k {\left( {\begin{array}{*{20}{c}}{n - 1}\\k\end{array}} \right){n^{ - k}}(k + 1)!}  = n<br>\]<br><strong>解答：</strong><br>等号左边可以写为<br>\[<br>\sum\limits_{0 \le k \le n - 1} {\left( {\begin{array}{*{20}{c}}{n - 1}\\k\end{array}} \right){n^{ - k}}(k + 1)!}<br>\]<br>替换$k$为$n-1-k$，得到<br>\[<br>\sum\limits_{0 \le k \le n - 1} {\left( {\begin{array}{*{20}{c}}{n - 1}\\k\end{array}} \right){n^{1 + k - n}}(n - k)!}<br>\]<br>即证<br>\[<br>\sum\limits_{0 \le k \le n - 1} {\left( {\begin{array}{*{20}{c}}{n - 1}\\k\end{array}} \right){n^{1 + k - n}}(n - k)!}  = n<br>\]<br>等式两边同时乘以$n^{n-1}$，即证<br>\[<br>\sum\limits_{0 \le k \le n - 1} {\left( {\begin{array}{*{20}{c}}{n - 1}\\k\end{array}} \right){n^k}(n - k)!}  = {n^n}<br>\]<br>等式左边等于<br>\[<br>\begin{array}{l}\sum\limits_{0 \le k \le n - 1} {\left( {\begin{array}{*{20}{c}}{n - 1}\\k\end{array}} \right){n^k}(n - k)!} \\ = (n - 1)!\sum\limits_{0 \le k \le n - 1} {\frac{ { {n^k}(n - k)}}{ {k!}}} \\ = (n - 1)!\sum\limits_{0 \le k \le n - 1} {\left( {\frac{ { {n^{k + 1}}}}{ {k!}} - \frac{ { {n^k}}}{ {(k - 1)!}}} \right)} \\ = (n - 1)!\frac{ { {n^n}}}{ {(n - 1)!}}\\ = {n^n}\end{array}<br>\]<br>得证。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第14课（牛顿级数和生成函数）</title>
      <link href="/2018/05/28/concrete-math-14/"/>
      <url>/2018/05/28/concrete-math-14/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=26492802&auto=1&height=66"></iframe></div><h1 id="牛顿级数"><a href="#牛顿级数" class="headerlink" title="牛顿级数"></a>牛顿级数</h1><hr><p>多项式函数的一般表示形式为：<br>\[f(x) = {a_d}{x^d} + {a_{d - 1}}{x^{d - 1}} +  \cdots  + {a_1}{x^1} + {a_0}{x^0}\]<br>也可以将其表示为下降阶乘幂的形式：<br>\[f(x) = {b_d}{x^\underline{d}} + {b_{d - 1}}{x^{\underline{d - 1}}} +  \cdots  + {b_1}{x^\underline{1}} + {b_0}{x^\underline{0}}\]<br>这种表示的好处是，求差分更加方便：<br>\[\Delta (f(x)) = {b_d}d{x^{\underline{d - 1}}} + {b_{d - 1}}(d - 1){x^{\underline {d - 2} }} +  \cdots  + {b_1}{x^\underline{0}}\]<br>因为有<br>\[\left( {\begin{array}{*{20}{c}}x\\k\end{array}} \right) = \frac{ { {x^\underline{k}}}}{ {k!}}\]<br>所以多项式又可以表示为组合数的形式，也被叫做牛顿级数：<br>\[f(x) = {c_d}\left( {\begin{array}{*{20}{c}}x\\d\end{array}} \right) + {c_{d - 1}}\left( {\begin{array}{*{20}{c}}x\\{d - 1}\end{array}} \right) +  \cdots  + {c_1}\left( {\begin{array}{*{20}{c}}x\\1\end{array}} \right) + {c_0}\left( {\begin{array}{*{20}{c}}x\\0\end{array}} \right)\]<br>这种形式的差分也特别简单，因为有<br>\[\Delta \left( {\left( {\begin{array}{*{20}{c}}x\\k\end{array}} \right)} \right) = \left( {\begin{array}{*{20}{c}}x\\{k - 1}\end{array}} \right)\]<br>所以$n$阶差分可以写为：<br>\[{\Delta ^n}(f(x)) = {c_d}\left( {\begin{array}{*{20}{c}}x\\{d - n}\end{array}} \right) + {c_{d - 1}}\left( {\begin{array}{*{20}{c}}x\\{d - 1 - n}\end{array}} \right) +  \cdots  + {c_1}\left( {\begin{array}{*{20}{c}}x\\{1 - n}\end{array}} \right) + {c_0}\left( {\begin{array}{*{20}{c}}x\\{ - n}\end{array}} \right)\]<br>所以有：<br>\[{\Delta ^n}(f(0)) = \left\{ {\begin{array}{*{20}{c}}{ {c_n},n \le d}\\{0,n &gt; d}\end{array}} \right.\]<br>所以牛顿级数又可以写为：<br>\[f(x) = {\Delta ^d}(f(0))\left( {\begin{array}{*{20}{c}}x\\d\end{array}} \right) + {\Delta ^{d - 1}}(f(0))\left( {\begin{array}{*{20}{c}}x\\{d - 1}\end{array}} \right) +  \cdots  + \Delta (f(0))\left( {\begin{array}{*{20}{c}}x\\1\end{array}} \right) + f(0){c_0}\left( {\begin{array}{*{20}{c}}x\\0\end{array}} \right)\]<br>这个形式是不是很像泰勒展开？</p><h1 id="生成函数"><a href="#生成函数" class="headerlink" title="生成函数"></a>生成函数</h1><hr><p>对于无限序列$\left\langle { {a_0},{a_1},{a_2}, \ldots } \right\rangle $，定义它的生成函数为：<br>\[A(z) = {a_0} + {a_1}z + {a_2}{z^2} +  \cdots  = \sum\limits_{k \ge 0} { {a_k}{z^k}} \]<br>定义一个函数用来表示$z^n$的系数：<br>\[[{z^n}]A(z) = {a_n}\]<br>两个生成函数相乘的结果为：<br>\[A(z)B(z) = \sum\limits_{k \ge 0} {\sum\limits_{i = 0}^k { {a_i}{b_{k - i}}} {z^k}} \]<br>考虑下面的二项展开：<br>\[{(1 + z)^r} = \sum\limits_{k \ge 0} {\left( {\begin{array}{*{20}{c}}r\\k\end{array}} \right){z^k}} \]<br>可以发现这就是序列$\left\langle {\left( {\begin{array}{*{20}{c}}r\\0\end{array}} \right),\left( {\begin{array}{*{20}{c}}r\\1\end{array}} \right),\left( {\begin{array}{*{20}{c}}r\\2\end{array}} \right), \ldots } \right\rangle $的生成函数。<br>替换变量可以得到：<br>\[{(1 + z)^s} = \sum\limits_{k \ge 0} {\left( {\begin{array}{*{20}{c}}s\\k\end{array}} \right){z^k}} \]<br>两个式子相乘可以得到：<br>\[{(1 + z)^r}{(1 + z)^s} = {(1 + z)^{r + s}}\]<br>等式两边$z^n$的系数相等，于是：<br>\[\sum\limits_{k = 0}^n {\left( {\begin{array}{*{20}{c}}r\\k\end{array}} \right)\left( {\begin{array}{*{20}{c}}s\\{n - k}\end{array}} \right)}  = \left( {\begin{array}{*{20}{c}}{r + s}\\n\end{array}} \right)\]<br>这和上节课讲到的范德蒙德卷积公式类似！这里是用生成函数证出来的。</p><p>同理根据<br>\[{(1 + z)^r}{(1 - z)^r} = {(1 - {z^2})^r}\]<br>可以得到<br>\[\sum\limits_{k = 0}^n {\left( {\begin{array}{*{20}{c}}r\\k\end{array}} \right)\left( {\begin{array}{*{20}{c}}r\\{n - k}\end{array}} \right)} {( - 1)^k} = {( - 1)^{n/2}}\left( {\begin{array}{*{20}{c}}r\\{n/2}\end{array}} \right)[n是偶数]\]<br>下面是一个重要的生成函数：<br>\[\frac{1}{ {1 - z}} = 1 + z + {z^2} + {z^3} +  \cdots  = \sum\limits_{k \ge 0} { {z^k}} \]<br>它其实就是序列$\left\langle { {1},{1},{1}, \ldots } \right\rangle $的生成函数。</p><h1 id="生成函数应用"><a href="#生成函数应用" class="headerlink" title="生成函数应用"></a>生成函数应用</h1><hr><p>那么生成函数有什么应用呢？一个很重要的应用就是用来求解递归式。</p><p>例如大家很熟悉的斐波那契数列：<br>\[\begin{array}{l}{g_0} = 0;{g_1} = 1\\{g_n} = {g_{n - 1}} + {g_{n - 2}},n \ge 2\end{array}\]</p><p>首先为了统一表示，将递归式改写为如下形式：<br>\[{g_n} = {g_{n - 1}} + {g_{n - 2}} + [n = 1]\]<br>然后两边同时乘以$z^n$，得到：<br>\[{g_n}{z^n} = {g_{n - 1}}{z^n} + {g_{n - 2}}{z^n} + [n = 1]{z^n}\]<br>两边对指标$n$同时求和，可以得到：<br>\[G(z) = zG(z) + {z^2}G(z) + z\]<br>所以<br>\[G(z) = \frac{z}{ {1 - z - {z^2}}}\]<br>最后只要将$\frac{z}{ {1 - z - {z^2}}}$表示成多项式的形式就行了，$[{z^n}]\frac{z}{ {1 - z - {z^2}}}$就是斐波那契数列的通项公式了。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第13课（组合数各种性质）</title>
      <link href="/2018/05/27/concrete-math-13/"/>
      <url>/2018/05/27/concrete-math-13/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=33035611&auto=1&height=66"></iframe></div><blockquote><p>首先庆祝我自己顺利毕业了，忙完了毕业论文答辩一直在浪，所以上周的具体数学没有更新，现在补更一下，大家见谅。</p></blockquote><p>首先这节课讲的基本都是组合数的相关性质，而且特别多，所以我就不在这里详细证明了，如果你们对某一个性质感兴趣，可以自己证明去。</p><h1 id="性质1"><a href="#性质1" class="headerlink" title="性质1"></a>性质1</h1><hr><p>首先将组合数推广到负数域，也就是底数为负数的情况：<br>\[\left( {\begin{array}{*{20}{c}}r\\k\end{array}} \right) = {( - 1)^k}\left( {\begin{array}{*{20}{c}}{k - r - 1}\\k\end{array}} \right)\]<br>证明可以从下降阶乘幂的定义直接得到。</p><h1 id="性质2"><a href="#性质2" class="headerlink" title="性质2"></a>性质2</h1><hr><p>由于<br>\[\left( {\begin{array}{*{20}{c}}{m + n}\\m\end{array}} \right) = \left( {\begin{array}{*{20}{c}}{m + n}\\n\end{array}} \right)\]<br>所以由性质1可得<br>\[{( - 1)^m}\left( {\begin{array}{*{20}{c}}{ - n - 1}\\m\end{array}} \right) = {( - 1)^n}\left( {\begin{array}{*{20}{c}}{ - m - 1}\\n\end{array}} \right)\]</p><h1 id="性质3"><a href="#性质3" class="headerlink" title="性质3"></a>性质3</h1><hr><p>\[\sum\limits_{k \le m} {\left( {\begin{array}{*{20}{c}}r\\k\end{array}} \right){ {( - 1)}^k}}  = {( - 1)^m}\left( {\begin{array}{*{20}{c}}{r - 1}\\m\end{array}} \right)\]<br>这就说明了杨辉三角同一行的前面若干项交错和是可以求得的，但是它们的直接和是无法求出的。</p><h1 id="性质4"><a href="#性质4" class="headerlink" title="性质4"></a>性质4</h1><hr><p>\[\sum\limits_{k \le m} {\left( {\begin{array}{*{20}{c}}{m + r}\\k\end{array}} \right){x^k}{y^{m - k}} = \sum\limits_{k \le m} {\left( {\begin{array}{*{20}{c}}{ - r}\\k\end{array}} \right){ {( - x)}^k}{ {(x + y)}^{m - k}}} } \]<br>证明可以通过令<br>\[{S_m} = \sum\limits_{k \le m} {\left( {\begin{array}{*{20}{c}}{m + r}\\k\end{array}} \right){x^k}{y^{m - k}}}  = \sum\limits_{k \le m} {\left( {\begin{array}{*{20}{c}}{m + r - 1}\\k\end{array}} \right){x^k}{y^{m - k}}}  + \sum\limits_{k \le m} {\left( {\begin{array}{*{20}{c}}{m + r - 1}\\{k - 1}\end{array}} \right){x^k}{y^{m - k}}} \]<br>将左边表示成递归式的形式，同理如果右边可以表示成相同的递归式，那么左右就相等了。</p><p>性质4看起来特别复杂，那么它有什么用呢？如果令$x$和$y$等于不同的值，那么就可以得到许多不同的恒等式。</p><h1 id="性质5"><a href="#性质5" class="headerlink" title="性质5"></a>性质5</h1><hr><p>令$x =  - 1,y = 1$可以得到<br>\[\sum\limits_{k \le m} {\left( {\begin{array}{*{20}{c}}{m + r}\\k\end{array}} \right){ {( - 1)}^k}}  = \left( {\begin{array}{*{20}{c}}{ - r}\\m\end{array}} \right)\]<br>这其实就是性质3的特例。</p><h1 id="性质6"><a href="#性质6" class="headerlink" title="性质6"></a>性质6</h1><hr><p>令$x = y = 1,r = m + 1$可以得到<br>\[\sum\limits_{k \le m} {\left( {\begin{array}{*{20}{c}}{2m + 1}\\k\end{array}} \right)}  = \sum\limits_{k \le m} {\left( {\begin{array}{*{20}{c}}{m + k}\\k\end{array}} \right){2^{m - k}}} \]<br>左边就是杨辉三角一行中左边一半的和，所以可以得到<br>\[\sum\limits_{k \le m} {\left( {\begin{array}{*{20}{c}}{m + k}\\k\end{array}} \right){2^{ - k}}} {\rm{ = }}{2^m}\]</p><h1 id="性质7"><a href="#性质7" class="headerlink" title="性质7"></a>性质7</h1><hr><p>\[\left( {\begin{array}{*{20}{c}}r\\m\end{array}} \right)\left( {\begin{array}{*{20}{c}}m\\k\end{array}} \right) = \left( {\begin{array}{*{20}{c}}r\\k\end{array}} \right)\left( {\begin{array}{*{20}{c}}{r - k}\\{m - k}\end{array}} \right)\]<br>这个公式可以形象理解为，从$r$个物品中取$m$个，再从这$m$个中取$k$个的方法数等于从$r$个物品中取$k$个，再从剩下的$r-k$个中取$m-k$个的方法数。证明的话直接用定义可证。</p><h1 id="性质8"><a href="#性质8" class="headerlink" title="性质8"></a>性质8</h1><hr><p>之前介绍了二项式系数，那么可以推广到任意$m$个未知数，它的展开式为<br>\[{({x_1} + {x_2} +  \cdots  + {x_m})^n} = \sum\limits_{\scriptstyle0 \le {a_1},{a_2}, \cdots ,{a_m} \le n\atop\scriptstyle{a_1} + {a_2} +  \cdots  + {a_m} = n} {\left( {\begin{array}{*{20}{c}}{ {a_1} + {a_2} +  \cdots  + {a_m}}\\{ {a_1},{a_2}, \cdots ,{a_m}}\end{array}} \right)} {x_1}^{ {a_1}}{x_2}^{ {a_2}} \cdots {x_m}^{ {a_m}}\]<br>其中<br>\[\left( {\begin{array}{*{20}{c}}{ {a_1} + {a_2} +  \cdots  + {a_m}}\\{ {a_1},{a_2}, \cdots ,{a_m}}\end{array}} \right) = \left( {\begin{array}{*{20}{c}}{ {a_1} + {a_2} +  \cdots  + {a_m}}\\{ {a_2} +  \cdots  + {a_m}}\end{array}} \right) \cdots \left( {\begin{array}{*{20}{c}}{ {a_{m - 1}} + {a_m}}\\{ {a_m}}\end{array}} \right)\]</p><h1 id="性质9"><a href="#性质9" class="headerlink" title="性质9"></a>性质9</h1><hr><p>范德蒙德卷积式：<br>\[\sum\limits_k {\left( {\begin{array}{*{20}{c}}r\\{m + k}\end{array}} \right)} \left( {\begin{array}{*{20}{c}}s\\{n - k}\end{array}} \right) = \left( {\begin{array}{*{20}{c}}{r + s}\\{m + n}\end{array}} \right)\]<br>很多公式都可以通过替换其中的一些变量推导得到：<br>\[\begin{array}{l}\sum\limits_k {\left( {\begin{array}{*{20}{c}}l\\{m + k}\end{array}} \right)} \left( {\begin{array}{*{20}{c}}s\\{n + k}\end{array}} \right) = \left( {\begin{array}{*{20}{c}}{l + s}\\{l - m + n}\end{array}} \right)\\\sum\limits_k {\left( {\begin{array}{*{20}{c}}l\\{m + k}\end{array}} \right)} \left( {\begin{array}{*{20}{c}}{s + k}\\n\end{array}} \right){( - 1)^k} = {( - 1)^{l + m}}\left( {\begin{array}{*{20}{c}}{s - m}\\{n - l}\end{array}} \right)\\\sum\limits_{k \le l} {\left( {\begin{array}{*{20}{c}}{l - k}\\m\end{array}} \right)} \left( {\begin{array}{*{20}{c}}s\\{k - n}\end{array}} \right){( - 1)^k} = {( - 1)^{l + m}}\left( {\begin{array}{*{20}{c}}{s - m - 1}\\{l - m - n}\end{array}} \right)\\\sum\limits_{0 \le k \le l} {\left( {\begin{array}{*{20}{c}}{l - k}\\m\end{array}} \right)} \left( {\begin{array}{*{20}{c}}{q + k}\\n\end{array}} \right) = \left( {\begin{array}{*{20}{c}}{l + q + 1}\\{m + n + 1}\end{array}} \right)\end{array}\]</p><h1 id="例题1"><a href="#例题1" class="headerlink" title="例题1"></a>例题1</h1><hr><p>最后详细求解一道组合题，其他的题目就不介绍了，可以去看具体数学英文版第173页。</p><p>求下面式子的闭形式解：<br>\[\sum\limits_{k = 0}^m {\left( {\begin{array}{*{20}{c}}m\\k\end{array}} \right)/\left( {\begin{array}{*{20}{c}}n\\k\end{array}} \right)} ,n \ge m \ge 0\]</p><p>根据性质7，可以得到<br>\[\left( {\begin{array}{*{20}{c}}m\\k\end{array}} \right)/\left( {\begin{array}{*{20}{c}}n\\k\end{array}} \right) = \left( {\begin{array}{*{20}{c}}{n - k}\\{m - k}\end{array}} \right)/\left( {\begin{array}{*{20}{c}}n\\m\end{array}} \right)\]<br>所以<br>\[\sum\limits_{k = 0}^m {\left( {\begin{array}{*{20}{c}}m\\k\end{array}} \right)/\left( {\begin{array}{*{20}{c}}n\\k\end{array}} \right)}  = \sum\limits_{k = 0}^m {\left( {\begin{array}{*{20}{c}}{n - k}\\{m - k}\end{array}} \right)/\left( {\begin{array}{*{20}{c}}n\\m\end{array}} \right)} \]<br>而<br>\[\begin{array}{l}\sum\limits_{k \ge 0} {\left( {\begin{array}{*{20}{c}}{n - k}\\{m - k}\end{array}} \right)}  = \sum\limits_{m - k \ge 0} {\left( {\begin{array}{*{20}{c}}{n - (m - k)}\\{m - (m - k)}\end{array}} \right)} \\ = \sum\limits_{k \le m} {\left( {\begin{array}{*{20}{c}}{n - m + k}\\k\end{array}} \right)} \\ = \left( {\begin{array}{*{20}{c}}{(n - m) + m + 1}\\m\end{array}} \right)\\ = \left( {\begin{array}{*{20}{c}}{n + 1}\\m\end{array}} \right)\end{array}\]<br>所以<br>\[\sum\limits_{k = 0}^m {\left( {\begin{array}{*{20}{c}}m\\k\end{array}} \right)/\left( {\begin{array}{*{20}{c}}n\\k\end{array}} \right)}  = \left( {\begin{array}{*{20}{c}}{n + 1}\\m\end{array}} \right)/\left( {\begin{array}{*{20}{c}}n\\m\end{array}} \right) = \frac{ {n + 1}}{ {n + 1 - m}}\]</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第12课（数论进阶与组合数入门）</title>
      <link href="/2018/05/14/concrete-math-12/"/>
      <url>/2018/05/14/concrete-math-12/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=554191378&auto=1&height=66"></iframe></div><blockquote><p>这节课内容太多了，再加上感冒身体不舒服，下面的定理就不一一证明了，大家可以自行练习。以后有空我会补上的！</p></blockquote><h1 id="例题1"><a href="#例题1" class="headerlink" title="例题1"></a>例题1</h1><hr><p>首先接着上节课同余继续讲，在<a href="http://godweiyang.com/2018/04/23/concrete-math-9/">第三章例题2</a>中，我们遗留了一个问题：对于如下序列<br>\[0\bmod m,n\bmod m,2n\bmod m, \ldots ,(m - 1)n\bmod m\]<br>它的值就是<br>\[0,d,2d, \ldots ,(m/d - 1)d\]<br>的某个排列，并且重复了$d$次。其中$d = gcd(m, n)$</p><p>首先我们有如下同余式：<br>\[jn \equiv kn(\bmod m) \Leftrightarrow j(n/d) \equiv k(n/d)(\bmod m/d)\]<br>这就可以看出该序列的确是重复出现了$d$次，那么剩下的问题就是证明这$m/d$个数恰好就是<br>\[\{ 0,d,2d, \ldots ,m - d\} \]<br>的某个排列。<br>令$m = m’d,n = n’d$，所以有<br>\[kn\bmod m = d(kn’\bmod m’)\]<br>所以我们只考虑$m \bot n$的情形，在此情形下，我们可以得到<br>\[jn \equiv kn(\bmod m) \Leftrightarrow j \equiv k(\bmod m)\]<br>由此可以看出，这$m-1$个数一定就是<br>\[\{ 0,1,2, \ldots ,m - 1\} \]<br>至此得证。</p><p>下面介绍几个著名的数论定理。</p><h1 id="费马最后定理"><a href="#费马最后定理" class="headerlink" title="费马最后定理"></a>费马最后定理</h1><hr><p>对于所有的正整数$a,b,c,n&gt;2$，有<br>\[{a^n} + {b^n} \ne {c^n}\]</p><h1 id="费马小定理"><a href="#费马小定理" class="headerlink" title="费马小定理"></a>费马小定理</h1><hr><p>如果$n \bot p$，那么有<br>\[{n^{p - 1}} \equiv 1(\bmod p)\]</p><p>证明也很好证。</p><p>之前证过了，序列<br>\[n\bmod p,2n\bmod p, \ldots ,(p - 1)n\bmod p\]<br>结果就是<br>\[1,2, \ldots ,p-1\]<br>的某个排列，所以有<br>\[n \cdot (2n) \cdot  \ldots  \cdot ((p - 1)n) \equiv (p - 1)!\]<br>所以<br>\[(p - 1)!{n^{p - 1}} \equiv (p - 1)!(\bmod p)\]<br>所以<br>\[{n^{p - 1}} \equiv 1(\bmod p)\]</p><h1 id="欧拉函数"><a href="#欧拉函数" class="headerlink" title="欧拉函数"></a>欧拉函数</h1><hr><p>定义$\varphi (m)$为小于$m$且与其互素的正整数个数。</p><p>所以我们有欧拉定理<br>\[{n^{\varphi (m)}} \equiv 1(\bmod m)\]<br>其中$n \bot m$，可以发现，当$m$是素数时，欧拉定理就是费马小定理，所以欧拉定理是费马小定理的推广形式。</p><p>欧拉定理有很多有趣的性质，这里就不一一介绍了，详情见<a href="https://blog.csdn.net/howe_young/article/details/50282775" target="_blank" rel="noopener">博客地址</a>。</p><h1 id="莫比乌斯函数"><a href="#莫比乌斯函数" class="headerlink" title="莫比乌斯函数"></a>莫比乌斯函数</h1><hr><p>定义莫比乌斯函数$\mu (m)$为<br>\[\sum\limits_{d|m} {\mu (d)}  = [m = 1]\]</p><p>这个定义看起来很奇怪是不是？其实这是一个递归定义，可以递归地计算得到所有的值。</p><p>这个函数有什么用呢？主要用来进行莫比乌斯反演：<br>\[g(m) = \sum\limits_{d|m} {f(d)}  \Leftrightarrow f(m) = \sum\limits_{d|m} {\mu (d)g(\frac{m}{d})} \]</p><p>详细的性质及应用也不介绍了，给大家推荐一个牛逼的博客<a href="https://blog.csdn.net/acdreamers/article/details/8542292" target="_blank" rel="noopener">博客地址</a>，我当时学ACM的时候这部分都是看着他的学的。</p><h1 id="组合数入门"><a href="#组合数入门" class="headerlink" title="组合数入门"></a>组合数入门</h1><hr><p>定义组合数$\left( {\begin{array}{c}n\\k\end{array}} \right)$为从$n$个物品中取出$k$个物品的方法数，具体计算为<br>\[\left( {\begin{array}{c}n\\k\end{array}} \right) = \frac{ {n(n - 1) \ldots (n - k + 1)}}{ {k(k - 1) \ldots 1}}\]</p><p>推广到实数领域，定义<br>\[\left( {\begin{array}{c}r\\k\end{array}} \right) = \left\{ {\begin{array}{c}{\frac{ {r(r - 1) \ldots (r - k + 1)}}{ {k(k - 1) \ldots 1}} = \frac{ { {r^{\underline{k}}}}}{ {k!}},k \ge 0}\\{0,k &lt; 0}\end{array}} \right.\]</p><p>下面介绍一些组合数性质。</p><h2 id="性质1"><a href="#性质1" class="headerlink" title="性质1"></a>性质1</h2><p>\[\left( {\begin{array}{c}n\\k\end{array}} \right) = \left( {\begin{array}{c}n\\{n - k}\end{array}} \right),n,k \in \mathbb{Z},n \ge 0\]<br>这里为什么要限定$n \ge 0$呢？举个例子，如果$n = -1$，那么有<br>\[\left( {\begin{array}{c}{ - 1}\\k\end{array}} \right) \ne \left( {\begin{array}{c}{ - 1}\\{ - 1 - k}\end{array}} \right)\]<br>因为左边等于${( - 1)^k}$，而右边等于${( - 1)^{-1-k}}$。</p><h2 id="性质2"><a href="#性质2" class="headerlink" title="性质2"></a>性质2</h2><p>\[\left( {\begin{array}{c}r\\k\end{array}} \right) = \frac{r}{k}\left( {\begin{array}{c}{r - 1}\\{k - 1}\end{array}} \right)\]</p><h2 id="性质3"><a href="#性质3" class="headerlink" title="性质3"></a>性质3</h2><p>\[(r - k)\left( {\begin{array}{c}r\\k\end{array}} \right) = r\left( {\begin{array}{c}{r - 1}\\k\end{array}} \right)\]</p><h2 id="性质4"><a href="#性质4" class="headerlink" title="性质4"></a>性质4</h2><p>\[\left( {\begin{array}{c}r\\k\end{array}} \right) = \left( {\begin{array}{c}{r - 1}\\k\end{array}} \right) + \left( {\begin{array}{c}{r - 1}\\{k - 1}\end{array}} \right)\]<br>这条性质可以通过性质3和性质4两边分别相加得到。</p><h2 id="性质5"><a href="#性质5" class="headerlink" title="性质5"></a>性质5</h2><p>\[\sum\limits_{k \le n} {\left( {\begin{array}{c}{r + k}\\k\end{array}} \right)}  = \left( {\begin{array}{c}{r + n + 1}\\n\end{array}} \right)\]</p><h2 id="性质6"><a href="#性质6" class="headerlink" title="性质6"></a>性质6</h2><p>\[\sum\limits_{0 \le k \le n} {\left( {\begin{array}{c}k\\m\end{array}} \right)}  = \left( {\begin{array}{c}{n + 1}\\{m + 1}\end{array}} \right)\]</p><h2 id="性质7"><a href="#性质7" class="headerlink" title="性质7"></a>性质7</h2><p>微分形式：<br>\[\Delta \left( {\left( {\begin{array}{c}x\\m\end{array}} \right)} \right) = \left( {\begin{array}{c}{x + 1}\\m\end{array}} \right) - \left( {\begin{array}{c}x\\m\end{array}} \right) = \left( {\begin{array}{c}x\\{m - 1}\end{array}} \right)\]<br>\[\sum {\left( {\begin{array}{c}x\\m\end{array}} \right)\delta x = } \left( {\begin{array}{c}x\\{m + 1}\end{array}} \right) + C\]</p><h1 id="二项式系数"><a href="#二项式系数" class="headerlink" title="二项式系数"></a>二项式系数</h1><hr><p>\[{(x + y)^r} = \sum\limits_k {\left( {\begin{array}{c}r\\k\end{array}} \right)} {x^k}{y^{r - k}},r \in \mathbb{Z}\]</p><p>二项式系数也有很多有趣的性质。</p><p>\[{2^n} = \left( {\begin{array}{c}n\\0\end{array}} \right) + \left( {\begin{array}{c}n\\1\end{array}} \right) +  \cdots  + \left( {\begin{array}{c}n\\n\end{array}} \right)\]</p><p>\[{0^n} = \left( {\begin{array}{c}n\\0\end{array}} \right) - \left( {\begin{array}{c}n\\1\end{array}} \right) +  \cdots  + {( - 1)^n}\left( {\begin{array}{c}n\\n\end{array}} \right)\]<br>即奇数项系数和等于偶数项系数和。</p><p>推广到实数域：<br>\[{(1 + z)^r} = \sum\limits_k {\left( {\begin{array}{c}r\\k\end{array}} \right){z^k}} ,\left| z \right| &lt; 1,r \in \mathbb{R}\]<br>可以通过泰勒展开证明。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第11课（Stern-Brocot树和同余关系）</title>
      <link href="/2018/05/07/concrete-math-11/"/>
      <url>/2018/05/07/concrete-math-11/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=543607345&auto=1&height=66"></iframe></div><h1 id="Stern-Brocot树"><a href="#Stern-Brocot树" class="headerlink" title="Stern-Brocot树"></a>Stern-Brocot树</h1><hr><p>我们接着上节课讲到的Stern-Brocot树继续往下讲。</p><h2 id="LR序列表示"><a href="#LR序列表示" class="headerlink" title="LR序列表示"></a>LR序列表示</h2><p>对于任意分数$\frac{a}{b}$，我们从$\frac{1}{1}$开始走到它所在的结点。如果向左走就记为L，向右走记为R，最终可以得到一个L和R的序列。例如$\frac{5}{7}$的表示就是LRRL。</p><p>这种表示产生了两个问题：</p><ol><li>给定满足正整数$m$和$n$互素的分数$\frac{m}{n}$，它所对应的LR序列是什么？</li><li>给定LR序列，它所表示的分数是什么？</li></ol><p>第二个问题看起来更好解决一点，我们先解决第二个问题。<br>我们定义<br>\[f(S) = 与S对应的分数\]<br>例如<br>\[f(LRRL) = \frac{5}{7}\]<br>如果用代码实现的话，对于每个L或者R，如果是L，那么就把右边界设为中间值，如果是R，那么就把左边界设为中间值。</p><p>但是如何用数学式子来表达这一过程呢？</p><p>我们建立一个2阶方阵：<br>\[M(S) = \left( {\begin{array}{*{20}{c}}n&amp;{n’}\\m&amp;{m’}\end{array}} \right)\]<br>表示$f(S)$的两个祖先分数$\frac{m}{n}$和$\frac{m’}{n’}$</p><p>那么初始状态就可以表示为<br>\[M(I) = \left( {\begin{array}{*{20}{c}}1&amp;0\\0&amp;1\end{array}} \right)\]</p><p>如果遇到了向左符号L，那么</p><p>\[M(SL) = \left( {\begin{array}{}n&amp;{n + n’}\\m&amp;{m + m’}\end{array}} \right) = M(S)\left( {\begin{array}{}1&amp;1\\0&amp;1\end{array}} \right)\]</p><p>如果遇到了向右符号R，那么<br>\[M(SL) = \left( {\begin{array}{}{n + n’}&amp;{n’}\\{m + m’}&amp;{m’}\end{array}} \right) = M(S)\left( {\begin{array}{}1&amp;0\\1&amp;1\end{array}} \right)\]<br>所以我们将L和R定义成2阶方阵就行了：<br>\[L = \left( {\begin{array}{}1&amp;1\\0&amp;1\end{array}} \right),R = \left( {\begin{array}{}1&amp;0\\1&amp;1\end{array}} \right)\]<br>所以<br>\[\begin{array}{l}M(LRRL) = LRRL\\ = \left( {\begin{array}{}1&amp;1\\0&amp;1\end{array}} \right)\left( {\begin{array}{}1&amp;0\\1&amp;1\end{array}} \right)\left( {\begin{array}{}1&amp;0\\1&amp;1\end{array}} \right)\left( {\begin{array}{}1&amp;1\\0&amp;1\end{array}} \right)\\ = \left( {\begin{array}{}3&amp;4\\2&amp;3\end{array}} \right)\end{array}\]<br>所以LRRL表示的分数为<br>\[\frac{ {2 + 3}}{ {3 + 4}} = \frac{5}{7}\]<br>那么第一个问题如何解决呢？<br>同样可以用类似二叉搜索的方法来求出LR序列，也可以用矩阵的方法来求解，根据上面的L和R的方阵，可以发现：<br>\[f(RS) = f(S) + 1\]<br>对于L也有类似的性质，所以我们得到了如下的求解算法：</p><ul><li>如果$m &gt; n$，输出R，令$m = m - n$。</li><li>如果$m &lt; n$，输出L，令$n = n - m$。</li></ul><h2 id="无理数近似表示"><a href="#无理数近似表示" class="headerlink" title="无理数近似表示"></a>无理数近似表示</h2><p>虽然说无理数不在Stern-Brocot树中，但是我们可以找到无限逼近它的分数。</p><p>方法仍然使用二叉搜索，不同的是，搜索过程不会终止，除非得到了我们想要的精度或者我们人为终止。</p><p>值得一提的是，无理数$e$的LR表示很有规律性：<br>\[e = R{L^0}RL{R^2}LR{L^4}RL{R^6}LR{L^8}RL{R^{10}}LR{L^{12}} \cdots \]</p><p>最后值得一提的是，欧几里得算法和有理数的Stern-Brocot树表示有密切的关系。给定$\alpha  = \frac{m}{n}$，根据之前的算法，它的LR表达式首先是$\left\lfloor {m/n} \right\rfloor $个R，然后是$\left\lfloor {n/(m\bmod n)} \right\rfloor $个L，依次下去，这些系数恰好就是求最大公因数的时候用到的系数。</p><h1 id="同余关系"><a href="#同余关系" class="headerlink" title="同余关系"></a>同余关系</h1><hr><p>同余定义为：<br>\[a \equiv b(\bmod m) \Leftrightarrow a\bmod m = b\bmod m\]<br>读作“a关于模m与b同余”，我们只讨论都是整数的情况。</p><p>同样可以写作：<br>\[a \equiv b(\bmod m) \Leftrightarrow a - b是m的倍数\]</p><p>同余是等价关系，满足自反律、对称律、传递律，即：<br>\[\begin{array}{l}a \equiv a\\a \equiv b \Rightarrow b \equiv a\\a \equiv b \equiv c \Rightarrow a \equiv c\end{array}\]<br>如果我们对同余两边的元素加减乘，同余仍然满足：<br>\[\begin{array}{l}a \equiv b,c \equiv d \Rightarrow a + c \equiv b + d(\bmod m)\\a \equiv b,c \equiv d \Rightarrow a - c \equiv b - d(\bmod m)\\a \equiv b,c \equiv d \Rightarrow ac \equiv bd(\bmod m)\end{array}\]<br>因此可以得到<br>\[a \equiv b \Rightarrow {a^n} \equiv {b^n}(\bmod m)\]</p><p>然而对于除法同余并不总是成立，一些特殊条件下可能成立。<br>如果<br>\[ad \equiv bd(\bmod m)\]<br>当$d,m$互素的时候，我们可以得到<br>\[a \equiv b(\bmod m)\]<br>同样<br>\[ad \equiv bd(\bmod md) \Leftrightarrow a \equiv b(\bmod m)\]<br>更一般的情况下，我们有<br>\[ad \equiv bd(\bmod m) \Leftrightarrow a \equiv b(\bmod \frac{m}{ {\gcd (d,m)}})\]<br>还有许多性质我就直接列举了，不做证明了，证明很简单：<br>\[\begin{array}{l}a \equiv b(\bmod md) \Rightarrow a \equiv b(\bmod m)\\a \equiv b(\bmod m),a \equiv b(\bmod n) \Leftrightarrow a \equiv b(\bmod lcm(m,n))\\a \equiv b(\bmod mn),m \bot n \Leftrightarrow a \equiv b(\bmod m),a \equiv b(\bmod n)\\a \equiv b(\bmod m) \Leftrightarrow \forall p,a \equiv b(\bmod {p^{ {m_p}}})\end{array}\]<br>其中$m = \prod\nolimits_p { {p^{ {m_p}}}} $是$m$的素因子分解。<br>第三条性质是中国剩余定理的特例，今后我们再做证明。</p><h1 id="独立剩余"><a href="#独立剩余" class="headerlink" title="独立剩余"></a>独立剩余</h1><hr><p>同余的应用之一就是剩余系，将整数$x$表示为一组互素的模的剩余（余数）序列：<br>\[(x\bmod {m_1}, \ldots ,x\bmod {m_r})\]<br>其中模$m$两两互素。</p><p>通过这个剩余序列可以确定出$x$的通解，其实可以看出来，这就是中国剩余定理的另一种表示形式。</p><p>这种表示形式有很多好处，比如可以直接在每个维度上面进行加减乘法。例如对于$m_1 = 3, m_2 = 5$的剩余系，有如下表示：<br>\[13 = (1,3),7 = (1,2)\]<br>那么$13 \cdot 7\,\bmod \,15$就可以这样计算：<br>\[(1 \cdot 1\bmod 3,3 \cdot 2\bmod 5) = (1,1)\]<br>所以<br>\[13 \cdot 7\bmod 15 = 1 \cdot 1\bmod 15 = 1\]</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第10课（素数和阶乘的有趣性质）</title>
      <link href="/2018/04/28/concrete-math-10/"/>
      <url>/2018/04/28/concrete-math-10/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=25639007&auto=1&height=66"></iframe></div><h1 id="欧几里得数"><a href="#欧几里得数" class="headerlink" title="欧几里得数"></a>欧几里得数</h1><hr><p>首先我们来证明一下，素数有无穷多个。</p><p>假设素数只有$k$个，分别为$2,3, \ldots ,{P_k}$，那么我们构造下面的数字：<br>\[M = 2 \cdot 3 \cdot  \ldots  \cdot {P_k} + 1\]<br>显然$M$无法被$2,3, \ldots ,{P_k}$中的任意一个整除，那么要么$M$可以被其他的素数整除，要么$M$自己就是一个素数。所以素数有无穷多个。</p><p>下面我们来定义欧几里得数，是用递归形式来定义的：<br>\[{e_n} = {e_0}{e_1}{e_2} \ldots {e_{n - 1}} + 1\]</p><p>那么欧几里得数是否是素数呢？当然不是的，${e_5} = 1807 = 13 \cdot 139$。</p><p>但是欧几里得数还是有很多奇妙的性质。</p><p><strong>性质1</strong><br>\[\gcd ({e_m},{e_n}) = 1,m \ne n\]<br><strong>证明：</strong><br>假设$n &gt; m$，那么有<br>\[\gcd ({e_m},{e_n}) = \gcd ({e_m},{e_0}{e_1} \ldots {e_m}{e_{m + 1}} \ldots {e_{n - 1}} + 1) = \gcd (1,{e_m}) = 1\]<br><strong>性质2</strong><br>如果令$q_j$等于$e_j$的最小素因子，那么${q_1},{q_2}, \ldots $就是一个不重复的素数序列，这也证明了素数有无穷多个。<br><strong>性质3</strong><br>\[{e_n} = {e_0}{e_1}{e_2} \ldots {e_{n - 1}} + 1 = ({e_{n - 1}} - 1){e_{n - 1}} + 1 = e_{n - 1}^2 - {e_{n - 1}} + 1\]<br>在后面的章节可以证明：<br>\[{e_n} = \left\lfloor { {E^{ {2^n}}} + \frac{1}{2}} \right\rfloor \]<br>其中$E \approx 1.264$</p><p>下面我们稍稍探究一下下面这个数的性质：<br>\[{2^p} - 1\]<br>这个数如果是素数，那么就被叫做<strong>梅森素数</strong>，那么它在什么情况下是素数呢？</p><p>首先$p$不能是合数，因为有<br>\[{2^{km}} - 1 = ({2^m} - 1)({2^{m(k - 1)}} + {2^{m(k - 2)}} +  \cdots  + 1)\]<br>但是如果$p$是素数，这个数也不一定是素数，2017年年末美国一个电气工程师发现了人类历史上最大的梅森素数——${2^{77232917}} - 1$。</p><h1 id="阶乘"><a href="#阶乘" class="headerlink" title="阶乘"></a>阶乘</h1><p>阶乘定义如下：<br>\[n! = 1 \cdot 2 \cdot  \ldots  \cdot n = \prod\limits_{k = 1}^n k \]<br>所以有<br>\[n{!^2} = (1 \cdot 2 \cdot  \ldots  \cdot n)(n \cdot  \ldots  \cdot 2 \cdot 1) = \prod\limits_{k = 1}^n {k(n + 1 - k)} \]<br>由基本不等式可以得到<br>\[n \le k(n + 1 - k) \le \frac{ { { {(n + 1)}^2}}}{4}\]<br>所以<br>\[\prod\limits_{k = 1}^n n  \le n{!^2} \le \prod\limits_{k = 1}^n {\frac{ { { {(n + 1)}^2}}}{4}} \]<br>所以<br>\[{n^{n/2}} \le n! \le \frac{ { { {(n + 1)}^n}}}{ { {2^n}}}\]<br>这里得到了阶乘的一个粗略范围，在后面章节中，我们会得到阶乘的一个更精确的表达式：<br>\[n! \sim \sqrt {2\pi n} {\left( {\frac{n}{e}} \right)^n}\]<br>这就是斯特林数，搞ACM还是很有用的。</p><p>下面我们来探讨$n!$中含有多少个素因子$p$，个数记为${\varepsilon _p}(n!)$。</p><p>从特殊情况讨论起，当$p = 2$的时候，我们首先看$n!$含有多少个2，然后看有多少个4，再看有多少个8，依次下去，所以答案为：<br>\[{\varepsilon _2}(n!) = \sum\limits_{k \ge 1} {\left\lfloor {\frac{n}{ { {2^k}}}} \right\rfloor } \]<br>可以看出，这个答案不就是$n$的二进制表示不停右移1位，然后相加吗？所以又可以写成：<br>\[{\varepsilon _2}(n!) = n - {\nu _2}(n)\]<br>其中${\nu _2}(n)$表示$n$的二进制表示中1的个数。</p><p>推广到一般情况：<br>\[{\varepsilon _p}(n!) = \sum\limits_{k \ge 1} {\left\lfloor {\frac{n}{ { {p^k}}}} \right\rfloor } \]<br>放缩一下有：<br>\[{\varepsilon _p}(n!) = \sum\limits_{k \ge 1} {\left\lfloor {\frac{n}{ { {p^k}}}} \right\rfloor }  &lt; \sum\limits_{k \ge 1} {\frac{n}{ { {p^k}}} = \frac{n}{ {p - 1}}} \]</p><p>如果我们令$p = 2$和$p = 3$可以发现：<br>\[{\varepsilon _2}(n!) \approx 2{\varepsilon _3}(n!)\]<br>但是这个式子在什么情况下相等呢？这仍然是一个未解之谜。</p><p>所以$p$对$n!$的贡献度满足如下式子：<br>\[{p^{ {\varepsilon _p}(n!)}} &lt; {p^{\frac{n}{ {p - 1}}}}\]<br>又因为$p \le 2^{p - 1}$，所以<br>\[{p^{ {\varepsilon _p}(n!)}} &lt; {p^{\frac{n}{ {p - 1}}}} \le {2^n}\]<br>假设素数只有$k$个，分别为$2,3, \ldots ,{P_k}$，那么有<br>\[n! &lt; {({2^n})^k}\]<br>如果我们令$n = {2^{2k}}$，那么<br>\[n! &lt; {({2^n})^k} = {2^{k{2^{2k}}}} = {2^{2k{2^{2k}}/2}} = {n^{n/2}}\]<br>这与我们之前推过的不等式矛盾！所以一定有无穷个素数。</p><p>设小于等于$n$的素数个数为$\pi (n)$，所以<br>\[n! &lt; {2^{n\pi (n)}}\]<br>根据斯特林数公式，我们可以得到<br>\[\pi (n) &gt; \lg (n/e)\]</p><h1 id="互素"><a href="#互素" class="headerlink" title="互素"></a>互素</h1><hr><p><strong>定义</strong><br>$m$和$n$互素定义为$gcd(m, n) = 1$，记作$m \bot n$。</p><p>互素也有很多性质。</p><p><strong>性质1</strong><br>\[m/\gcd (m,n) \bot n/\gcd (m,n)\]<br><strong>性质2</strong><br>\[m \bot n \Leftrightarrow \min ({m_p},{n_p}) = 0\]<br>其中${m_p},{n_p}$就是两个数的素数指数表示法，详细定义见上一节课。<br>或者可以表示为<br>\[m \bot n \Leftrightarrow {m_p}{n_p} = 0\]<br><strong>性质3</strong><br>\[k \bot m,k \bot n \Leftrightarrow k \bot mn\]</p><h1 id="Stern-Brocot树"><a href="#Stern-Brocot树" class="headerlink" title="Stern-Brocot树"></a>Stern-Brocot树</h1><hr><p><img src="1.png" alt><br>如上图所示，Stern-Brocot树就是0到1之间的分数生成的一棵二叉树。</p><p>初始时只有$\frac{0}{1},\frac{1}{0}$两个数，第一轮将两者分母相加，分子也相加作为新的分数的分母分子。第二轮再对相邻的两个分数做相同的操作，生成新的分数序列。不断生成下去，得到了上图的二叉树。</p><p>Stern-Brocot树有下面四个性质：</p><ol><li>0到1之间的所有有理数都出现在了这棵树中。</li><li>每个分数仅出现了1次。</li><li>每个分数都是不可约分的，即分子分母互素。</li><li>生成的序列是单调递增的。</li></ol><p>下面我们来一个一个证明。</p><p><strong>引理</strong><br>对于相邻的两个分数$\frac{m}{n},\frac{ {m’}}{ {n’}}$，满足：<br>\[m’n - mn’ = 1\]<br><strong>证明</strong><br>用数学归纳法证明。</p><p>性质4就是证明：<br>\[\frac{m}{n} &lt; \frac{ {m + m’}}{ {n + n’}} &lt; \frac{ {m’}}{ {n’}}\]<br>结论是很显然的，这样性质2同时就成立了。</p><p>性质1的话，对于任意有理数$\frac{a}{b}$，假设$\frac{m}{n} &lt; \frac{a}{b} &lt; \frac{ {m’}}{ {n’}}$。<br>我们采用如下策略生成$\frac{a}{b}$。</p><ul><li>如果$\frac{ {m + m’}}{ {n + n’}} = \frac{a}{b}$，那么成功。</li><li>如果$\frac{ {m + m’}}{ {n + n’}} &lt; \frac{a}{b}$，那么令$m = m + m’,n = n + n’$。</li><li>如果$\frac{ {m + m’}}{ {n + n’}} &gt; \frac{a}{b}$，那么令$m’ = m + m’,n’ = n + n’$。</li></ul><p>那么有<br>\[an - bm \ge 1,bm’ - an’ \ge 1\]<br>所以<br>\[(m’ + n’)(an - bm) + (m + n)(bm’ - an’) \ge m’ + n’ + m + n\]<br>而左边式子就等于$a + b$，所以<br>\[a + b \ge m’ + n’ + m + n\]<br>因为$m,n,m’,n’$都在不断增加，所以最多$a + b$轮就能生成$\frac{a}{b}$。</p><p>性质3的话，同样用数学归纳法。通过引理可以得到<br>\[(m + m’)n - m(n + n’) = 1\]<br>由扩展欧几里得定理可以得到$m + m’$与$n + n’$互素。</p><p><strong>Farey序列</strong><br>我们引申出Farey序列的概念，定义如下：<br>\[{F_n} = \{ a/b|\gcd (a,b) = 1,0 \le a,b \le n,0 \le a/b \le 1\} \]<br>关于它的更多性质，留到下一节课继续。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K-best Iterative Viterbi Parsing</title>
      <link href="/2018/04/24/ivp-eacl17/"/>
      <url>/2018/04/24/ivp-eacl17/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=426852531&auto=1&height=66"></iframe></div><p>本文链接：<a href="https://www.aclweb.org/anthology/E/E17/E17-2049.pdf" target="_blank" rel="noopener">EACL17</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>CKY算法或维特比inside算法是成分句法分析的主要方法之一，但是当产生式数量特别大之后，时间复杂度也线性增大。可行的一种方法是剪枝，但是剪枝会造成准确率的下降。所以本文就提出了一种迭代的维特比句法分析算法，通过剪枝去除掉没用的边。实验表明，时间上加快了一个数量级，但是本文并没有说准确率怎么样。。。</p><p>本文用到的inside和outside算法之前已经介绍过了，详见<a href="http://godweiyang.com/2018/04/19/inside-outside/">PCFG中inside和outside算法详解</a>。</p><h1 id="算法框架"><a href="#算法框架" class="headerlink" title="算法框架"></a>算法框架</h1><hr><h2 id="分层聚类"><a href="#分层聚类" class="headerlink" title="分层聚类"></a>分层聚类</h2><p>首先提出分层聚类的概念。</p><p><img src="1.jpg" alt><br>如上图所示，原来的类别标记有很多，将他们聚类成几个小类，再将这几个小类聚成更小的类，依次下去，最后类别标记会少很多很多。<br><img src="2.jpg" alt><br>以上图为例，${X_1}{\rm{ = \{ }}A,B{\rm{\} ,}}{X_2}{\rm{ = \{ }}C,D{\rm{\} }}$，聚类之后的分析表为b图，原始的分析表为a图，聚类之后的表（下面叫<strong>粗表</strong>）b唯一对应了聚类之前的表（下面叫<strong>原始表</strong>）a，而反过来原始表a能对应多种不同的粗表b。</p><h2 id="形式化定义"><a href="#形式化定义" class="headerlink" title="形式化定义"></a>形式化定义</h2><p>我们将类别分为$m + 1$层，分别表示为${N_0} \ldots {N_m}$，那么第$m$层的类别集合$N_m$就是原始的类别集合，而$0$到$m - 1$层的类别就称之为<strong>收缩符号</strong>。</p><p>对于$0 \le i \le j \le m$，我们定义${\pi _{i \to j}}:{N_i} \mapsto \Im ({N_j})$，其中$\Im ({N_j})$就是$N_j$的一个子集。该式将$N_i$中的一个类别$X_i$映射为了$N_j$中所有聚类为$X_i$的类别集合。</p><p>举个例子吧，在第一张图中，${\pi _{1 \to 2}}(ADJ\_) = \{ JJ,JJR,JJS\} $。如果$i = j$，那么${\pi _{i \to j}}(A) = \{ A\} $。</p><p>那么对于${X_i} \in {N_i},{X_j} \in {N_j},{X_k} \in {N_k}$，我们定义产生式${X_i} \to {X_j}{X_k}$的概率为：<br>\[\log q({X_i} \to {X_j}{X_k}) = \mathop {\max }\limits_{\scriptstyle A \in {\pi _{i \to m}}({X_i})\atop{\scriptstyle B \in {\pi _{j \to m}}({X_j})\atop\scriptstyle C \in {\pi _{k \to m}}({X_k})}} \log q(A \to BC)\]<br>也就是说，粗表中的每一棵句法树都给出了它在原始表中的句法树的分数的上界，通俗说就是，如果把粗表中的收缩符号全部替换成原始表中的符号，那么新的句法树的分数一定会小于等于粗表中的句法树。</p><h2 id="引理"><a href="#引理" class="headerlink" title="引理"></a>引理</h2><blockquote><p>如果粗表中的最优句法树${\hat d}$不包含任意收缩符号，那么它等价于原始表中的最优句法树。</p></blockquote><p><strong>证明：</strong><br>令$Y$等于原始表中的句法树集合，$Y’ \subset Y$等于没有出现在粗表中，但是出现在原始表中的句法树集合，${Y’’}$等于粗表中的句法树集合。</p><p>那么对于每一个句法树$d \in Y’$，都存在唯一的句法树$d’ \in Y’’$与之对应。所以可以推出：<br>\[\forall d \in Y,\exists d’ \in Y’’,s(d) \le s(d’) &lt; s(\hat d)\]<br>这就意味着$\hat d$也是原始表中的最优句法树。</p><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><p><img src="3.jpg" alt></p><ul><li><p>$lb$初始化为句法树的最优得分或者负无穷，其中<code>det()</code>用来求解句法树的最优得分，但是没有必要真的求出最优句法树，只需要在每个结点处保留得分最高的边即可。尽管这样得出来的句法树基本不是最高的，但是能够缩小$lb$范围即可。</p></li><li><p><code>init-chart()</code>首先初始化分析表，全部初始化为收缩符号。</p></li><li><p>然后开始迭代过程，首先执行维特比inside算法，也就是CKY算法<code>Viterbi-inside()</code>，得到最优句法树$\hat d$。</p></li><li><p>如果最优句法树不含有任意收缩符号，那么迭代结束，直接返回该句法树。</p></li><li><p>否则的话，更新$lb$为最优句法树的分数<code>best()</code>。</p></li><li><p><code>expand-chart()</code>将所有收缩符号替换为下一层的收缩符号。</p></li><li><p><code>Viterbi-outside()</code>计算outside值。</p></li><li><p><code>prune-chart()</code>进行剪枝，过滤掉无用的边。</p></li></ul><h2 id="剪枝过程"><a href="#剪枝过程" class="headerlink" title="剪枝过程"></a>剪枝过程</h2><p>算法的重要部分就是<code>prune-chart()</code>剪枝过程，这里要详细讲一下。</p><p>对于一条边$e = (A,i,j)$，定义$\alpha \beta (e) = \alpha (e) + \beta (e)$为含有边$e$的句法树的最大分数。那么如果<br>$\alpha \beta (e) &lt; lb$，这条边$e$就没有搜索的必要了，可以从分析表中去掉。</p><p>但是每次迭代都从原始表中计算$\alpha \beta (e)$值太麻烦了，可以在每次迭代的时候计算粗表中的值：<br>\[\alpha \beta (e) \le \hat \alpha (e) + \hat \beta (e) = \hat {\alpha \beta} (e)\]</p><p>所以当$\hat {\alpha \beta} (e) &lt; lb$时，从分析表中删除这条边。虽然搜索空间减少了，但是不影响算法的迭代轮数。</p><p>虽然在<code>expand-chart()</code>这一步要扩展收缩符号为下一层所有符号，但是实际运行起来时间比普通的CKY算法大大减少。</p><h2 id="K-best扩展"><a href="#K-best扩展" class="headerlink" title="K-best扩展"></a>K-best扩展</h2><p><img src="4.jpg" alt></p><p>基本框架和1-best是一样的，主要思路就是首先求出最优句法树，如果包含收缩符号，那么就下面步骤和1-best一样。否则的话求出后面k-1棵最优的句法树，如果都不包含收缩符号，直接返回k-best棵句法树。否则从中选出最好的一棵含有收缩符号的句法树，下面的步骤和1-best一样。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><p>数据集用的是PTB中长度小于35的句子。<br><img src="5.jpg" alt></p><p>上面这张表显示出，IVP算法的边的数量远远小于CKY算法，虽然迭代次数大大增加，但是总时间仍然远远小于CKY算法，而且边数减少了之后inside和outside算法的时间可以忽略不计了。最后一行是平均数据。<br><img src="6.jpg" alt><br>上图说明了，当k较小时，IVP算法时间快于普通的k-best算法，但是k大了之后就变慢了，原因如下图所示：<br><img src="7.jpg" alt><br>当k太大了之后，lb不能很好的得到最优得分的下界，所以无法有效地剪枝。而且k越小，算法收敛的也越快。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><hr><p>提出了K-best IVP算法，基本框架还是inside-outside算法。</p><p>但是全文自始自终没有提及算法的准确率，感觉应该不是很高，不知道有没有又高又快的优化方法？</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
            <tag> EACL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第9课（取整进阶与数论入门）</title>
      <link href="/2018/04/23/concrete-math-9/"/>
      <url>/2018/04/23/concrete-math-9/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=108910&auto=1&height=66"></iframe></div><p>今天讲完了取整的最后一部分知识，并给第四章数论开了个头。</p><p>首先还是以一道例题开始我们今天的课程。</p><h1 id="例题1"><a href="#例题1" class="headerlink" title="例题1"></a>例题1</h1><hr><p>求和：<br>\[\sum\limits_{0 \le k &lt; n} {\left\lfloor {\sqrt k } \right\rfloor } \]</p><h2 id="方法1"><a href="#方法1" class="headerlink" title="方法1"></a>方法1</h2><p>首先令$m = \left\lfloor {\sqrt k } \right\rfloor $<br>那么有<br>\[\begin{array}{l}\sum\limits_{0 \le k &lt; n} {\left\lfloor {\sqrt k } \right\rfloor }  = \sum\limits_{k,m \ge 0} {m\left[ {k &lt; n} \right]\left[ {m = \left\lfloor {\sqrt k } \right\rfloor } \right]} \\ = \sum\limits_{k,m \ge 0} {m\left[ {k &lt; n} \right]\left[ {m \le \sqrt k  &lt; m + 1} \right]} \\ = \sum\limits_{k,m \ge 0} {m\left[ {k &lt; n} \right]\left[ { {m^2} \le k &lt; { {(m + 1)}^2}} \right]} \\ = \sum\limits_{k,m \ge 0} {m\left[ { {m^2} \le k &lt; { {(m + 1)}^2} \le n} \right]} \\ + \sum\limits_{k,m \ge 0} {m\left[ { {m^2} \le k &lt; n &lt; { {(m + 1)}^2}} \right]} \end{array}\]<br>我们先算左半部分，先假设$n = {a^2}$，那么有<br>\[\begin{array}{l}\sum\limits_{k,m \ge 0} {m\left[ { {m^2} \le k &lt; { {(m + 1)}^2} \le {a^2}} \right]} \\ = \sum\limits_{m \ge 0} {m(2m + 1)\left[ {m &lt; a} \right]} \\ = \frac{1}{6}(4a + 1)a(a - 1)\end{array}\]<br>而对于一般的$n$，令$a = \left\lfloor {\sqrt n } \right\rfloor $，我们只需要计算${a^2} \le k &lt; n$的部分，而这部分$\sqrt k  = a$，所以结果为$(n - {a^2})a$。</p><p>所以总的结果为：<br>\[\sum\limits_{0 \le k &lt; n} {\left\lfloor {\sqrt k } \right\rfloor }  = na - \frac{1}{3}{a^3} - \frac{1}{2}{a^2} - \frac{1}{6}a,a = \left\lfloor {\sqrt n } \right\rfloor \]</p><p>这里解释一下为什么没有算右半部分？因为右半部分就是${a^2} \le k &lt; n$的这部分，已经计算过了。</p><h2 id="方法2"><a href="#方法2" class="headerlink" title="方法2"></a>方法2</h2><p>因为$\left\lfloor x \right\rfloor  = \sum\nolimits_j {\left[ {1 \le j \le x} \right]} $，所以可以将原式替换掉，还是令$n = {a^2}$，然后如下计算：<br>\[\begin{array}{l}\sum\limits_{0 \le k &lt; n} {\left\lfloor {\sqrt k } \right\rfloor }  = \sum\limits_{j,k} {\left[ {1 \le j \le \sqrt k } \right]\left[ {0 \le k &lt; {a^2}} \right]} \\ = \sum\limits_{1 \le j &lt; a} {\sum\limits_k {\left[ { {j^2} \le k &lt; {a^2}} \right]} } \\ = \sum\limits_{1 \le j &lt; a} {({a^2} - {j^2})}  = {a^3} - \frac{1}{3}a(a + \frac{1}{2})(a + 1)\end{array}\]<br>其中第二行交换了变量计算顺序。</p><h1 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h1><hr><p>这里直接介绍一个定理，就不证明了，过程比较复杂：<br>\[\mathop {\lim }\limits_{n \to \infty } \frac{1}{n}\sum\limits_{0 \le k &lt; n} {f(\{ k\alpha \} )}  = \int_0^1 {f(x)dx} \]<br>其中$\alpha $是一个无理数。</p><p>这个公式说明了，无理数$\alpha $的整数倍的小数部分均匀分布在$(0,1)$之间。</p><p>这就给了我们一个启示，我们可以用它来生成随机数啊！其他用处还有很多，自己想咯。</p><h1 id="例题2"><a href="#例题2" class="headerlink" title="例题2"></a>例题2</h1><hr><p>求如下和式：<br>\[\sum\limits_{0 \le k &lt; m} {\left\lfloor {\frac{ {nk + x}}{m}} \right\rfloor } \]<br>其中整数$m &gt; 0$，$n$也是整数。</p><p>通过枚举$m = 1,2,3, \ldots $，可以发现和式满足如下形式：<br>\[a\left\lfloor {\frac{x}{a}} \right\rfloor  + bn + c\]<br>那么怎么计算出来呢？</p><p>首先做一个变形：<br>\[\left\lfloor {\frac{ {x + kn}}{m}} \right\rfloor  = \left\lfloor {\frac{ {x + kn\bmod m}}{m}} \right\rfloor  + \frac{ {kn}}{m} - \frac{ {kn\bmod m}}{m}\]<br>这就将原来的和式分为了三个部分求和。</p><p><strong>第一个部分为：</strong><br>\[\left\lfloor {\frac{x}{m}} \right\rfloor  + \left\lfloor {\frac{ {x + n\bmod m}}{m}} \right\rfloor  +  \cdots  + \left\lfloor {\frac{ {x + (m - 1)n\bmod m}}{m}} \right\rfloor \]<br>具体怎么算留到下一章节，这里通过枚举可以发现它的值是有周期的，周期重复次数是$d = \gcd (m,n)$。所以算出来结果为：<br>\[\begin{array}{l}d\left( {\left\lfloor {\frac{x}{m}} \right\rfloor  + \left\lfloor {\frac{ {x + d}}{m}} \right\rfloor  +  \cdots  + \left\lfloor {\frac{ {x + m - d}}{m}} \right\rfloor } \right)\\ = d\left( {\left\lfloor {\frac{ {x/d}}{ {m/d}}} \right\rfloor  + \left\lfloor {\frac{ {x/d + 1}}{ {m/d}}} \right\rfloor  +  \cdots  + \left\lfloor {\frac{ {x/d + m/d - 1}}{ {m/d}}} \right\rfloor } \right)\\ = d\left\lfloor {\frac{x}{d}} \right\rfloor \end{array}\]<br><strong>第二个部分为：</strong><br>\[\sum\limits_{0 \le k &lt; m} {\frac{ {kn}}{m}}  = \frac{ {(m - 1)n}}{2}\]<br><strong>第三个部分为：</strong><br>\[d\left( {\frac{0}{m} + \frac{d}{m} +  \cdots  + \frac{ {m - d}}{m}} \right) = \frac{ {m - d}}{2}\]</p><p>所以总的结果为：<br>\[\sum\limits_{0 \le k &lt; m} {\left\lfloor {\frac{ {nk + x}}{m}} \right\rfloor }  = d\left\lfloor {\frac{x}{d}} \right\rfloor  + \frac{ {(m - 1)n}}{2} + \frac{ {d - m}}{2}\]</p><p>这里我们对结果稍稍变形，可以得到另一个结果：<br>\[\begin{array}{l}\sum\limits_{0 \le k &lt; m} {\left\lfloor {\frac{ {nk + x}}{m}} \right\rfloor }  = d\left\lfloor {\frac{x}{d}} \right\rfloor  + \frac{ {(m - 1)(n - 1)}}{2} + \frac{ {m - 1}}{2} + \frac{ {d - m}}{2}\\ = d\left\lfloor {\frac{x}{d}} \right\rfloor  + \frac{ {(m - 1)(n - 1)}}{2} + \frac{ {d - 1}}{2}\end{array}\]<br>可以发现，$m$和$n$是对称的！所以可以得到如下结论：<br>\[\sum\limits_{0 \le k &lt; m} {\left\lfloor {\frac{ {nk + x}}{m}} \right\rfloor }  = \sum\limits_{0 \le k &lt; n} {\left\lfloor {\frac{ {mk + x}}{n}} \right\rfloor } \]<br>这有什么用呢？当$m$特别大、$n$很小的时候可以大大减少项的个数！</p><p>如果我们令$n=1$，就会发现，得到的式子和之前证过的一个式子一模一样！<br>\[\sum\limits_{0 \le k &lt; m} {\left\lfloor {\frac{ {k + x}}{m}} \right\rfloor }  = \left\lfloor x \right\rfloor \]</p><p>到这里为止，第三章取整就讲完了，下面开始讲第四章数论部分。</p><h1 id="数论相关性质"><a href="#数论相关性质" class="headerlink" title="数论相关性质"></a>数论相关性质</h1><hr><h2 id="整除定义"><a href="#整除定义" class="headerlink" title="整除定义"></a>整除定义</h2><p>\[m|n \Leftrightarrow m &gt; 0,n = mk,k \in \mathbb{Z}\]<br>注意这里整除的定义中要求$m&gt;0$。</p><h2 id="最大公约数和最小公倍数"><a href="#最大公约数和最小公倍数" class="headerlink" title="最大公约数和最小公倍数"></a>最大公约数和最小公倍数</h2><p>定义我就不说了，大家应该都知道的。</p><h2 id="欧几里得定理"><a href="#欧几里得定理" class="headerlink" title="欧几里得定理"></a>欧几里得定理</h2><p>又叫辗转相除法，就是用来求最大公约数的。<br>\[\begin{array}{l}\gcd (0,n) = n\\\gcd (m,n) = \gcd (n\bmod m,m)\end{array}\]</p><h2 id="扩展欧几里得定理"><a href="#扩展欧几里得定理" class="headerlink" title="扩展欧几里得定理"></a>扩展欧几里得定理</h2><p>在用欧几里得定理求到最大公约数之后，反过来可以将最大公约数表示为两个数的线性和：<br>\[\gcd (m,n) = m’m + n’n\]</p><h2 id="性质1"><a href="#性质1" class="headerlink" title="性质1"></a>性质1</h2><p>如果$k|m,k|n$，那么$k|gcd(m,n)$。</p><h2 id="性质2"><a href="#性质2" class="headerlink" title="性质2"></a>性质2</h2><p>\[\sum\limits_{m|n} { {a_m}}  = \sum\limits_{m|n} { {a_{n/m}}} \]<br>这个就是用了交换律，按照因子顺序倒过来算。</p><h2 id="性质3"><a href="#性质3" class="headerlink" title="性质3"></a>性质3</h2><p>\[\sum\limits_{m|n} { {a_m}}  = \sum\limits_k {\sum\limits_{m &gt; 0} { {a_m}[n = mk]} } \]<br>这个虽然变成了二重求和，但是对于每个$k$，其实只有一个$m$有效。</p><h2 id="性质4"><a href="#性质4" class="headerlink" title="性质4"></a>性质4</h2><p>\[\sum\limits_{m|n} {\sum\limits_{k|m} { {a_{k,m}}} }  = \sum\limits_{k|n} {\sum\limits_{l|(n/k)} { {a_{k,kl}}} } \]<br>这个一眼就不一定能看出来了。</p><p>左边等于：<br>\[\begin{array}{l}\sum\limits_{m|n} {\sum\limits_{k|m} { {a_{k,m}}} }  = \sum\limits_{j,l} {\sum\limits_{k,m &gt; 0} { {a_{k,m}}[n = jm][m = kl]} } \\ = \sum\limits_j {\sum\limits_{k,l &gt; 0} { {a_{k,kl}}[n = jkl]} } \end{array}\]<br>右边等于：<br>\[\begin{array}{l}\sum\limits_{k|n} {\sum\limits_{l|(n/k)} { {a_{k,kl}}} }  = \sum\limits_{j,m} {\sum\limits_{k,l &gt; 0} { {a_{k,kl}}[n = jk][n/k = ml]} } \\ = \sum\limits_m {\sum\limits_{k,l &gt; 0} { {a_{k,kl}}[n = mlk]} } \end{array}\]<br>可以看出左右两边相等。</p><h2 id="算数基本定理"><a href="#算数基本定理" class="headerlink" title="算数基本定理"></a>算数基本定理</h2><p>一个整数可以唯一表示为若干个素数乘积：<br>\[n = \prod\limits_p { {p^{ {n_p}}}} ,{n_p} \ge 0\]<br>所以用指数形式来表示一个整数$n$，例如$18 = {2^1} \times {3^2}$，那么$18$可以表示为：<br>\[ &lt; 1,2,0,0, \ldots  &gt; \]<br>最大公约数和最小公倍数也能很方便的用指数形式计算：<br>其中最大公约数的每个素数的指数等于两个数对应指数最小值，最小公倍数的每个素数的指数等于两个数对应指数最大值。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第三章作业解答</title>
      <link href="/2018/04/20/concrete-math-hw3/"/>
      <url>/2018/04/20/concrete-math-hw3/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=29534449&auto=1&height=66"></iframe></div><h1 id="题3"><a href="#题3" class="headerlink" title="题3"></a>题3</h1><hr><p><strong>题目</strong><br>求$\left\lfloor {nx} \right\rfloor  = n\left\lfloor x \right\rfloor $的充要条件。<br><strong>解答</strong><br>因为</p><p>\[<br>x = \left\lfloor x \right\rfloor  + \{ x\}<br>\]<br>所以</p><p>\[<br>\left\lfloor {nx} \right\rfloor  = \left\lfloor {n(\left\lfloor x \right\rfloor  + \{ x\} )} \right\rfloor  = n\left\lfloor x \right\rfloor  + \left\lfloor {n\{ x\} } \right\rfloor<br>\]<br>要使得$\left\lfloor {nx} \right\rfloor  = n\left\lfloor x \right\rfloor$，就必须有</p><p>\[<br>\left\lfloor {n\{ x\} } \right\rfloor  = 0<br>\]<br>所以</p><p>\[<br>n\{ x\}  &lt; 1<br>\]<br>即</p><p>\[<br>\{ x\}  &lt; \frac{1}{n}<br>\]</p><h1 id="题7"><a href="#题7" class="headerlink" title="题7"></a>题7</h1><hr><p><strong>题目</strong><br>求下列递推式<br>\[\begin{array}{l}{X_n} = n,0 \le n &lt; m\\{X_n} = {X_{n - m}} + 1,n \ge m\end{array}\]<br><strong>解答</strong><br>因为</p><p>\[<br>n - \left\lfloor {\frac{n}{m}} \right\rfloor m = n\bmod m &lt; m<br>\]<br>所以</p><p>\[<br>\begin{array}{l}{X_n} = {X_{n - m}} + 1 = {X_{n - 2m}} + 2 =  \cdots \\ = {X_{n - \left\lfloor {\frac{n}{m}} \right\rfloor m}} + \left\lfloor {\frac{n}{m}} \right\rfloor  = n - \left\lfloor {\frac{n}{m}} \right\rfloor m + \left\lfloor {\frac{n}{m}} \right\rfloor \\ = n\,\bmod \,m + \left\lfloor {\frac{n}{m}} \right\rfloor \end{array}<br>\]</p><h1 id="题8"><a href="#题8" class="headerlink" title="题8"></a>题8</h1><hr><p><strong>题目</strong><br>$n$个物品放到$m$个盒子中，求证至少有一个盒子物品数大于等于$\left\lceil {\frac{n}{m}} \right\rceil$，至少有一个盒子物品数小于等于$\left\lfloor {\frac{n}{m}} \right\rfloor$。<br><strong>解答</strong><br>假设所有的盒子物品数都小于$\left\lceil {\frac{n}{m}} \right\rceil$，那么总物品数$S$满足</p><p>\[<br>S \le m(\left\lceil {\frac{n}{m}} \right\rceil  - 1)<br>\]<br>令$n = qm + r,0 \le r &lt; m$，那么有</p><p>\[<br>S \le m(\left\lceil {q + \frac{r}{m}} \right\rceil  - 1) = qm - m + m\left\lceil {\frac{r}{m}} \right\rceil<br>\]<br>如果$r=0$，那么有</p><p>\[<br>S \le qm - m &lt; n<br>\]<br>如果$r&gt;0$，那么有</p><p>\[<br>S \le qm &lt; n<br>\]<br>这与$S=n$矛盾！所以至少有一个盒子物品数大于等于$\left\lceil {\frac{n}{m}} \right\rceil$。</p><p>假设所有的盒子物品数都大于$\left\lfloor {\frac{n}{m}} \right\rfloor$，那么总物品数$S$满足</p><p>\[<br>S \ge m(\left\lfloor {\frac{n}{m}} \right\rfloor  + 1)<br>\]<br>令$n = qm + r,0 \le r &lt; m$，那么有</p><p>\[<br>S \ge m(\left\lfloor {q + \frac{r}{m}} \right\rfloor  + 1) = qm + m + m\left\lfloor {\frac{r}{m}} \right\rfloor  = qm + m &gt; qm + r = n<br>\]<br>这与$S=n$矛盾！所以至少有一个盒子物品数小于等于$\left\lfloor {\frac{n}{m}} \right\rfloor$。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inside-outside Algorithm in PCFG</title>
      <link href="/2018/04/19/inside-outside/"/>
      <url>/2018/04/19/inside-outside/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=506196018&auto=1&height=66"></iframe></div><p>inside-outside算法是用来预测一棵句法分析树的概率的算法，算法建立在文法是乔姆斯基范式（CFG）的基础之上，CFG的定义见<a href="https://en.wikipedia.org/wiki/Chomsky_normal_form" target="_blank" rel="noopener">维基百科</a>。一棵句法分析树的potential定义为它包含的产生式的potential乘积，在PCFG中表示概率，在CRF-CFG中表示特征集合的分数。</p><p>inside-outside算法需要定义两个变量：</p><ul><li>$\alpha (A,i,j)$定义为内部的potential之和，即以$A$为根结点，短语为${x_{i;j}}$的所有可能的子树的potential之和。</li><li>$\beta (A,i,j)$定义为外部的potential之和，即以$A$为根结点，短语为${x_{1;i - 1}}A{x_{j + 1;n}}$的所有可能的子结构的potential之和。</li></ul><p><strong>给定文法CFG，输入字符串${x_{1;n}}$，计算inside和outside值。</strong></p><h1 id="inside"><a href="#inside" class="headerlink" title="inside"></a>inside</h1><hr><p>初始化：<br>如果$A \to {x_i} \in R$，那么$\alpha (A,i,i) = \varphi (A \to {x_i},i,i,i)$。否则就等于0。<br>其中$\varphi (A \to {x_i},i,i,i)$为potential值。</p><p>类似于CKY算法，自底向上计算inside值：<br>\[\alpha (A,i,j) = \sum\limits_{A \to BC \in R} {\sum\limits_{k = i}^{j - 1} {\varphi (A \to BC,i,k,j) \cdot \alpha (B,i,k) \cdot \alpha (C,k + 1,j)} } \]</p><h1 id="outside"><a href="#outside" class="headerlink" title="outside"></a>outside</h1><hr><p>初始化：<br>$\beta (S,1,n) = 1$，其余都等于0。</p><p>outside值要分为两部分计算：<br><img src="1.jpg" alt><br>第一部分是${B \to AC}$，如上图所示。<br><img src="2.jpg" alt><br>第二部分是${B \to CA}$，如上图所示。</p><p>和inside相反，通过自顶向下计算outside值：<br>\[\begin{array}{l}\beta (A,i,j) = \sum\limits_{B \to AC \in R} {\sum\limits_{k = j + 1}^n {\varphi (B \to AC,i,j,k) \cdot \beta (B,i,k) \cdot \alpha (C,j + 1,k)} } \\ + \sum\limits_{B \to CA \in R} {\sum\limits_{k = 1}^{i - 1} {\varphi (B \to CA,k,i - 1,j) \cdot \beta (B,k,j) \cdot \alpha (C,k,i - 1)} } \end{array}\]</p><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><hr><p>所有可能的句法树potential之和为：<br>\[{Z_s} = \alpha (S,1,n)\]<br>包含产生式$(A \to BC,i,k,j)$的所有可能句法树potential之和是：<br>\[\mu (A \to BC,i,k,j) = \varphi (A \to BC,i,k,j) \cdot \beta (A,i,j) \cdot \alpha (B,i,k) \cdot \alpha (C,k + 1,j)\]<br>存在非终结符$A$，且短语是${x_{i;j}}$的所有可能句法树potential之和是：<br>\[\mu (A,i,j) = \alpha (A,i,j) \cdot \beta (A,i,j)\]</p><h1 id="PCFG参数估计"><a href="#PCFG参数估计" class="headerlink" title="PCFG参数估计"></a>PCFG参数估计</h1><hr><p>参数估计的目的就是为了估计出PCFG的概率$P$，使得所有句子的概率之和最大，采用的是EM迭代法。<br>首先定义：<br>\[\varphi (A \to BC,i,k,j) = P(A \to BC)\]<br>这里$P(A \to BC)$是随机初始化的，满足归一化条件就行。<br>对于语料库的每一条句子，可以计算出：<br>\[\begin{array}{l}count(A \to BC) = \frac { {\sum\limits_{i,k,j} {\mu (A \to BC,i,k,j)} }}{ { {Z_s}}}\\P(A \to BC) = \frac{ {count(A \to BC)}}{ {\sum\limits_r {count(r)} }}\end{array}\]<br>然后算出期望，更新概率，迭代就行了。</p><h1 id="CRF-CFG参数估计"><a href="#CRF-CFG参数估计" class="headerlink" title="CRF-CFG参数估计"></a>CRF-CFG参数估计</h1><hr><p>首先定义:<br>\[\varphi (A \to BC,i,k,j) = \exp \sum\limits_t { {\theta _t}{f_t}(A \to BC,i,k,j)} \]<br>其中$f_t$为特征函数。<br>那么我们的目的就是训练特征参数$\theta$。<br>然后定义似然函数为<br>\[L(D;\theta ) = \sum\limits_{(t,s) \in D} {\left( {\sum\limits_{r \in t} {\sum\limits_i { {\theta _i}{f_i}(r,s) - {Z_s}} } } \right)}  + \sum\limits_i {\frac{ {\theta _i^2}}{ {2{\sigma ^2}}}} \]<br>求偏导为<br>\[\frac{ {\partial L(D;\theta )}}{ {\partial {\theta _i}}} = \sum\limits_{(t,s) \in D} {(\sum\limits_{r \in t} { {f_i}(r,s)}  - {E_\theta }[{f_i}|s])}  + \frac{ { {\theta _i}}}{ { {\sigma ^2}}}\]</p><p>这里可能有人看不懂，似然函数和偏导是怎么来的呢？下面我详细写一下过程。<br>似然函数：<br>\[\begin{array}{l}L(D;\theta ) = \sum\limits_{(t,s) \in D} {\log \frac{ {\exp \sum\limits_{r \in t} {\sum\limits_i { {\theta _i}{f_i}(r,s)} } }}{ {\sum\limits_{t \in T(s)} {\exp \sum\limits_{r \in t} {\sum\limits_i { {\theta _i}{f_i}(r,s)} } } }}}  + \sum\limits_i {\frac{ {\theta _i^2}}{ {2{\sigma ^2}}}} \\ = \sum\limits_{(t,s) \in D} {\left( {\sum\limits_{r \in t} {\sum\limits_i { {\theta _i}{f_i}(r,s)} }  - \log \sum\limits_{t \in T(s)} {\exp \sum\limits_{r \in t} {\sum\limits_i { {\theta _i}{f_i}(r,s)} } } } \right)}  + \sum\limits_i {\frac{ {\theta _i^2}}{ {2{\sigma ^2}}}} \\ = \sum\limits_{(t,s) \in D} {\left( {\sum\limits_{r \in t} {\sum\limits_i { {\theta _i}{f_i}(r,s)} }  - {Z_s}} \right)}  + \sum\limits_i {\frac{ {\theta _i^2}}{ {2{\sigma ^2}}}} \end{array}\]<br>所以偏导为：<br>\[\frac{ {\partial L(D;\theta )}}{ {\partial {\theta _i}}} = \sum\limits_{(t,s) \in D} {\left( {\sum\limits_{r \in t} { {f_i}(r,s)}  - \frac{ {\partial \left( {\log \sum\limits_{t \in T(s)} {\exp \sum\limits_{r \in t} {\sum\limits_i { {\theta _i}{f_i}(r,s)} } } } \right)}}{ {\partial {\theta _i}}}} \right)}  + \frac{ { {\theta _i}}}{ { {\sigma ^2}}}\]<br>而<br>\[\begin{array}{l}\frac{ {\partial \left( {\log \sum\limits_{t \in T(s)} {\exp \sum\limits_{r \in t} {\sum\limits_i { {\theta _i}{f_i}(r,s)} } } } \right)}}{ {\partial {\theta _i}}}\\ = \frac{ {\sum\limits_{t \in T(s)} {\left( {\left( {\exp \sum\limits_{r \in t} {\sum\limits_i { {\theta _i}{f_i}(r,s)} } } \right) \cdot \sum\limits_{r \in t} { {f_i}(r,s)} } \right)} }}{ {\sum\limits_{t \in T(s)} {\exp \sum\limits_{r \in t} {\sum\limits_i { {\theta _i}{f_i}(r,s)} } } }}\\ = {E_\theta }[{f_i}|s]\end{array}\]<br>所以偏导就是这么来的。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 句法分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第8课（取整进阶）</title>
      <link href="/2018/04/16/concrete-math-8/"/>
      <url>/2018/04/16/concrete-math-8/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=306662&auto=1&height=66"></iframe></div><p>今天主要讲了取整与递归式的结合，还有取模的相关知识。</p><h1 id="例题1"><a href="#例题1" class="headerlink" title="例题1"></a>例题1</h1><hr><p>给出下列递归式：<br>\[\begin{array}{l}{K_0}{\rm{ = }}1\\{K_{n + 1}} = 1 + \min (2{K_{\left\lfloor {n/2} \right\rfloor }},3{K_{\left\lfloor {n/3} \right\rfloor }}),n \ge 0\end{array}\]<br>现在不要求你求解，要你证明：<br>\[{K_n} \ge n\]<br>首先想到的就是数学归纳法，假设对于任意$k \le n$，都有${K_k} \ge k$，那么：<br>\[\begin{array}{l}{K_{n + 1}} = 1 + \min (2{K_{\left\lfloor {n/2} \right\rfloor }},3{K_{\left\lfloor {n/3} \right\rfloor }})\\ \ge 1 + \min (2\left\lfloor {\frac{n}{2}} \right\rfloor ,3\left\lfloor {\frac{n}{3}} \right\rfloor )\end{array}\]<br>如果$n = 2k$，那么${K_{n + 1}} \ge 1 + n$。<br>如果$n = 2k + 1$，那么${K_{n + 1}} \ge n$，这时不成立。</p><p>所以数学归纳法无法证明，今后我们会用其他方法来证明这个式子。</p><h1 id="约瑟夫环新解"><a href="#约瑟夫环新解" class="headerlink" title="约瑟夫环新解"></a>约瑟夫环新解</h1><hr><p>还记得约瑟夫环问题吗？详见<a href="http://godweiyang.com/2018/02/27/concrete-math-1/">第一节课</a>。</p><p>这里我们继续推广到一般情况，如果有$n$个人，每隔$q$个人踢掉一个人，最后剩下的是几号？</p><p>初始编号为$1 \ldots n$，现在考虑一种新的编号方式。</p><p>第一个人不会被踢掉，编号加$1$，变成$n + 1$，然后第二个人编号变为$n + 2$，直到第$q$个人，他被踢掉了。</p><p>然后第$q + 1$个人编号继续加$1$，变成了$n + q$，依次下去。</p><p>考虑当前踢到的人编号为$kq$，那么此时已经踢掉了$k$个人，所以接下去的人新的编号为$n + k(q - 1) + 1 \ldots$。</p><p>所以编号为$kq+d$的人编号变成了$n + k(q - 1) + d$，其中$1 \le d &lt; q$。</p><p>直到最后，可以发现活下来的人编号为$qn$，问题是怎么根据这个编号推出他原来的编号？</p><p>以$n = 10$，$q = 3$为例，下图就是每个人新的编号：<br><img src="1.jpg" alt></p><p>令<br>\[N = n + k(q - 1) + d\]<br>所以他上一次的编号是<br>\[kq + d = kq + N - n - k(q - 1) = k + N - n\]<br>因为<br>\[k = \frac{ {N - n - d}}{ {q - 1}} = \left\lfloor {\frac{ {N - n - 1}}{ {q - 1}}} \right\rfloor \]<br>所以上一次编号可以写为<br>\[\left\lfloor {\frac{ {N - n - 1}}{ {q - 1}}} \right\rfloor  + N - n\]</p><p>因此最后存活的人编号可以用如下的算法计算：</p><pre><code>N = qnwhile N &gt; n:    N = k + N - nans = N</code></pre><p>其中$k = \left\lfloor {\frac{ {N - n - 1}}{ {q - 1}}} \right\rfloor $</p><p>如果我们用$D = qn + 1 - N$替代$N$，将会进一步简化算法：<br>\[\begin{array}{l}D = qn + 1 - N\\ = qn + 1 - \left( {\left\lfloor {\frac{ {(qn + 1 - D) - n - 1}}{ {q - 1}}} \right\rfloor  + qn + 1 - D - n} \right)\\ = n + D - \left\lfloor {\frac{ {(q - 1)n - D}}{ {q - 1}}} \right\rfloor \\ = D - \left\lfloor {\frac{ { - D}}{ {q - 1}}} \right\rfloor \\ = D + \left\lceil {\frac{D}{ {q - 1}}} \right\rceil \\ = \left\lceil {\frac{q}{ {q - 1}}D} \right\rceil \end{array}\]</p><p>算法伪代码如下：</p><pre><code>D = 1while D &lt;= (q-1)n:    D = kans = qn + 1 - D</code></pre><p>其中$k = \left\lceil {\frac{q}{ {q - 1}}D} \right\rceil $</p><h1 id="模的性质"><a href="#模的性质" class="headerlink" title="模的性质"></a>模的性质</h1><hr><h2 id="定义与性质"><a href="#定义与性质" class="headerlink" title="定义与性质"></a>定义与性质</h2><p>模定义如下：<br>\[x\bmod y = x - y\left\lfloor {\frac{x}{y}} \right\rfloor \]<br>特别的<br>\[x\bmod 0 = x\]</p><p>与此类似，定义一个与模类似的运算：<br>\[x{\rm{ mumble }}y = y\left\lceil {\frac{x}{y}} \right\rceil  - x\]<br>形象理解如下图所示：<br><img src="2.jpg" alt><br>圆的周长是$y$，一共走过的路长（红色+绿色部分）是$x$，所以$x\bmod y$就是绿色部分，$x{\rm{ mumble }}y$就是一圈长度减去绿色部分。</p><p>模有一些性质：<br>\[c(x\bmod y) = (cx)\bmod (cy)\]</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>考虑如下问题，怎么平均分配$n$个东西给$m$个人？</p><p>很容易想到，首先分给每个人$\left\lfloor {\frac{n}{m}} \right\rfloor $个东西，剩下$n\bmod m$件东西分给前$n\bmod m$个人，一人多一件就行。</p><p>概括起来就是，前$n\bmod m$个人，每人$\left\lceil {\frac{n}{m}} \right\rceil $件，剩下的人，每人$\left\lfloor {\frac{n}{m}} \right\rfloor $件。</p><p>那有没有办法统一表示呢？有的，每个人分到的件数为<br>\[\left\lceil {\frac{ {n - k + 1}}{m}} \right\rceil ,1 \le k \le m\]</p><p>为什么呢？假设<br>\[n = qm + r,0 \le r &lt; m\]<br>那么<br>\[\begin{array}{l}\left\lceil {\frac{ {n - k + 1}}{m}} \right\rceil  = \left\lceil {\frac{ {qm + r - k + 1}}{m}} \right\rceil \\ = q + \left\lceil {\frac{ {r - k + 1}}{m}} \right\rceil \end{array}\]<br>当$1 \le k \le r$时，<br>\[\left\lceil {\frac{ {n - k + 1}}{m}} \right\rceil  = 1\]<br>当$r &lt; k \le m$时，<br>\[\left\lceil {\frac{ {n - k + 1}}{m}} \right\rceil  = 0\]</p><p>得证，因此可以得到如下等式：<br>\[n = \left\lceil {\frac{n}{m}} \right\rceil  + \left\lceil {\frac{ {n - 1}}{m}} \right\rceil  +  \cdots  + \left\lceil {\frac{ {n - m + 1}}{m}} \right\rceil \]</p><p>由$n = \left\lfloor {\frac{n}{2}} \right\rfloor  + \left\lceil {\frac{n}{2}} \right\rceil $<br>可以进一步将其转换为下取整形式：<br>\[n = \left\lfloor {\frac{n}{m}} \right\rfloor  + \left\lfloor {\frac{ {n + 1}}{m}} \right\rfloor  +  \cdots  + \left\lfloor {\frac{ {n + m - 1}}{m}} \right\rfloor \]</p><p>令$n = \left\lfloor {mx} \right\rfloor $<br>我们得到了一个令人惊奇的等式：<br>\[\left\lfloor {mx} \right\rfloor  = \left\lfloor x \right\rfloor  + \left\lfloor {x + \frac{1}{m}} \right\rfloor  +  \cdots  + \left\lfloor {x + \frac{ {m - 1}}{m}} \right\rfloor \]</p><h1 id="HDU3089"><a href="#HDU3089" class="headerlink" title="HDU3089"></a>HDU3089</h1><hr><p>最后用今天介绍的约瑟夫环算法来解决一道经典的ACM题！题目链接：<a href="http://acm.hdu.edu.cn/showproblem.php?pid=3089" target="_blank" rel="noopener">杭电3089</a>。</p><p>C++代码如下：</p><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span><span class="token string">&lt;bits/stdc++.h></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span class="token keyword">typedef</span> <span class="token keyword">long</span> <span class="token keyword">long</span> LL<span class="token punctuation">;</span>LL <span class="token function">Ceil</span><span class="token punctuation">(</span>LL x<span class="token punctuation">,</span> LL y<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>x <span class="token operator">%</span> y <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">return</span> x <span class="token operator">/</span> y<span class="token punctuation">;</span>    <span class="token keyword">return</span> x <span class="token operator">/</span> y <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span><span class="token punctuation">}</span>LL <span class="token function">J</span><span class="token punctuation">(</span>LL n<span class="token punctuation">,</span> LL q<span class="token punctuation">)</span> <span class="token punctuation">{</span>    LL D <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> end <span class="token operator">=</span> <span class="token punctuation">(</span>q <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> n<span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>D <span class="token operator">&lt;=</span> end<span class="token punctuation">)</span> <span class="token punctuation">{</span>        D <span class="token operator">=</span> <span class="token function">Ceil</span><span class="token punctuation">(</span>q <span class="token operator">*</span> D<span class="token punctuation">,</span> q <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> q <span class="token operator">*</span> n <span class="token operator">+</span> <span class="token number">1</span> <span class="token operator">-</span> D<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    LL n<span class="token punctuation">,</span> q<span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token operator">~</span><span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%lld%lld"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>n<span class="token punctuation">,</span> <span class="token operator">&amp;</span>q<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%lld\n"</span><span class="token punctuation">,</span> <span class="token function">J</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> q<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>比网上各种快速算法还要快哦，理论时间复杂度是$\log n$的。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>超详细Hexo+Github博客搭建小白教程</title>
      <link href="/2018/04/13/hexo-blog/"/>
      <url>/2018/04/13/hexo-blog/</url>
      
        <content type="html"><![CDATA[<h1 id="更新（2019-07-20）"><a href="#更新（2019-07-20）" class="headerlink" title="更新（2019.07.20）"></a>更新（2019.07.20）</h1><p>这两天花时间将我的博客换了一个主题，现在这个主题看着更加的炫（zhuang）酷（bi），并且响应式更友好，点起来就很舒服，功能也多很多。</p><p>主题的原地址在这里：<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank" rel="noopener">hexo-theme-matery</a>，它的文档写得也非常的详细，还有中英文两个版本，作者回复也很及时。效果图如下，可以看出非常合我的口味：<br><img src="21.jpg" alt></p><p>但是我自己使用起来还是遇到了好几个问题，经过两天的不懈摸鱼，终于基本解决了，这里分享一下。</p><p>首先先按照文档教程安装一遍主题，然后是可以正常打开的，如果你是一般使用的话，基本没啥问题了。但是我是重度强迫症，一点小毛病就看着难受，下面列举一下我遇到的问题以及解决方法。</p><h2 id="新建文章"><a href="#新建文章" class="headerlink" title="新建文章"></a>新建文章</h2><p>首先为了新建文章方便，建议将<code>/scaffolds/post.md</code>修改为如下代码：</p><pre class=" language-json"><code class="language-json">---title<span class="token operator">:</span> <span class="token punctuation">{</span><span class="token punctuation">{</span> title <span class="token punctuation">}</span><span class="token punctuation">}</span>date<span class="token operator">:</span> <span class="token punctuation">{</span><span class="token punctuation">{</span> date <span class="token punctuation">}</span><span class="token punctuation">}</span>top<span class="token operator">:</span> <span class="token boolean">false</span>cover<span class="token operator">:</span> <span class="token boolean">false</span>password<span class="token operator">:</span>toc<span class="token operator">:</span> <span class="token boolean">true</span>mathjax<span class="token operator">:</span> <span class="token boolean">true</span>summary<span class="token operator">:</span>tags<span class="token operator">:</span>categories<span class="token operator">:</span>---</code></pre><p>这样新建文章后不用你自己补充了，修改信息就行。</p><h2 id="添加404页面"><a href="#添加404页面" class="headerlink" title="添加404页面"></a>添加404页面</h2><p>原来的主题没有404页面，加一个也不是什么难事。首先在<code>/source/</code>目录下新建一个<code>404.md</code>，内容如下：</p><pre class=" language-json"><code class="language-json">---title<span class="token operator">:</span> <span class="token number">404</span>date<span class="token operator">:</span> <span class="token number">2019</span>-<span class="token number">07</span>-<span class="token number">19</span> <span class="token number">16</span><span class="token operator">:</span><span class="token number">41</span><span class="token operator">:</span><span class="token number">10</span>type<span class="token operator">:</span> <span class="token string">"404"</span>layout<span class="token operator">:</span> <span class="token string">"404"</span>description<span class="token operator">:</span> <span class="token string">"你来到了没有知识的荒原 :("</span>---</code></pre><p>然后在<code>/themes/matery/layout/</code>目录下新建一个<code>404.ejs</code>文件，内容如下：</p><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>text/css<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token style language-css">    <span class="token comment" spellcheck="true">/* don't remove. */</span>    <span class="token selector"><span class="token class">.about-cover</span> </span><span class="token punctuation">{</span>        <span class="token property">height</span><span class="token punctuation">:</span> <span class="token number">75</span>vh<span class="token punctuation">;</span>    <span class="token punctuation">}</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>bg-cover pd-header about-cover<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>container<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>row<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>col s10 offset-s1 m8 offset-m2 l8 offset-l2<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>brand<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>title center-align<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>                        404                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>description center-align<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>                        &lt;%= page.description %>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span><span class="token punctuation">></span></span><span class="token script language-javascript">    <span class="token comment" spellcheck="true">// 每天切换 banner 图.  Switch banner image every day.</span>    <span class="token function">$</span><span class="token punctuation">(</span><span class="token string">'.bg-cover'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">css</span><span class="token punctuation">(</span><span class="token string">'background-image'</span><span class="token punctuation">,</span> <span class="token string">'url(/medias/banner/'</span> <span class="token operator">+</span> <span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getDay</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'.jpg)'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span></code></pre><hr><p>去年9月的时候开始搭建了第一个自己的独立博客，到现在也稍微像模像样了。很多小伙伴应该也想过搭建一个自己的博客，网上也有一堆详细教程。我在此稍稍总结一下具体的搭建步骤，另外网上很少有修改博客源码的个性化教程，我就稍稍分享一下我的一些修改经验，更多的个性化操作需要你自己以后去摸索。</p><p>具体效果可以参观我的博客：<a href="http://godweiyang.com">godweiyang.com</a>，欢迎大家支持。</p><p>我不是一个前端程序员，有些东西不是很了解，说的不好大家见谅。</p><p>首先要了解一下我们搭建博客要用到的框架。Hexo是高效的静态站点生成框架，它基于Node.js。通过Hexo，你可以直接使用Markdown语法来撰写博客。相信很多小伙伴写工程都写过README.md文件吧，对，就是这个格式的！写完后只需两三条命令即可将生成的网页上传到你的github上，然后别人就可以看到你的网页啦。是不是很简单？你无需关心网页源代码的具体细节，你只需要用心写好你的博客内容就行。</p><h1 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h1><hr><p>首先下载最新版<a href="https://nodejs.org/dist/v9.11.1/node-v9.11.1-x64.msi" target="_blank" rel="noopener">Node.js</a>，我这里给的是64位的。</p><p>安装选项全部默认，一路点击<code>Next</code>。</p><p>最后安装好之后，按<code>Win+R</code>打开命令提示符，输入<code>node -v</code>和<code>npm -v</code>，如果出现版本号，那么就安装成功了。</p><h1 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h1><hr><p>为了把本地的网页文件上传到github上面去，我们需要用到分布式版本控制工具————Git<a href="https://git-scm.com/download/win" target="_blank" rel="noopener">[下载地址]</a>。</p><p>安装选项还是全部默认，只不过最后一步添加路径时选择<code>Use Git from the Windows Command Prompt</code>，这样我们就可以直接在命令提示符里打开git了。</p><p>安装完成后在命令提示符中输入<code>git --version</code>验证是否安装成功。</p><h1 id="注册Github账号"><a href="#注册Github账号" class="headerlink" title="注册Github账号"></a>注册Github账号</h1><hr><p>接下来就去注册一个github账号，用来存放我们的网站。大多数小伙伴应该都有了吧，作为一个合格的程序猿（媛）还是要有一个的。</p><p>打开<a href="https://github.com/" target="_blank" rel="noopener">https://github.com/</a>，新建一个项目，如下所示：<br><img src="1.jpg" alt><br>然后如下图所示，输入自己的项目名字，后面一定要加<code>.github.io</code>后缀，README初始化也要勾上。<br><img src="2.jpg" alt><br>然后项目就建成了，点击<code>Settings</code>，向下拉到最后有个<code>GitHub Pages</code>，点击<code>Choose a theme</code>选择一个主题。然后等一会儿，再回到<code>GitHub Pages</code>，会变成下面这样：<br><img src="3.jpg" alt><br>点击那个链接，就会出现自己的网页啦，效果如下：<br><img src="4.jpg" alt></p><h1 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h1><hr><p>在合适的地方新建一个文件夹，用来存放自己的博客文件，比如我的博客文件都存放在<code>D:\study\program\blog</code>目录下。</p><p>在该目录下右键点击<code>Git Bash Here</code>，打开git的控制台窗口，以后我们所有的操作都在git控制台进行，就不要用Windows自带的控制台了。</p><p>定位到该目录下，输入<code>npm install hexo-cli -g</code>安装Hexo。会有几个报错，无视它就行。</p><p>然后输入<code>npm install hexo --save</code>继续安装。</p><p>安装完后输入<code>hexo -v</code>验证是否安装成功。</p><p>然后就要初始化我们的网站，输入<code>hexo init</code>初始化文件夹，接着输入<code>npm install</code>安装必备的组件。</p><p>这样本地的网站配置也弄好啦，输入<code>hexo g</code>生成静态网页，然后输入<code>hexo s</code>打开本地服务器，然后浏览器打开<a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a>，就可以看到我们的博客啦，效果如下：<br><img src="5.jpg" alt></p><p>按<code>ctrl+c</code>关闭本地服务器。</p><h1 id="连接Github与本地"><a href="#连接Github与本地" class="headerlink" title="连接Github与本地"></a>连接Github与本地</h1><hr><p>首先右键打开git bash，然后输入下面命令：</p><pre><code>git config --global user.name &quot;godweiyang&quot;git config --global user.email &quot;792321264@qq.com&quot;</code></pre><p>用户名和邮箱根据你注册github的信息自行修改。</p><p>然后生成密钥SSH key：</p><pre><code>ssh-keygen -t rsa -C &quot;792321264@qq.com&quot;</code></pre><p>输入<code>eval &quot;$(ssh-agent -s)&quot;</code>，添加密钥到ssh-agent。</p><p>再输入<code>ssh-add ~/.ssh/id_rsa</code>，添加生成的SSH key到ssh-agent。</p><p>打开<a href="http://github.com" target="_blank" rel="noopener">github</a>，在头像下面点击<code>settings</code>，再点击<code>SSH and GPG keys</code>，新建一个SSH，名字随便。</p><p>打开<code>C:\Users\Administrator\.ssh\id_rsa.pub</code>，注意是隐藏文件夹，将其中的内容复制到新建的SSH中。</p><p>输入<code>ssh -T git@github.com</code>，如果如下图所示，出现你的用户名，那就成功了。<br><img src="6.jpg" alt></p><p>打开博客根目录下的<code>_config.yml</code>文件，这是博客的配置文件，在这里你可以修改与博客相关的各种信息。</p><p>修改最后一行的配置：</p><pre><code>deploy:  type: git  repository: https://github.com/godweiyang/godweiyang.github.io  branch: master</code></pre><p>repository修改为你自己的github项目地址。<strong>不过你这里看到的可能与我有点不一样，因为我已经修改过主题了，所以这一步先不用管，换完主题之后记得回来修改！</strong></p><h1 id="写文章、发布文章"><a href="#写文章、发布文章" class="headerlink" title="写文章、发布文章"></a>写文章、发布文章</h1><hr><p>首先在博客根目录下右键打开git，安装一个扩展<code>npm install hexo-deployer-git --save</code>。</p><p>然后输入<code>hexo new post &quot;article title&quot;</code>，新建一篇文章。</p><p>然后打开<code>D:\study\program\blog\source\_posts</code>的目录，可以发现下面多了一个文件夹和一个<code>.md</code>文件，一个用来存放你的图片等数据，另一个就是你的文章文件啦。</p><p>编写完markdown文件后，根目录下输入<code>hexo g</code>生成静态网页，然后输入<code>hexo s</code>可以本地预览效果，最后输入<code>hexo d</code>上传到github上。这时打开你的github.io主页就能看到发布的文章啦。</p><h1 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h1><hr><p>现在默认的域名还是<code>xxx.github.io</code>，是不是很没有牌面？想不想也像我一样弄一个专属域名呢，首先你得购买一个域名，xx云都能买，看你个人喜好了。</p><p>以我的百度云为例，如下图所示，添加两条解析记录：<br><img src="7.jpg" alt></p><p>然后打开你的github博客项目，点击<code>settings</code>，拉到下面<code>Custom domain</code>处，填上你自己的域名，保存：<br><img src="8.jpg" alt></p><p>这时候你的项目根目录应该会出现一个名为<code>CNAME</code>的文件了。如果没有的话，打开你本地博客<code>/source</code>目录，我的是<code>D:\study\program\blog\source</code>，新建<code>CNAME</code>文件，注意没有后缀。然后在里面写上你的域名，保存。最后运行<code>hexo g</code>、<code>hexo d</code>上传到github。</p><h1 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h1><hr><p>网上大多数主题都是github排名第一的<code>Next</code>主题，但是我个人不是很喜欢，我更喜欢<code>beantech</code>主题，地址在<a href="https://github.com/YenYuHsuan/hexo-theme-beantech" target="_blank" rel="noopener">传送门</a>。</p><p>首先要注意的是，<strong>这个github项目不仅包含了主题文件，还包含了hexo的各种文件</strong>。</p><p>所以首先下载下来这个项目，然后推荐将下图所有文件全部替换你原本博客根目录下的文件：<br><img src="9.jpg" alt></p><p>然后运行<code>hexo clean</code>清空所有生成的网页缓存，<code>hexo g</code>、<code>hexo d</code>。这时候新的主题网页就生成好了，博客根目录的情况应该大致如下：<br><img src="10.jpg" alt></p><h1 id="博客目录构成介绍"><a href="#博客目录构成介绍" class="headerlink" title="博客目录构成介绍"></a>博客目录构成介绍</h1><hr><p>从上图可以看出，博客的目录结构如下：</p><pre><code>- node_modules- public- scaffolds- source    - _posts    - about    - archive    - img    - tags- themes</code></pre><p><code>node_modules</code>是node.js各种库的目录，<code>public</code>是生成的网页文件目录，<code>scaffolds</code>里面就三个文件，存储着新文章和新页面的初始设置，<code>source</code>是我们最常用到的一个目录，里面存放着文章、各类页面、图像等文件，<code>themes</code>存放着主题文件，一般也用不到。</p><p>我们平时写文章只需要关注<code>source/_posts</code>这个文件夹就行了。</p><h1 id="个性化设置及bug处理"><a href="#个性化设置及bug处理" class="headerlink" title="个性化设置及bug处理"></a>个性化设置及bug处理</h1><hr><p><strong>首先打开<code>_config.yml</code>，根据自己需求配置，具体不说了，有注释。</strong></p><h2 id="修复文档分类bug"><a href="#修复文档分类bug" class="headerlink" title="修复文档分类bug"></a>修复文档分类bug</h2><p>这个主题文档分类功能有个bug，一直没有得到解决，直到最近，我才发现是源文件的单词拼错了。。。</p><p>打开<code>D:\study\program\blog\scaffolds\post.md</code>，单词<code>catagories</code>改为<code>categories</code>。</p><h2 id="添加畅言评论插件"><a href="#添加畅言评论插件" class="headerlink" title="添加畅言评论插件"></a>添加畅言评论插件</h2><p>主题自带了多说评论插件，但是多说已经关闭了，所以我换成了畅言评论插件。</p><p>首先你得注册一个畅言账号，<a href="http://changyan.kuaizhan.com/" target="_blank" rel="noopener">地址</a>。<br>然后打开如下页面，复制该段代码：<br><img src="11.jpg" alt><br>打开<code>D:\study\program\blog\themes\beantech\layout\post.ejs</code>，将代码粘贴到如下位置：<br><img src="12.jpg" alt><br>然后重新生成一下网页，可以看到效果图如下：<br><img src="13.jpg" alt><br>更多插件例如热评话题之类的，可以自行在畅言后台找到代码添加。</p><h2 id="添加图片放大功能"><a href="#添加图片放大功能" class="headerlink" title="添加图片放大功能"></a>添加图片放大功能</h2><p>首先下载<code>zooming.js</code>文件<a href="https://github.com/godweiyang/godweiyang.github.io/blob/master/js/zooming.js" target="_blank" rel="noopener">地址</a>，保存在<code>D:\study\program\blog\themes\beantech\source\js</code>目录下。</p><p>打开<code>D:\study\program\blog\themes\beantech\layout\post.ejs</code>，在最下方粘贴如下代码：</p><pre><code>&lt;script type=&quot;text/javascript&quot; src=&quot;/js/zooming.js&quot;&gt;&lt;/script&gt;</code></pre><p>然后文章里的图片就可以单击全屏啦。</p><h2 id="添加数学公式显示"><a href="#添加数学公式显示" class="headerlink" title="添加数学公式显示"></a>添加数学公式显示</h2><p>打开<code>D:\study\program\blog\themes\beantech\layout\post.ejs</code>，在最下方粘贴如下代码：</p><pre><code>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;</code></pre><p>由于markdown语法与mathjax语法存在冲突，所以还需要修改源文件。</p><p>打开<code>D:\study\program\blog\node_modules\marked\lib\marked.js</code><br><code>escape:</code>处替换成：</p><pre><code>escape: /^$[`*\[\]()#$+\-.!_&gt;])/</code></pre><p><code>em:</code>处替换成：</p><pre><code>em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</code></pre><p>这时在文章里写数学公式基本没有问题了，但是要注意：<br><strong>数学公式中如果出现了连续两个{，中间一定要加空格！</strong></p><p>举个例子:<br>行内公式：$y = f(x)$<br>代码：</p><pre><code>$y = f(x)$</code></pre><p>行间公式：<br>\[y = {f_{ {g_1}}}(x)\]<br>代码：</p><pre><code>\\[y = {f_{ {g_1}}}(x)\\]</code></pre><p><strong>注意上面花括号之间有空格！</strong></p><h2 id="添加留言板"><a href="#添加留言板" class="headerlink" title="添加留言板"></a>添加留言板</h2><p>在<code>D:\study\program\blog\themes\beantech\layout</code>中新建<code>bbs.ejs</code>，文件内容如下：</p><pre><code>---layout: page---&lt;style type=&quot;text/css&quot;&gt;    header.intro-header{        background-position: right;     }&lt;/style&gt;&lt;!-- Chinese Version --&gt;&lt;div class=&quot;zh post-container&quot;&gt;    &lt;%- page.content %&gt;&lt;/div&gt;</code></pre><p>然后在<code>D:\study\program\blog\source</code>中新建<code>\bbs</code>文件夹，里面在新建<code>index.md</code>文件，内容如下：</p><pre><code>---layout: &quot;bbs&quot;title: &quot;BBS&quot;date: 2017-09-19 12:48:33description: &quot;欢迎交换友链，一起交流学习！&quot;header-img: &quot;img/header_img/home-bg-2-dark.png&quot;comments: true---此处替换为你的畅言评论代码~~~</code></pre><h2 id="添加置顶功能"><a href="#添加置顶功能" class="headerlink" title="添加置顶功能"></a>添加置顶功能</h2><p>运行如下两条命令安装置顶插件：</p><pre><code>npm uninstall hexo-generator-index --savenpm install hexo-generator-index-pin-top --save</code></pre><p>然后打开<code>D:\study\program\blog\themes\beantech\layout\index.ejs</code>，在如下位置添加代码：<br><img src="14.jpg" alt></p><pre><code>&lt;% if (post.top) {%>    <i class="fa fa-thumb-tack"></i>    <font color="7D26CD">置顶</font>    <span class="post-meta-divider">|</span><%}%&gt;< code></%}%&gt;<></code></pre><p>然后在你想置顶的文章md文件里，添加如下配置选项：</p><pre><code>top: true</code></pre><h2 id="添加网易云音乐BGM"><a href="#添加网易云音乐BGM" class="headerlink" title="添加网易云音乐BGM"></a>添加网易云音乐BGM</h2><p>写文章的时候，想插入一段BGM怎么办？</p><p>首先打开网易云网页版，找到想听的歌曲，然后点击生成外链：<br><img src="15.jpg" alt><br>复制如下代码：<br><img src="16.jpg" alt></p><p>粘贴到文章里就行了，为了美观，设置一下居中，具体代码如下：</p><pre><code>&lt;div align=&quot;middle&quot;&gt;这里粘贴刚刚复制的代码&lt;/div&gt;</code></pre><h2 id="添加访客人数统计和字数统计"><a href="#添加访客人数统计和字数统计" class="headerlink" title="添加访客人数统计和字数统计"></a>添加访客人数统计和字数统计</h2><p>我们使用一个国外的流量统计网站：<a href="gostats.com">gostats.com</a>，首先注册一下。</p><p>然后自己添加网站地址，过程就不详细说了，然后点击<code>Get counter code</code>，选择一个自己喜欢的风格。<br>如下图所示，选择一个样式，选择<code>HTML</code>，生成代码：<br><img src="17.jpg" alt></p><p>复制这段代码到<code>D:\study\program\blog\themes\beantech\layout\_partial\footer.ejs</code>，具体位置如下：<br><img src="18.jpg" alt><br>默认的代码和我图中不一样，因为我不想点击图片跳转到它统计网站的链接，可以模仿我的自行修改。</p><p>字数统计首先安装插件</p><pre><code>npm i --save hexo-wordcount</code></pre><p>然后打开<code>D:\study\program\blog\themes\beantech\layout\_partial\footer.ejs</code>，添加如下代码：</p><pre><code>&lt;span class=&quot;post-count&quot;&gt;&lt;%= totalcount(site) %&gt; words altogether&lt;/span&gt;</code></pre><p>具体位置见上图。</p><h1 id="一些注意事项"><a href="#一些注意事项" class="headerlink" title="一些注意事项"></a>一些注意事项</h1><hr><p>首先解释一下文章开头的配置，如下图所示：<br><img src="19.jpg" alt></p><pre><code>title: 文章标题catalog: 是否显示段落目录date: 文章日期subtitle: 子标题header-img: 顶部背景图片top: 是否置顶tags: 标签categories: 分类</code></pre><p>网站图片都保存在<code>D:\study\program\blog\source\img</code>目录下，可以修改成自己的图片。</p><p>如果换一台电脑想迁移博客的话，最简单的方法就是把博客整个目录拷贝过去，就是这么暴力。</p><p><strong>其他还有什么问题的话等我想起来了再继续添加，如果遇到问题欢迎联系我。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 软件安装与配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Github </tag>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第7课（取整基础）</title>
      <link href="/2018/04/09/concrete-math-7/"/>
      <url>/2018/04/09/concrete-math-7/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=509728841&auto=1&height=66"></iframe></div><p>首先声明一下，最近这段时间忙毕设，没时间更新博客了，大家见谅。</p><p>今天这节课开始讲解取整相关知识，主要是数论相关的了。</p><h1 id="符号定义"><a href="#符号定义" class="headerlink" title="符号定义"></a>符号定义</h1><hr><p>向下取整函数$\left\lfloor x \right\rfloor $定义为小于等于$x$的最大整数。<br>向上取整函数$\left\lceil x \right\rceil $定义为大于等于$x$的最小整数。<br>$\{ x\} $定义为实数$x$的小数部分，即<br>\[\{ x\}  = x - \left\lfloor x \right\rfloor \]</p><h1 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h1><hr><h2 id="性质1"><a href="#性质1" class="headerlink" title="性质1"></a>性质1</h2><p>\[\left\lceil x \right\rceil  - \left\lfloor x \right\rfloor  = [x \in \mathbb{Z}]\]</p><h2 id="性质2"><a href="#性质2" class="headerlink" title="性质2"></a>性质2</h2><p>取整函数范围：<br>\[x - 1 &lt; \left\lfloor x \right\rfloor  \le x \le \left\lceil x \right\rceil  &lt; x + 1\]</p><h2 id="性质3"><a href="#性质3" class="headerlink" title="性质3"></a>性质3</h2><p>负数的取整：<br>\[\begin{array}{l}\left\lfloor { - x} \right\rfloor  =  - \left\lceil x \right\rceil \\\left\lceil { - x} \right\rceil  =  - \left\lfloor x \right\rfloor \end{array}\]</p><h2 id="性质4"><a href="#性质4" class="headerlink" title="性质4"></a>性质4</h2><p>取整函数中的整数可以提取出来：<br>\[\left\lfloor {x + n} \right\rfloor  = \left\lfloor x \right\rfloor  + n\]</p><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><hr><h2 id="应用1"><a href="#应用1" class="headerlink" title="应用1"></a>应用1</h2><p>证明：<br>\[\left\lfloor {\sqrt {\left\lfloor x \right\rfloor } } \right\rfloor  = \left\lfloor {\sqrt x } \right\rfloor \]</p><p>更一般的，我们还可以证明，对于任意连续、递增的函数$f(x)$，如果它满足<br>\[f(x) \in \mathbb{Z} \Rightarrow x \in \mathbb{Z}\]<br>那么有<br>\[\begin{array}{l}\left\lfloor {f(x)} \right\rfloor  = \left\lfloor {f(\left\lfloor x \right\rfloor )} \right\rfloor \\\left\lceil {f(x)} \right\rceil  = \left\lceil {f(\left\lceil x \right\rceil )} \right\rceil \end{array}\]</p><p>我们证明第2个式子，第1个同理可证。</p><p>如果$x = \left\lceil x \right\rceil $，显然成立。</p><p>否则$x &lt; \left\lceil x \right\rceil $，因为$f(x)$递增，所以有<br>\[f(x) &lt; f(\left\lceil x \right\rceil )\]<br>两边同时取整，有<br>\[\left\lceil {f(x)} \right\rceil  \le \left\lceil {f(\left\lceil x \right\rceil )} \right\rceil \]<br>要证左右两边相等，那么只要证<br>\[\left\lceil {f(x)} \right\rceil  &lt; \left\lceil {f(\left\lceil x \right\rceil )} \right\rceil \]<br>不成立即可。假设上式成立，那么由中间值定理，一定存在$x \le y &lt; \left\lceil x \right\rceil $，使得<br>\[f(y) = \left\lceil {f(x)} \right\rceil \]<br><strong>敲黑板！！</strong>这里是怎么来的呢？<br>由下图可以看出，当下面式子成立时，满足中间值定理<br>\[f(x) &lt; \left\lceil {f(x)} \right\rceil  &lt; f(\left\lceil x \right\rceil )\]<br>但是在这里，我们假设是<br>\[\left\lceil {f(x)} \right\rceil  &lt; \left\lceil {f(\left\lceil x \right\rceil )} \right\rceil \]<br>那么由$\left\lceil {f(x)} \right\rceil  &lt; \left\lceil {f(\left\lceil x \right\rceil )} \right\rceil $能否推出$\left\lceil {f(x)} \right\rceil  &lt; f(\left\lceil x \right\rceil )$呢？当然是可以的。<br>\[\begin{array}{l}\left\lceil {f(x)} \right\rceil  &lt; \left\lceil {f(\left\lceil x \right\rceil )} \right\rceil \\ \Rightarrow \left\lceil {f(x)} \right\rceil  \le \left\lceil {f(\left\lceil x \right\rceil )} \right\rceil  - 1 &lt; f(\left\lceil x \right\rceil )\end{array}\]<br><img src="1.jpg" alt><br>所以<br>\[f(y) \in \mathbb{Z} \Rightarrow y \in \mathbb{Z}\]<br>又因为$x \le y &lt; \left\lceil x \right\rceil $，所以不存在整数$y$，矛盾！</p><p>所以证得<br>\[\left\lceil {f(x)} \right\rceil  = \left\lceil {f(\left\lceil x \right\rceil )} \right\rceil \]</p><p>另一个特殊的例子是<br>\[\left\lfloor {\frac{ {x + m}}{n}} \right\rfloor  = \left\lfloor {\frac{ {\left\lfloor x \right\rfloor  + m}}{n}} \right\rfloor \]<br>其中$m$和$n$都是整数，并且$n$是正整数。</p><h2 id="应用2"><a href="#应用2" class="headerlink" title="应用2"></a>应用2</h2><p>接着介绍区间相关的性质。</p><p>求1到1000中使得下列式子成立的$n$一共有多少个？<br>\[\left\lfloor {\sqrt[3]{n}} \right\rfloor |n\]<br>求解方法如下：<br>\[\begin{array}{l}W{\rm{ = }}\sum\limits_{1 \le n \le 1000} {\left[ {\left\lfloor {\sqrt[3]{n}} \right\rfloor |n} \right]} \\ = \sum\limits_{k,n} {\left[ {k = \left\lfloor {\sqrt[3]{n}} \right\rfloor } \right]\left[ {k|n} \right]\left[ {1 \le n \le 1000} \right]} \\ = \sum\limits_{k,m,n} {\left[ { {k^3} \le n &lt; { {(k + 1)}^3}} \right]\left[ {n = km} \right]} \left[ {1 \le n \le 1000} \right]\\ = 1 + \sum\limits_{k,m} {\left[ { {k^3} \le km &lt; { {(k + 1)}^3}} \right]} \left[ {1 \le k &lt; 10} \right]\\ = 1 + \sum\limits_{k,m} {\left[ {m \in [{k^2},{ {(k + 1)}^3}/k)} \right]} \left[ {1 \le k &lt; 10} \right]\\ = 1 + \sum\limits_{1 \le k &lt; 10} {(\left\lceil { {k^2} + 3k + 3 + 1/k} \right\rceil  - \left\lceil { {k^2}} \right\rceil )} \\ = 1 + \sum\limits_{1 \le k &lt; 10} {(3k + 4)} \\ = 172\end{array}\]</p><p>继续推广，求1到$N$中使得上面式子成立的$n$有多少个？<br>令<br>\[K = \left\lfloor {\sqrt[3]{N}} \right\rfloor \]<br>也就是小于等于$\left\lfloor {\sqrt[3]{N}} \right\rfloor $的最大整数。<br>所以<br>\[\begin{array}{l}W = \sum\limits_{1 \le k &lt; K} {(3k + 4)}  + \sum\limits_m {\left[ { {K^3} \le Km \le N} \right]} \\ = \left\lfloor {N/K} \right\rfloor  + \frac{1}{2}{K^2} + \frac{5}{2}K - 3\end{array}\]<br>渐进地等于<br>\[W = \frac{3}{2}{N^{2/3}} + O({N^{1/3}})\]</p><h2 id="应用3"><a href="#应用3" class="headerlink" title="应用3"></a>应用3</h2><p>定义一个实数的谱为：<br>\[Spec(\alpha ) = \{ \left\lfloor \alpha  \right\rfloor ,\left\lfloor {2\alpha } \right\rfloor ,\left\lfloor {3\alpha } \right\rfloor , \ldots \} \]</p><p>很容易证明如果两个实数$\alpha  \ne \beta $，那么<br>\[Spec(\alpha ) \ne Spec(\beta )\]</p><p>假设$\alpha  &lt; \beta $，那么令<br>\[m(\beta  - \alpha ) \ge 1\]<br>所以<br>$m\beta  \ge m\alpha  + 1 \Rightarrow \left\lfloor {m\beta } \right\rfloor  &gt; \left\lfloor {m\alpha } \right\rfloor $<br>所以集合$Spec(\beta )$中小于$\left\lfloor {m\alpha } \right\rfloor $的元素个数小于$m$。而集合$Spec(\alpha )$中小于$\left\lfloor {m\alpha } \right\rfloor $的元素个数大于等于$m$。所以两个集合不相等。</p><p>谱有很多奇妙的性质，例如下面两个谱：<br>\[\begin{array}{l}Spec(\sqrt 2 ) = \{ \left\lfloor {\sqrt 2 } \right\rfloor ,\left\lfloor {2\sqrt 2 } \right\rfloor ,\left\lfloor {3\sqrt 2 } \right\rfloor , \ldots \} \\Spec(2{\rm{ + }}\sqrt 2 ) = \{ \left\lfloor {2{\rm{ + }}\sqrt 2 } \right\rfloor ,\left\lfloor {2(2{\rm{ + }}\sqrt 2 )} \right\rfloor ,\left\lfloor {3(2{\rm{ + }}\sqrt 2 )} \right\rfloor , \ldots \} \end{array}\]<br>可以发现，这两个谱正好划分了正整数集。<br>证明方法也很简单，只要证明对任意正整数$n$，两个集合中小于$n$的元素个数之和为$n$，过程如下：<br>\[\begin{array}{l}\left\lfloor {k\sqrt 2 } \right\rfloor  \le n\\ \Rightarrow k\sqrt 2  &lt; n + 1\\ \Rightarrow k &lt; \frac{ {n + 1}}{ {\sqrt 2 }}\end{array}\]<br>所以第一个集合中小于$n$的元素个数为<br>\[\left\lfloor {\frac{ {n + 1}}{ {\sqrt 2 }}} \right\rfloor \]<br>同理第二个集合中小于$n$的元素个数为<br>\[\left\lfloor {\frac{ {n + 1}}{ {2 + \sqrt 2 }}} \right\rfloor \]<br>所以总个数为<br>\[\begin{array}{l}\left\lfloor {\frac{ {n + 1}}{ {\sqrt 2 }}} \right\rfloor  + \left\lfloor {\frac{ {n + 1}}{ {2 + \sqrt 2 }}} \right\rfloor \\ = \left\lfloor {\frac{ {\sqrt 2 }}{2}(n + 1)} \right\rfloor  + \left\lfloor {\frac{ {2 - \sqrt 2 }}{2}(n + 1)} \right\rfloor \\ = n + 1 + \left\lfloor {\frac{ {\sqrt 2 }}{2}(n + 1)} \right\rfloor  + \left\lfloor { - \frac{ {\sqrt 2 }}{2}(n + 1)} \right\rfloor \\ = n + 1 + \left\lfloor {\frac{ {\sqrt 2 }}{2}(n + 1)} \right\rfloor  + \left\lfloor {\frac{ {\sqrt 2 }}{2}(n + 1)} \right\rfloor  - 1\\ = n\end{array}\]<br>得证。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第6课（下降阶乘幂）</title>
      <link href="/2018/04/02/concrete-math-6/"/>
      <url>/2018/04/02/concrete-math-6/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=28680028&auto=1&height=66"></iframe></div><p>上节课讲到下降阶乘幂和差分运算，这节课继续讲它和差分的各种性质。</p><h1 id="性质1"><a href="#性质1" class="headerlink" title="性质1"></a>性质1</h1><hr><p>首先在后面章节会证明，${(x + y)^{\underline{m}}}$的二项展开形式和普通的${(x + y)^m}$是一样的，这里提一下，暂时用不到。</p><h1 id="性质2"><a href="#性质2" class="headerlink" title="性质2"></a>性质2</h1><hr><p>接下来给出下降阶乘幂为负数的定义：<br>\[{x^{ \underline{- m}}} = \frac{1}{ {(x + 1)(x + 2) \ldots (x + m)}}\]</p><h1 id="性质3"><a href="#性质3" class="headerlink" title="性质3"></a>性质3</h1><hr><p>和普通幂${x^{m + n}} = {x^m}{x^n}$不同，下降阶乘幂有如下性质：<br>\[{x^{\underline{m + n}}} = {x^{\underline{m}}}{(x - m)^{\underline{n}}}\]</p><h1 id="性质4"><a href="#性质4" class="headerlink" title="性质4"></a>性质4</h1><hr><p>上一节课说到，定义下降阶乘幂的好处就是为了求差分方便，下降阶乘幂的差分为：<br>\[\Delta ({x^{\underline{m}}}) = m{x^{\underline{ {m - 1}}}}\]<br>反之，类比不定积分，它的不定和为：<br>\[\sum\nolimits_a^b { {x^{\underline{m}}}\delta x}  = \left. {\frac{ { {x^{\underline{m + 1}}}}}{ {m + 1}}} \right|_a^b\]</p><p>但是这里$m \ne  - 1$，那要是$m =  - 1$怎么办呢？<br>直接运用差分定义可以求出<br>\[\begin{array}{l}{x^{ \underline{- 1}}} = \frac{1}{ {x + 1}} = \Delta f(x) = f(x + 1) - f(x)\\ \Rightarrow f(x) = {H_x}\end{array}\]</p><p>所以<br>\[\sum\nolimits_a^b { {x^{\underline{m}}}\delta x}  = \left\{ {\begin{array}{*{20}{c}}{\left. {\frac{ { {x^{\underline{m + 1}}}}}{ {m + 1}}} \right|_a^b,m \ne  - 1}\\{\left. { {H_x}} \right|_a^b,m =  - 1}\end{array}} \right.\]</p><h1 id="性质5"><a href="#性质5" class="headerlink" title="性质5"></a>性质5</h1><hr><p>在微积分里面，$e^x$的导数是它自身。那么什么函数的差分是自身呢？<br>通过定义可以很容易算出来：<br>\[\begin{array}{l}f(x + 1) - f(x) = f(x)\\ \Rightarrow f(x + 1) = 2f(x)\\ \Rightarrow f(x) = {2^x}\end{array}\]</p><p>进一步推广可以得到：<br>\[\Delta ({c^x}) = {c^{x + 1}} - {c^x} = (c - 1){c^x}\]<br>所以得到如下一种新的等比数列计算方式：<br>\[\sum\limits_{a \le k &lt; b} { {c^k}}  = \sum\nolimits_a^b { {c^x}\delta x}  = \left. {\frac{ { {c^x}}}{ {c - 1}}} \right|_a^b = \frac{ { {c^b} - {c^a}}}{ {c - 1}}\]</p><h1 id="性质6"><a href="#性质6" class="headerlink" title="性质6"></a>性质6</h1><hr><p>结合律和分配律在差分运算里也适用。<br>\[\begin{array}{l}\Delta (cf) = c\Delta (f)\\\Delta (f + g) = \Delta (f) + \Delta (g)\end{array}\]</p><h1 id="性质7"><a href="#性质7" class="headerlink" title="性质7"></a>性质7</h1><hr><p>类似分部积分，这里也可以分部来求差分。<br>\[\begin{array}{l}\Delta (u(x)v(x)) = u(x + 1)v(x + 1) - u(x)v(x)\\ = u(x + 1)v(x + 1) - u(x)v(x + 1) + u(x)v(x + 1) - u(x)v(x)\\ = [u(x + 1) - u(x)]v(x + 1) + u(x)[v(x + 1) - v(x)]\\ = u(x)\Delta (v(x)) + v(x + 1)\Delta (u(x))\end{array}\]<br>这里给出一个新的记号叫做移位运算：<br>\[Ef(x) = f(x + 1)\]<br>所以就得到了差分的分部运算法则：<br>\[\Delta (uv) = u\Delta (v) + Ev\Delta (u)\]<br>对两边求和，又可以得到不定求和的分部运算法则：<br>\[\sum {u\Delta (v)}  = uv - \sum {Ev\Delta (u)} \]</p><p>这个分部法则非常有用，下面举两个例子来说明一下怎么用。</p><h2 id="例1"><a href="#例1" class="headerlink" title="例1"></a>例1</h2><p>一道老题，计算：<br>\[\sum\limits_{k = 0}^n {k{2^k}} \]<br>首先计算<br>\[\sum {x{2^x}\delta x} \]<br>在这里可以令<br>\[u = x,v = {2^x}\]<br>所以<br>\[\sum {x{2^x}\delta x}  = x{2^x} - \sum { {2^{x + 1}}\delta x}  = x{2^x} - {2^{x + 1}} + C\]<br>那么求和式就可以转化为不定求和来算了：<br>\[\begin{array}{l}\sum\limits_{k = 0}^n {k{2^k}}  = \sum\nolimits_0^{n + 1} {x{2^x}\delta x} \\ = \left. {x{2^x} - {2^{x + 1}}} \right|_0^{n + 1}\\ = (n - 1){2^{n + 1}} + 2\end{array}\]</p><h2 id="例2"><a href="#例2" class="headerlink" title="例2"></a>例2</h2><p>计算<br>\[\sum\limits_{0 \le k &lt; n} {k{H_k}} \]<br>首先计算<br>\[\sum {x{H_x}\delta x} \]<br>这里注意要令<br>\[u = {H_x},\Delta v = x\]<br>不能倒过来哦，因为$H_x$的不定和很难求出来的。所以<br>\[\begin{array}{l}\sum {x{H_x}\delta x}  = \frac{ { {x^{\underline{2}}}}}{2}{H_x} - \sum {\frac{ { { {(x + 1)}^{\underline{2}}}}}{2}} {x^{ \underline{- 1}}}\delta x\\ = \frac{ { {x^{\underline{2}}}}}{2}{H_x} - \frac{1}{2}\sum { {x^{\underline{1}}}\delta x} \\ = \frac{ { {x^{\underline{2}}}}}{2}{H_x} - \frac{ { {x^{\underline{2}}}}}{4} + C\end{array}\]</p><p>所以<br>\[\sum\limits_{0 \le k &lt; n} {k{H_k}}  = \sum\nolimits_0^n {x{H_x}\delta x}  = \frac{ { {n^{\underline{2}}}}}{2}({H_n} - \frac{1}{2})\]</p><h1 id="无限求和"><a href="#无限求和" class="headerlink" title="无限求和"></a>无限求和</h1><hr><p>回顾一下以前我们是怎么计算下面求和式的。<br>\[S = {\rm{1}} + \frac{1}{2} + \frac{1}{4} +  \cdots \]<br>首先两边同时乘2，得到：<br>\[2S = 2 + {\rm{1}} + \frac{1}{2} + \frac{1}{4} +  \cdots  = 2 + S\]<br>解出<br>\[S = 2\]</p><p>那么可不可以用同样的方法计算下面式子呢？<br>\[T = 1 + 2 + 4 + 8 +  \cdots \]<br>两边同时乘2，得到：<br>\[2T = 2 + 4 + 8 +  \cdots  = T - 1\]<br>解出<br>\[T = -1\]<br>显然不可能，因为这里的$T$是发散的，所以不能这么求。那么如何用一般的方法来求解呢？</p><p>首先我们只考虑正数求和，求解$\sum\limits_{k \in K} { {a_k}} $，其中$K$是一个无限集合。<br>那么，如果存在$A$，使得对任意$F \subset K$，都有<br>\[\sum\limits_{k \in F} { {a_k}}  \le A\]<br>那么我们说这个最小的$A$就是$\sum\limits_{k \in K} { {a_k}} $的结果。<br>如果不存在这么一个$A$，那么这个求和式就是发散的，即结果为正无穷。</p><p>一般使用中，对于$K = N$，我们可以令$F = \{ 0,1,2, \ldots ,n\} $<br>所以<br>\[\sum\limits_{k \ge 0} { {a_k}}  = \mathop {\lim }\limits_{n \to \infty } \sum\limits_{k = 0}^n { {a_k}} \]</p><p>举两个例子，比如<br>\[\sum\limits_{k \ge 0} { {x^k}}  = \mathop {\lim }\limits_{n \to \infty } \frac{ {1 - {x^{n + 1}}}}{ {1 - x}} = \left\{ {\begin{array}{*{20}{c}}{\frac{1}{ {1 - x}},0 \le x &lt; 1}\\{\infty ,x \ge 1}\end{array}} \right.\]<br>再如：<br>\[\sum\limits_{k \ge 0} {\frac{1}{ {(k + 1)(k + 2)}}}  = \sum\limits_{k \ge 0} { {k^{ \underline{- 2}}}}  = \mathop {\lim }\limits_{n \to \infty } \sum\limits_{k = 0}^n { {k^{ \underline{- 2}}}}  = \mathop {\lim }\limits_{n \to \infty } \left. {\frac{ { {k^{ \underline{- 1}}}}}{ { - 1}}} \right|_0^{n + 1} = 1\]</p><p>剩下的问题就是如何求有正有负的和式？</p><p>可以考虑的方案就是用不同的配对，将正负组合在一起，从而相消求和。</p><p>但是不同的组合方式会得到不同的答案。就比如：<br>\[\sum\limits_{k \ge 0} { { {( - 1)}^k}}  = 1 - 1 + 1 - 1 +  \cdots \]<br>有两种组合方式：<br>\[(1 - 1) + (1 - 1) +  \cdots  = 0\]<br>和<br>\[1 - (1 - 1) - (1 - 1) -  \cdots  = 1\]<br>得到了两种不同的结果。</p><p>事实上，我们可以将正数和负数分开求和，因为正数求和我们已经解决了，所以我们定义：<br>\[x = {x^ + } - {x^ - }\]<br>其中<br>\[{x^ + } = x \cdot [x &gt; 0],{x^ - } =  - x \cdot [x &lt; 0]\]</p><p>所以求和式可以分成两部分分别求和：<br>\[\sum\limits_{k \in K} { {a_k}}  = \sum\limits_{k \in K} { {a_k}^ + }  - \sum\limits_{k \in K} { {a_k}^ - } \]</p><p>最后推广到二重求和：<br>\[\sum\limits_{j \in J,k \in {K_j}} { {a_{j,k}}}  = \sum\limits_{j \in J} {\sum\limits_{k \in {K_j}} { {a_{j,k}}} }  = \sum\limits_{j \in J} { {A_j}}  = A\]</p><p>这里也没啥好细说的，就先了解了解吧。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第5课（8种方法求和）</title>
      <link href="/2018/03/26/concrete-math-5/"/>
      <url>/2018/03/26/concrete-math-5/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=4875084&auto=1&height=66"></iframe></div><p>今天继续讲求和的方法。<br>针对以下求和式，我们用8种方法来求解：<br>\[{S_n} = \sum\limits_{0 \le k \le n} { {k^2}} \]<br>大家应该都已经背上了它的答案：<br>\[{S_n} = \frac{ {n(n + 1)(2n + 1)}}{6}\]</p><h1 id="方法0"><a href="#方法0" class="headerlink" title="方法0"></a>方法0</h1><hr><p><strong>查表。</strong><br>这就不用说了，很多文献都有现成的解，拿来直接用就行了。<br>再给大家推荐一个整数序列查询网站OEIS：<a href="http://oeis.org/" target="_blank" rel="noopener">链接</a></p><h1 id="方法1"><a href="#方法1" class="headerlink" title="方法1"></a>方法1</h1><hr><p><strong>猜答案，然后用数学归纳法证明。</strong><br>这个也不多说了，前提是你得猜得出来，这题的公式还是很难猜的。</p><h1 id="方法2"><a href="#方法2" class="headerlink" title="方法2"></a>方法2</h1><hr><p><strong>扰动法。</strong><br>令<br>\[T = \sum\limits_{0 \le k \le n} { {k^3}} \]<br>所以<br>\[\begin{array}{l}T + {(n + 1)^3}\\ = \sum\limits_{1 \le k \le n + 1} { {k^3}} \\ = \sum\limits_{1 \le k \le n + 1} { { {(k - 1)}^3}}  + \sum\limits_{1 \le k \le n + 1} {(3{k^2} - 3k + 1)} \\ = \sum\limits_{0 \le k \le n} { {k^3}}  + \sum\limits_{1 \le k \le n + 1} {[3{ {(k - 1)}^2} + 3k - 2]} \\ = T + 3{S_n} + \frac{ {3(n + 2)(n + 1)}}{2} - 2(n + 1)\end{array}\]<br>解出<br>\[3{S_n} = {(n + 1)^3} - \frac{ {3(n + 2)(n + 1)}}{2} + 2(n + 1) = n(n + \frac{1}{2})(n + 1)\]<br>最终得到<br>\[{S_n} = \frac{ {n(n + 1)(2n + 1)}}{6}\]</p><p>可以看出，我们本来是要对$k^2$求和的，但是只要对$k^3$用扰动法求和即可，因为求和过程中$k^3$项会被抵消掉。</p><h1 id="方法3"><a href="#方法3" class="headerlink" title="方法3"></a>方法3</h1><hr><p><strong>成套方法。</strong><br>定义如下递归式：<br>\[\begin{array}{l}{R_0} = \alpha \\{R_n} = {R_{n - 1}} + \beta  + \gamma n + \delta {n^2}\end{array}\]<br>由<a href="http://godweiyang.com/2018/03/05/concrete-math-2/">第2课</a>可知，设解的形式为：<br>\[{R_n} = A(n)\alpha  + B(n)\beta  + C(n)\gamma  + D(n)\delta \]<br>分别令${R_n} = 1,n,{n^2}$可以解出<br>\[A(n) = 1,B(n) = n,C(n) = \frac{ {n(n + 1)}}{2}\]<br>再另${R_n} = {n^3}$，可以得到<br>\[3D(n) = {n^3} + 3C(n) - B(n) = n(n + \frac{1}{2})(n + 1)\]<br>即<br>\[D(n) = \frac{ {n(n + 1)(2n + 1)}}{6}\]</p><p>这时如果令<br>\[\alpha  = \beta  = \gamma  = 0,\delta  = 1\]<br>那么<br>\[{R_n} = \sum\limits_{0 \le k \le n} { {k^2}}  = D(n) = \frac{ {n(n + 1)(2n + 1)}}{6}\]</p><h1 id="方法4"><a href="#方法4" class="headerlink" title="方法4"></a>方法4</h1><hr><p><strong>积分法</strong><br>求和式可以近似成积分\[\int_0^n { {x^2}dx}  = {n^3}/3\]<br>但是还少算了一部分误差，设为$E_n$，则有<br>\[{E_n} = {S_n} - \frac{1}{3}{n^3} = {S_{n - 1}} + {n^2} - \frac{1}{3}{n^3} = {E_{n - 1}} + n - \frac{1}{3}\]<br>解得<br>\[{E_n} = \frac{ {3{n^2} + n}}{6}\]<br>所以<br>\[{S_n} = {E_n} + \frac{1}{3}{n^3} = \frac{ {n(n + 1)(2n + 1)}}{6}\]</p><p>其实这种方法就是把最高次直接给算出来了，低次项可以直接求和的。</p><h1 id="方法5"><a href="#方法5" class="headerlink" title="方法5"></a>方法5</h1><hr><p><strong>扩展成二重指标求和</strong><br>\[\begin{array}{l}{S_n} = \sum\limits_{1 \le k \le n} { {k^2}}  = \sum\limits_{1 \le j \le k \le n} { {k^2}} \\ = \sum\limits_{1 \le j \le n} {\sum\limits_{j \le k \le n} k } \\ = \sum\limits_{1 \le j \le n} {(\frac{ {j + n}}{2})(n - j + 1)} \\ = \frac{1}{2}\sum\limits_{1 \le j \le n} {(n(n + 1) + j - {j^2})} \\ = \frac{1}{2}{n^2}(n + 1) + \frac{1}{4}n(n + 1) - \frac{1}{2}{S_n}\\ = \frac{1}{2}n(n + \frac{1}{2})(n + 1) - \frac{1}{2}{S_n}\end{array}\]<br>所以<br>\[{S_n} = \frac{ {n(n + 1)(2n + 1)}}{6}\]</p><h1 id="方法6"><a href="#方法6" class="headerlink" title="方法6"></a>方法6</h1><hr><p><strong>用有限微分求和</strong><br>微分的形式大家都知道，如下：<br>\[\Delta f(x) = f(x + 1) - f(x)\]<br>那如果我们定义<br>\[f(x) = {x^m}\]<br>则有<br>\[\Delta f(x) = {(x + 1)^m} - {x^m}\]<br>似乎并不能和导数形式统一起来，用起来也不方便，那么我们定义一个新的函数，叫做<strong>下降阶乘幂</strong>：<br>\[f(x) = {x^{\underline{m}}} = x(x - 1) \ldots (x - m + 1)\]<br>同理还可以定义<strong>上升阶乘幂</strong>。<br>这个函数有一个很好的性质，那就是<br>\[\Delta ({x^{\underline{m}}}) = m{x^{\underline{ {m - 1}}}}\]<br>令<br>\[g(x) = \Delta f(x)\]<br>那么和积分类似，有<br>\[\sum\nolimits_a^b {g(x)\delta x}  = f(b) - f(a)\]<br>所以<br>\[\sum\limits_{0 \le k &lt; n} { {k^{\underline{m}}}}  = \left. {\frac{ { {k^{ {\underline{m + 1}}}}}}{ {m + 1}}} \right|_0^n = \frac{ { {n^{ {\underline{m + 1}}}}}}{ {m + 1}}\]</p><p>因为有<br>\[{k^2} = {k^{\underline{2}}} + {k^{\underline{1}}}\]<br>所以<br>\[\sum\limits_{0 \le k &lt; n} { {k^2}}  = \frac{ { {n^{\underline{3}}}}}{3} + \frac{ { {n^{\underline{2}}}}}{2} = \frac{ {n(n - 1)(2n - 1)}}{6}\]<br>同样可以得到<br>\[{S_n} = \frac{ {n(n + 1)(2n + 1)}}{6}\]</p><p>下降阶乘幂还有很多好用的性质，下节课继续。</p><h1 id="方法7"><a href="#方法7" class="headerlink" title="方法7"></a>方法7</h1><hr><p><strong>生成函数。</strong><br>以后章节会讲。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第二章作业解答</title>
      <link href="/2018/03/19/concrete-math-hw2/"/>
      <url>/2018/03/19/concrete-math-hw2/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=30569513&auto=1&height=66"></iframe></div><p>这章作业提前先做掉了，不是很难，有些算着麻烦了一点，就是倒数第二题不大解释的清楚。</p><p><img src="1.jpg" alt><br><img src="2.jpg" alt><br><img src="3.jpg" alt><br><img src="4.jpg" alt><br><img src="5.jpg" alt><br><img src="6.jpg" alt><br><img src="7.jpg" alt><br><img src="8.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第4课（多重求和方法）</title>
      <link href="/2018/03/19/concrete-math-4/"/>
      <url>/2018/03/19/concrete-math-4/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=436514312&auto=1&height=66"></iframe></div><p>今天讲了多重求和，也就是一个和式由多个下标来指定。</p><p>首先是最简单的形式：<br>\[\sum\limits_{1 \le j,k \le n} { {a_j}{b_k}}  = (\sum\limits_{1 \le j \le n} { {a_j}} )(\sum\limits_{1 \le k \le n} { {a_k}} )\]</p><h1 id="例题1"><a href="#例题1" class="headerlink" title="例题1"></a>例题1</h1><hr><p>下面给出一个对称矩阵：<br>\[A(i,j) = {a_i}{a_j}\]<br>求：<br>\[S = \sum\limits_{1 \le j \le k \le n} { {a_j}{a_k}} \]<br>这是这个矩阵的上三角加对角线求和，因为是对称的嘛，可以补全下三角，加上对角线就行了。<br>\[2S = \sum\limits_{1 \le j,k \le n} { {a_j}{a_k}}  + \sum\limits_{1 \le j = k \le n} { {a_j}{a_k}}  = {(\sum\limits_{1 \le k \le n} { {a_k}} )^2} + \sum\limits_{1 \le k \le n}^{} { {a_k}^2} \]<br>所以<br>\[S = \frac{1}{2}({(\sum\limits_{1 \le k \le n} { {a_k}} )^2} + \sum\limits_{1 \le k \le n}^{} { {a_k}^2} )\]</p><h1 id="例题2"><a href="#例题2" class="headerlink" title="例题2"></a>例题2</h1><hr><p>下面再看一个例子：<br>\[S = \sum\limits_{1 \le j &lt; k \le n} {({a_j} - {a_k})({b_j} - {b_k})} \]<br>同样模仿上例调换$j,k$位置，得到：<br>\[\begin{array}{l}2S = \sum\limits_{1 \le j,k \le n} {({a_j} - {a_k})({b_j} - {b_k})}  - \sum\limits_{1 \le j = k \le n} {({a_j} - {a_k})({b_j} - {b_k})} \\ = \sum\limits_{1 \le j,k \le n} {({a_j}{b_j} - {a_j}{b_k} - {a_k}{b_j} + {a_k}{b_k})} \\ = 2\sum\limits_{1 \le j,k \le n} { {a_j}{b_j}}  - 2\sum\limits_{1 \le j,k \le n} { {a_j}{b_k}} \\ = 2n\sum\limits_{1 \le j \le n} { {a_j}{b_j}}  - 2(\sum\limits_{1 \le j \le n} { {a_j}} )(\sum\limits_{1 \le k \le n} { {b_k}} )\end{array}\]<br>所以<br>\[S = n\sum\limits_{1 \le j \le n} { {a_j}{b_j}}  - (\sum\limits_{1 \le j \le n} { {a_j}} )(\sum\limits_{1 \le k \le n} { {b_k}} )\]<br>至此解完，然后可以推出一个著名的不等式————切比雪夫不等式：<br>\[(\sum\limits_{1 \le j \le n} { {a_j}} )(\sum\limits_{1 \le k \le n} { {b_k}} ) = n\sum\limits_{1 \le j \le n} { {a_j}{b_j}}  - \sum\limits_{1 \le j &lt; k \le n} {({a_j} - {a_k})({b_j} - {b_k})} \]<br>如果<br>\[{a_1} \le {a_2} \le  \cdots  \le {a_n},{b_1} \le {b_2} \le  \cdots  \le {b_n}\]<br>那么<br>\[(\sum\limits_{1 \le j \le n} { {a_j}} )(\sum\limits_{1 \le k \le n} { {b_k}} ) \le n\sum\limits_{1 \le j \le n} { {a_j}{b_j}} \]<br>反之如果<br>\[{a_1} \le {a_2} \le  \cdots  \le {a_n},{b_1} \ge {b_2} \ge  \cdots  \ge {b_n}\]<br>那么<br>\[(\sum\limits_{1 \le j \le n} { {a_j}} )(\sum\limits_{1 \le k \le n} { {b_k}} ) \ge n\sum\limits_{1 \le j \le n} { {a_j}{b_j}} \]<br>更一般的结论，给定两个序列$a$和$b$，求下面式子最大值与最小值：<br>\[\sum\limits_{k = 1}^n { {a_k}{b_{p(k)}}} \]<br>其中$p(k)$是$\{ 1,2, \cdots ,n\} $的一个排列。<br>答案是$b$增序最大，降序最小，至于为什么，下面给出两种证明方法。</p><h3 id="方法1"><a href="#方法1" class="headerlink" title="方法1"></a>方法1</h3><p><img src="1.jpg" alt><br>如上图所示，$a$和$b$按照递增顺序排列，每个方格的面积代表$a_i$与$b_j$的乘积，记为$s_{ij}$。<br>那么上面的求和式其实就是每一行每一列都必须有且只有一块被取。<br>考虑第一行，如果不取$s_{11}$，取其他的$s_{1j}$，那么第一列也只能取其他的$s_{i1}$，这样的话$s_{ij}$也就取不了了。但是发现<br>\[s_{11}+s_{ij} \ge s_{i1}+s_{1j}\]<br>并且两种取法影响的行和列都是相同的，这说明了，取$s_{i1}$和$s_{1j}$不如取$s_{11}$和$s_{ij}$。所以$s_{11}$必取，然后第一行第一列就不能取了。剩下的方阵用相同的方法可以得出必取$s_{22}, \cdots ,s_{nn}$，也就是主对角线。<br>同理最小取法用副对角线可以推出。</p><h3 id="方法2"><a href="#方法2" class="headerlink" title="方法2"></a>方法2</h3><p>设数列$a$和$b$非单调递减，那么有如下证明：<br>\[\begin{array}{l}{S_k} = \sum\limits_{i = 1}^k { {b_i}} ,{ {S’}_k} = \sum\limits_{i = 1}^k { {b_{p(i)}}} \\ \Rightarrow {S_k} \le { {S’}_k}\\ \Rightarrow \\\sum\limits_{i = 1}^n { {a_i}{b_i}}  = {S_1}{a_1} - {S_1}{a_2} + {S_2}{a_2} - {S_2}{a_3} +  \cdots  + {S_n}{a_n}\\ = \sum\limits_{i = 1}^{n - 1} { {S_i}} ({a_i} - {a_{i + 1}}) + {S_n}{a_n}\\ \ge \sum\limits_{i = 1}^{n - 1} { { {S’}_i}} ({a_i} - {a_{i + 1}}) + {S_n}{a_n}\\ = \sum\limits_{i = 1}^n { {a_i}{b_{p(i)}}} \end{array}\]<br>反之亦证。</p><p>题外话，其实切比雪夫不等式原来是以微积分形式给出的：<br>如果函数$f(x)$和$g(x)$非单调递减，那么有：<br>\[(\int_a^b {f(x)dx} )(\int_a^b {g(x)dx} ) \le (b - a)(\int_a^b {f(x)g(x)dx} )\]</p><h1 id="例题3"><a href="#例题3" class="headerlink" title="例题3"></a>例题3</h1><hr><p>求<br>\[S = \sum\limits_{1 \le j &lt; k \le n} {\frac{1}{ {k - j}}} \]<br>我将用三种方法来求解这个式子。</p><h3 id="方法1-1"><a href="#方法1-1" class="headerlink" title="方法1"></a>方法1</h3><p>首先将$j$和$k$分开，首先计算对$j$求和：<br>\[\begin{array}{l}S = \sum\limits_{1 \le k \le n} {\sum\limits_{1 \le j &lt; k} {\frac{1}{ {k - j}}} } \\ = \sum\limits_{1 \le k \le n} {\sum\limits_{1 \le k - j &lt; k} {\frac{1}{j}} } \\ = \sum\limits_{1 \le k \le n} {\sum\limits_{0 &lt; j \le k - 1} {\frac{1}{j}} } \\ = \sum\limits_{1 \le k \le n} { {H_{k - 1}}} \\ = \sum\limits_{0 \le k &lt; n} { {H_k}} \end{array}\]</p><h3 id="方法2-1"><a href="#方法2-1" class="headerlink" title="方法2"></a>方法2</h3><p>先计算对$k$求和：<br>\[\begin{array}{l}S = \sum\limits_{1 \le j \le n} {\sum\limits_{j &lt; k \le n} {\frac{1}{ {k - j}}} } \\ = \sum\limits_{1 \le j \le n} {\sum\limits_{j &lt; k + j \le n} {\frac{1}{k}} } \\ = \sum\limits_{1 \le j \le n} {\sum\limits_{0 &lt; k \le n - j} {\frac{1}{k}} } \\ = \sum\limits_{1 \le j \le n} { {H_{n - j}}} \\ = \sum\limits_{0 \le j &lt; n} { {H_j}} \end{array}\]</p><h3 id="方法3"><a href="#方法3" class="headerlink" title="方法3"></a>方法3</h3><p>按对角线求和：<br>\[\begin{array}{l}S = \sum\limits_{1 \le j &lt; k \le n} {\frac{1}{ {k - j}}} \\ = \sum\limits_{1 \le j &lt; k + j \le n} {\frac{1}{k}} \\ = \sum\limits_{1 \le k \le n} {\sum\limits_{1 \le j \le n - k} {\frac{1}{k}} } \\ = \sum\limits_{1 \le k \le n} {\frac{ {n - k}}{k}} \\ = n\sum\limits_{1 \le k \le n} {\frac{1}{k} - } \sum\limits_{1 \le k \le n} 1 \\ = n{H_n} - n\end{array}\]</p><p>由此得到了一个完全不同的表示形式！<br>所以我们得到了：<br>\[\sum\limits_{0 \le j &lt; n} { {H_j}}  = n{H_n} - n\]</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>毕业论文相关细节记录</title>
      <link href="/2018/03/18/graduate-technique/"/>
      <url>/2018/03/18/graduate-technique/</url>
      
        <content type="html"><![CDATA[<div align="middle"><audio src="http://mp3.qqmusic.cc/yq/3583938.mp3" controls autoplay>Your browser does not support the element.</audio></div><h1 id="随机数种子"><a href="#随机数种子" class="headerlink" title="随机数种子"></a>随机数种子</h1><hr><p>这是玄学，姑且就设为我的QQ号792321264，看起来效果不错。</p><h1 id="神经网络维数"><a href="#神经网络维数" class="headerlink" title="神经网络维数"></a>神经网络维数</h1><hr><p>不断测试发现，64维效果最好，但最后可能改成512维的。<br>而且64维的话CPU跑的比GPU还要快6倍，但是512维的话GPU就比CPU快6倍左右了。<br>所以维度低还是用CPU比较好。</p><h1 id="结点分数"><a href="#结点分数" class="headerlink" title="结点分数"></a>结点分数</h1><hr><p>\[\begin{array}{l}Score(A \to BC) = \lambda d \cdot (W \cdot e + b) + Spcfg(A)\\Spcfg(A) = \log (Spcfg(B) \cdot Spcfg(C) \cdot p(A \to BC))\end{array}\]<br>其中$d,W,b$是权值矩阵，$\lambda$是超参数，测试发现设为100左右效果最好。</p><h1 id="结点表示"><a href="#结点表示" class="headerlink" title="结点表示"></a>结点表示</h1><hr><p>\[e = LST{M^{left}}({e_{right}})\]<br>至于用左儿子还是右儿子作为LSTM，还是加一层动态规划记录两者最优值，小数据上暂时没有太大差别。</p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><hr><p>\[L(\theta ) = Scor{e_{predict}}(ROOT) - Scor{e_{gold}}(ROOT) + k \cdot \Delta (predict,gold) + \frac{1}{2}{\left| \theta  \right|^2}\]<br>其中正则项加了可以使loss下降更稳定，但是效果貌似不如不加，可能是因为数据集太小吧。<br>$k$一般取0.1。</p><h1 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h1><hr><p>batch取50左右效果最好，不过我用的是dynet自带的自动batch，手动batch还不是很会写，所以效率提升不是很大。</p><h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><hr><p>原来是4层循环，用时特别久，改进了一下变成6层循环效率大大提高。<br>原算法：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">from</span> <span class="token number">0</span> to n    <span class="token keyword">for</span> j <span class="token keyword">from</span> <span class="token number">0</span> to n        <span class="token keyword">for</span> k <span class="token keyword">from</span> i to j            <span class="token keyword">for</span> A<span class="token operator">-</span><span class="token operator">></span>BC <span class="token keyword">in</span> grammar                balabala<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></code></pre><p>改进算法：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">from</span> <span class="token number">0</span> to n    <span class="token keyword">for</span> j <span class="token keyword">from</span> <span class="token number">0</span> to n        <span class="token keyword">for</span> k <span class="token keyword">from</span> i to j            <span class="token keyword">for</span> B <span class="token keyword">in</span> nodetype<span class="token punctuation">[</span>left<span class="token punctuation">]</span>                <span class="token keyword">for</span> C <span class="token keyword">in</span> nodetype<span class="token punctuation">[</span>right<span class="token punctuation">]</span>                    <span class="token keyword">for</span> A <span class="token keyword">in</span> panode<span class="token punctuation">[</span>BC<span class="token punctuation">]</span>                        balabala<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></code></pre><h1 id="未来改进"><a href="#未来改进" class="headerlink" title="未来改进"></a>未来改进</h1><hr><ul><li>如果测试集中的句法规则在训练集中没有出现的话，会直接产生None的结果，是否可以考虑产生新的规则，这样就可以对所有句子进行句法分析了？</li><li>效率虽然有了很大提升，但是大数据依然跑的很慢，可以考虑加上手动batch、减少规则数量、动态规划算法优化等等。</li></ul><p>最后附上我的主要代码（丑是丑了点，不喜勿喷，封装什么的以后再说）：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> random<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> collections <span class="token keyword">import</span> defaultdict <span class="token keyword">as</span> dd<span class="token punctuation">,</span> defaultdict<span class="token keyword">from</span> itertools <span class="token keyword">import</span> count<span class="token keyword">import</span> re<span class="token keyword">import</span> time<span class="token keyword">import</span> math<span class="token keyword">import</span> _dynet <span class="token keyword">as</span> dydyparams <span class="token operator">=</span> dy<span class="token punctuation">.</span>DynetParams<span class="token punctuation">(</span><span class="token punctuation">)</span>dyparams<span class="token punctuation">.</span>from_args<span class="token punctuation">(</span><span class="token punctuation">)</span>dyparams<span class="token punctuation">.</span>set_requested_gpus<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>dyparams<span class="token punctuation">.</span>set_mem<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">)</span>dyparams<span class="token punctuation">.</span>set_random_seed<span class="token punctuation">(</span><span class="token number">792321264</span><span class="token punctuation">)</span>dyparams<span class="token punctuation">.</span>init<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># ==============================================================</span><span class="token comment" spellcheck="true"># read train file</span>DEBUG <span class="token operator">=</span> <span class="token boolean">True</span>train_string_file <span class="token operator">=</span> <span class="token string">"data/train.strings"</span>train_tree_file <span class="token operator">=</span> <span class="token string">"data/train.trees.pre.unk"</span>dev_string_file <span class="token operator">=</span> <span class="token string">"data/dev.strings"</span>dev_tree_file <span class="token operator">=</span> <span class="token string">"data/dev.trees"</span>dev_parse_file <span class="token operator">=</span> <span class="token string">"data/dev.parses"</span><span class="token keyword">if</span> DEBUG<span class="token punctuation">:</span>    train_string_file <span class="token operator">=</span> <span class="token string">"data/train_small.strings"</span>    train_tree_file <span class="token operator">=</span> <span class="token string">"data/train_small.trees.pre.unk"</span>    dev_string_file <span class="token operator">=</span> <span class="token string">"data/dev_small.strings"</span>    dev_tree_file <span class="token operator">=</span> <span class="token string">"data/dev_small.trees"</span>    dev_parse_file <span class="token operator">=</span> <span class="token string">"data/dev_small.parses"</span>train_string <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>train_tree <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>words <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">with</span> open<span class="token punctuation">(</span>train_string_file<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> fh<span class="token punctuation">:</span>    <span class="token keyword">for</span> line <span class="token keyword">in</span> fh<span class="token punctuation">:</span>        train_string<span class="token punctuation">.</span>append<span class="token punctuation">(</span>line<span class="token punctuation">)</span>        <span class="token keyword">for</span> word <span class="token keyword">in</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            words<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word<span class="token punctuation">)</span>words<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"&lt;unk>"</span><span class="token punctuation">)</span><span class="token keyword">with</span> open<span class="token punctuation">(</span>train_tree_file<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> fh<span class="token punctuation">:</span>    <span class="token keyword">for</span> line <span class="token keyword">in</span> fh<span class="token punctuation">:</span>        train_tree<span class="token punctuation">.</span>append<span class="token punctuation">(</span>line<span class="token punctuation">)</span>w2i <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>count<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>next<span class="token punctuation">)</span><span class="token keyword">for</span> word <span class="token keyword">in</span> words<span class="token punctuation">:</span>    w2i<span class="token punctuation">[</span>word<span class="token punctuation">]</span>i2w <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span>w <span class="token keyword">for</span> w<span class="token punctuation">,</span> i <span class="token keyword">in</span> w2i<span class="token punctuation">.</span>iteritems<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>nwords <span class="token operator">=</span> len<span class="token punctuation">(</span>w2i<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># ==============================================================</span><span class="token comment" spellcheck="true"># read grammar file</span>nonTerms <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>rules_set1 <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>rules_set2 <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>rules <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>lexicons <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>origText <span class="token operator">=</span> list<span class="token punctuation">(</span><span class="token punctuation">)</span>probs <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>float<span class="token punctuation">)</span>node_pa <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token keyword">def</span> <span class="token function">read_grammar</span><span class="token punctuation">(</span>f<span class="token punctuation">)</span><span class="token punctuation">:</span>    grammar <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    file <span class="token operator">=</span> open<span class="token punctuation">(</span>f<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> rule <span class="token keyword">in</span> file<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># AAA -> # BBB @ prob</span>        tokens <span class="token operator">=</span> re<span class="token punctuation">.</span>split<span class="token punctuation">(</span>r<span class="token string">"\-\>|\@"</span><span class="token punctuation">,</span> rule<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        lhs <span class="token operator">=</span> tokens<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>        rhs <span class="token operator">=</span> tokens<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span>r<span class="token string">'\''</span><span class="token punctuation">)</span>        rhs <span class="token operator">=</span> rhs<span class="token punctuation">.</span>strip<span class="token punctuation">(</span>r<span class="token string">'\"'</span><span class="token punctuation">)</span>        prob <span class="token operator">=</span> tokens<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>        probs<span class="token punctuation">[</span><span class="token punctuation">(</span>lhs<span class="token punctuation">,</span> rhs<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> float<span class="token punctuation">(</span>prob<span class="token punctuation">)</span>        nonTerms<span class="token punctuation">.</span>add<span class="token punctuation">(</span>lhs<span class="token punctuation">)</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>rhs<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>            rules_set1<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token punctuation">(</span>lhs<span class="token punctuation">,</span> rhs<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            rules_set2<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token punctuation">(</span>lhs<span class="token punctuation">,</span> rhs<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> rhs <span class="token keyword">in</span> node_pa<span class="token punctuation">:</span>                node_pa<span class="token punctuation">[</span>rhs<span class="token punctuation">]</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span>lhs<span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                node_pa<span class="token punctuation">[</span>rhs<span class="token punctuation">]</span> <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>                node_pa<span class="token punctuation">[</span>rhs<span class="token punctuation">]</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span>lhs<span class="token punctuation">)</span>        rules<span class="token punctuation">[</span>lhs<span class="token punctuation">]</span> <span class="token operator">=</span> rhs        <span class="token keyword">if</span> len<span class="token punctuation">(</span>rhs<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span> <span class="token operator">and</span> rhs <span class="token operator">!=</span> <span class="token string">'&lt;unk>'</span><span class="token punctuation">:</span>            lexicons<span class="token punctuation">.</span>append<span class="token punctuation">(</span>rhs<span class="token punctuation">)</span><span class="token keyword">if</span> DEBUG<span class="token punctuation">:</span>    grammar <span class="token operator">=</span> read_grammar<span class="token punctuation">(</span><span class="token string">'data/pcfg_small'</span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>    grammar <span class="token operator">=</span> read_grammar<span class="token punctuation">(</span><span class="token string">'data/pcfg'</span><span class="token punctuation">)</span><span class="token keyword">print</span> rules_set1<span class="token punctuation">.</span>__len__<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> rules_set2<span class="token punctuation">.</span>__len__<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># ==============================================================</span><span class="token comment" spellcheck="true"># LSTM and parameters initialization</span>EPOCH <span class="token operator">=</span> <span class="token number">40</span>EMBDDING_SIZE <span class="token operator">=</span> <span class="token number">512</span>lamda <span class="token operator">=</span> <span class="token number">100</span>k <span class="token operator">=</span> <span class="token number">0.1</span>model <span class="token operator">=</span> dy<span class="token punctuation">.</span>ParameterCollection<span class="token punctuation">(</span><span class="token punctuation">)</span>builder <span class="token operator">=</span> dy<span class="token punctuation">.</span>FastLSTMBuilder<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> EMBDDING_SIZE<span class="token punctuation">,</span> EMBDDING_SIZE<span class="token punctuation">,</span> model<span class="token punctuation">)</span>trainer <span class="token operator">=</span> dy<span class="token punctuation">.</span>AdamTrainer<span class="token punctuation">(</span>model<span class="token punctuation">)</span>WORDS_LOOKUP <span class="token operator">=</span> model<span class="token punctuation">.</span>add_lookup_parameters<span class="token punctuation">(</span><span class="token punctuation">(</span>nwords<span class="token punctuation">,</span> EMBDDING_SIZE<span class="token punctuation">)</span><span class="token punctuation">)</span>pd <span class="token operator">=</span> model<span class="token punctuation">.</span>add_parameters<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> EMBDDING_SIZE<span class="token punctuation">)</span><span class="token punctuation">)</span>pW <span class="token operator">=</span> model<span class="token punctuation">.</span>add_parameters<span class="token punctuation">(</span><span class="token punctuation">(</span>EMBDDING_SIZE<span class="token punctuation">,</span> EMBDDING_SIZE<span class="token punctuation">)</span><span class="token punctuation">)</span>pb <span class="token operator">=</span> model<span class="token punctuation">.</span>add_parameters<span class="token punctuation">(</span><span class="token punctuation">(</span>EMBDDING_SIZE<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># ==============================================================</span><span class="token comment" spellcheck="true"># construct trees</span><span class="token keyword">class</span> <span class="token class-name">MTree</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lhs<span class="token punctuation">,</span> wrd<span class="token operator">=</span>None<span class="token punctuation">,</span> subs<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>label <span class="token operator">=</span> lhs        self<span class="token punctuation">.</span>word <span class="token operator">=</span> wrd        self<span class="token punctuation">.</span>subs <span class="token operator">=</span> subs        self<span class="token punctuation">.</span>str <span class="token operator">=</span> None    <span class="token keyword">def</span> <span class="token function">is_lexicon</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>word <span class="token keyword">is</span> <span class="token operator">not</span> None    <span class="token keyword">def</span> <span class="token function">dostr</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string">"(%s %s)"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>label<span class="token punctuation">,</span> self<span class="token punctuation">.</span>word<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>is_lexicon<span class="token punctuation">(</span><span class="token punctuation">)</span> \                <span class="token keyword">else</span> <span class="token string">"(%s %s)"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>label<span class="token punctuation">,</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>map<span class="token punctuation">(</span>str<span class="token punctuation">,</span> self<span class="token punctuation">.</span>subs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__str__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token boolean">True</span> <span class="token operator">or</span> self<span class="token punctuation">.</span>str <span class="token keyword">is</span> None<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>str <span class="token operator">=</span> self<span class="token punctuation">.</span>dostr<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>str<span class="token keyword">def</span> <span class="token function">helper</span><span class="token punctuation">(</span>next<span class="token punctuation">,</span> text<span class="token punctuation">,</span> backPointers<span class="token punctuation">,</span> terminals<span class="token punctuation">,</span> score<span class="token punctuation">)</span><span class="token punctuation">:</span>    begin <span class="token operator">=</span> next<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    end <span class="token operator">=</span> next<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    A <span class="token operator">=</span> next<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>    <span class="token keyword">if</span> next <span class="token operator">not</span> <span class="token keyword">in</span> backPointers<span class="token punctuation">:</span>        <span class="token keyword">if</span> next <span class="token keyword">in</span> terminals<span class="token punctuation">:</span>   <span class="token comment" spellcheck="true">#base condition</span>            word <span class="token operator">=</span> origText<span class="token punctuation">[</span>next<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>            node <span class="token operator">=</span> MTree<span class="token punctuation">(</span>lhs<span class="token operator">=</span>A<span class="token punctuation">,</span> subs<span class="token operator">=</span>None<span class="token punctuation">,</span> wrd<span class="token operator">=</span>word<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>node<span class="token punctuation">,</span> score<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>split<span class="token punctuation">,</span> B<span class="token punctuation">,</span> C<span class="token punctuation">)</span> <span class="token operator">=</span> backPointers<span class="token punctuation">[</span>next<span class="token punctuation">]</span>    next1 <span class="token operator">=</span> <span class="token punctuation">(</span>begin<span class="token punctuation">,</span> split<span class="token punctuation">,</span> B<span class="token punctuation">)</span>    next2 <span class="token operator">=</span> <span class="token punctuation">(</span>split<span class="token punctuation">,</span> end<span class="token punctuation">,</span> C<span class="token punctuation">)</span>    t1<span class="token punctuation">,</span> s1 <span class="token operator">=</span> helper<span class="token punctuation">(</span>next1<span class="token punctuation">,</span> text<span class="token punctuation">,</span> backPointers<span class="token punctuation">,</span> terminals<span class="token punctuation">,</span> score<span class="token punctuation">)</span>    t2<span class="token punctuation">,</span> s2 <span class="token operator">=</span> helper<span class="token punctuation">(</span>next2<span class="token punctuation">,</span> text<span class="token punctuation">,</span> backPointers<span class="token punctuation">,</span> terminals<span class="token punctuation">,</span> score<span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>MTree<span class="token punctuation">(</span>lhs<span class="token operator">=</span>A<span class="token punctuation">,</span> subs<span class="token operator">=</span><span class="token punctuation">[</span>t1<span class="token punctuation">,</span> t2<span class="token punctuation">]</span><span class="token punctuation">,</span> wrd<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">,</span> score<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">backtrack</span><span class="token punctuation">(</span>text<span class="token punctuation">,</span> backPointers<span class="token punctuation">,</span> terminals<span class="token punctuation">,</span> score<span class="token punctuation">)</span><span class="token punctuation">:</span>    n <span class="token operator">=</span> len<span class="token punctuation">(</span>text<span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> n<span class="token punctuation">,</span> <span class="token string">'S'</span><span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token keyword">in</span> backPointers<span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>None<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>    t<span class="token punctuation">,</span> s <span class="token operator">=</span> helper<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> n<span class="token punctuation">,</span> <span class="token string">'S'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> text<span class="token punctuation">,</span> backPointers<span class="token punctuation">,</span> terminals<span class="token punctuation">,</span> score<span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>t<span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">math_log</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> x <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">100</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">score_calc</span><span class="token punctuation">(</span>d<span class="token punctuation">,</span> W<span class="token punctuation">,</span> p<span class="token punctuation">,</span> b<span class="token punctuation">,</span> lamda<span class="token punctuation">,</span> s_pcfg<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> d <span class="token operator">*</span> <span class="token punctuation">(</span>W <span class="token operator">*</span> p <span class="token operator">+</span> b<span class="token punctuation">)</span> <span class="token operator">*</span> lamda <span class="token operator">+</span> s_pcfg<span class="token keyword">def</span> <span class="token function">cal_loss</span><span class="token punctuation">(</span>result<span class="token punctuation">,</span> gold<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> result <span class="token operator">==</span> None<span class="token punctuation">:</span>        <span class="token keyword">return</span> dy<span class="token punctuation">.</span>inputTensor<span class="token punctuation">(</span>list<span class="token punctuation">(</span><span class="token punctuation">[</span>len<span class="token punctuation">(</span>gold<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    result <span class="token operator">=</span> result<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>    gold <span class="token operator">=</span> gold<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>    cnt <span class="token operator">=</span> dy<span class="token punctuation">.</span>inputTensor<span class="token punctuation">(</span>list<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>result<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> result<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">!=</span> gold<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span>            cnt <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">return</span> cnt<span class="token keyword">def</span> <span class="token function">cal_gold</span><span class="token punctuation">(</span>gold<span class="token punctuation">,</span> d<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>    words <span class="token operator">=</span> gold<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>    n <span class="token operator">=</span> len<span class="token punctuation">(</span>words<span class="token punctuation">)</span>    <span class="token keyword">if</span> n <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>        A <span class="token operator">=</span> words<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>        word <span class="token operator">=</span> words<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># print gold, word, w2i[word]</span>        LSTM <span class="token operator">=</span> builder<span class="token punctuation">.</span>initial_state<span class="token punctuation">(</span><span class="token punctuation">)</span>        TMP <span class="token operator">=</span> LSTM<span class="token punctuation">.</span>add_input<span class="token punctuation">(</span>WORDS_LOOKUP<span class="token punctuation">[</span>w2i<span class="token punctuation">[</span>word<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        e <span class="token operator">=</span> TMP<span class="token punctuation">.</span>output<span class="token punctuation">(</span><span class="token punctuation">)</span>        s_pcfg <span class="token operator">=</span> math_log<span class="token punctuation">(</span>probs<span class="token punctuation">[</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> word<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        s <span class="token operator">=</span> score_calc<span class="token punctuation">(</span>d<span class="token punctuation">,</span> W<span class="token punctuation">,</span> e<span class="token punctuation">,</span> b<span class="token punctuation">,</span> lamda<span class="token punctuation">,</span> probs<span class="token punctuation">[</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> word<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>e<span class="token punctuation">,</span> s<span class="token punctuation">,</span> s_pcfg<span class="token punctuation">,</span> TMP<span class="token punctuation">,</span> A<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        sz <span class="token operator">=</span> len<span class="token punctuation">(</span>gold<span class="token punctuation">)</span>        p <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> xrange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> sz<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> gold<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">' '</span><span class="token punctuation">:</span>                p <span class="token operator">=</span> i                <span class="token keyword">break</span>        m <span class="token operator">=</span> <span class="token number">0</span>        cnt <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> xrange<span class="token punctuation">(</span>p<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> sz<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> gold<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'('</span><span class="token punctuation">:</span>                cnt <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token keyword">elif</span> gold<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">')'</span><span class="token punctuation">:</span>                cnt <span class="token operator">-=</span> <span class="token number">1</span>            <span class="token keyword">if</span> cnt <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                m <span class="token operator">=</span> i                <span class="token keyword">break</span>        x1<span class="token punctuation">,</span> s1<span class="token punctuation">,</span> s1_pcfg<span class="token punctuation">,</span> LSTM1<span class="token punctuation">,</span> B <span class="token operator">=</span> cal_gold<span class="token punctuation">(</span>gold<span class="token punctuation">[</span>p<span class="token operator">+</span><span class="token number">1</span> <span class="token punctuation">:</span> m<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> d<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span>        x2<span class="token punctuation">,</span> s2<span class="token punctuation">,</span> s2_pcfg<span class="token punctuation">,</span> LSTM2<span class="token punctuation">,</span> C <span class="token operator">=</span> cal_gold<span class="token punctuation">(</span>gold<span class="token punctuation">[</span>m<span class="token operator">+</span><span class="token number">2</span> <span class="token punctuation">:</span> sz<span class="token number">-1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> d<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span>        A <span class="token operator">=</span> gold<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span>p<span class="token punctuation">]</span>        TMP <span class="token operator">=</span> LSTM2<span class="token punctuation">.</span>add_input<span class="token punctuation">(</span>x1<span class="token punctuation">)</span>        e <span class="token operator">=</span> TMP<span class="token punctuation">.</span>output<span class="token punctuation">(</span><span class="token punctuation">)</span>        s_pcfg <span class="token operator">=</span> math_log<span class="token punctuation">(</span>probs<span class="token punctuation">[</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token operator">+</span><span class="token string">" "</span><span class="token operator">+</span>C<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> s1_pcfg <span class="token operator">+</span> s2_pcfg        ss1 <span class="token operator">=</span> score_calc<span class="token punctuation">(</span>d<span class="token punctuation">,</span> W<span class="token punctuation">,</span> e<span class="token punctuation">,</span> b<span class="token punctuation">,</span> lamda<span class="token punctuation">,</span> s_pcfg<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>e<span class="token punctuation">,</span> ss1<span class="token punctuation">,</span> s_pcfg<span class="token punctuation">,</span> TMP<span class="token punctuation">,</span> A<span class="token punctuation">)</span>total_time <span class="token operator">=</span> <span class="token number">0.0</span><span class="token comment" spellcheck="true"># print nonTerms.__len__()</span>ff <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">"loss.txt"</span><span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> xrange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> EPOCH<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span> <span class="token string">"epoch %d"</span> <span class="token operator">%</span> epoch    sumloss <span class="token operator">=</span> <span class="token number">0</span>    num <span class="token operator">=</span> len<span class="token punctuation">(</span>train_string<span class="token punctuation">)</span>    batch <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> idx<span class="token punctuation">,</span> line <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_string<span class="token punctuation">)</span><span class="token punctuation">:</span>        sstart <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>        gold <span class="token operator">=</span> train_tree<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>        sent <span class="token operator">=</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>        origText <span class="token operator">=</span> list<span class="token punctuation">(</span>sent<span class="token punctuation">)</span>        n <span class="token operator">=</span> len<span class="token punctuation">(</span>sent<span class="token punctuation">)</span>        d <span class="token operator">=</span> pd<span class="token punctuation">.</span>expr<span class="token punctuation">(</span><span class="token punctuation">)</span>        W <span class="token operator">=</span> pW<span class="token punctuation">.</span>expr<span class="token punctuation">(</span><span class="token punctuation">)</span>        b <span class="token operator">=</span> pb<span class="token punctuation">.</span>expr<span class="token punctuation">(</span><span class="token punctuation">)</span>        terminals <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        embdding <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        score <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>float<span class="token punctuation">)</span>        score_pcfg <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>float<span class="token punctuation">)</span>        backPointers <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        LSTM <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        node_rules <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>            begin <span class="token operator">=</span> i            end <span class="token operator">=</span> i <span class="token operator">+</span> <span class="token number">1</span>            node_rules<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>            word <span class="token operator">=</span> sent<span class="token punctuation">[</span>i<span class="token punctuation">]</span>            <span class="token keyword">for</span> A <span class="token keyword">in</span> nonTerms<span class="token punctuation">:</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>A<span class="token punctuation">,</span> word<span class="token punctuation">)</span> <span class="token keyword">in</span> rules_set1<span class="token punctuation">:</span>                    LSTM<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> builder<span class="token punctuation">.</span>initial_state<span class="token punctuation">(</span><span class="token punctuation">)</span>                    LSTM<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> LSTM<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>add_input<span class="token punctuation">(</span>WORDS_LOOKUP<span class="token punctuation">[</span>w2i<span class="token punctuation">[</span>sent<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                    embdding<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> LSTM<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>output<span class="token punctuation">(</span><span class="token punctuation">)</span>                    score_pcfg<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> math_log<span class="token punctuation">(</span>probs<span class="token punctuation">[</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> word<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                    score<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> score_calc<span class="token punctuation">(</span>d<span class="token punctuation">,</span> W<span class="token punctuation">,</span> embdding<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> b<span class="token punctuation">,</span> lamda<span class="token punctuation">,</span> probs<span class="token punctuation">[</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> word<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                    terminals<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> word                    node_rules<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span>A<span class="token punctuation">)</span>        <span class="token keyword">for</span> span <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> begin <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> n <span class="token operator">-</span> span <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                end <span class="token operator">=</span> begin <span class="token operator">+</span> span                node_rules<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token keyword">for</span> split <span class="token keyword">in</span> range<span class="token punctuation">(</span>begin <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token keyword">for</span> B <span class="token keyword">in</span> node_rules<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> split<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                        <span class="token keyword">for</span> C <span class="token keyword">in</span> node_rules<span class="token punctuation">[</span><span class="token punctuation">(</span>split<span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                            X <span class="token operator">=</span> B<span class="token operator">+</span><span class="token string">" "</span><span class="token operator">+</span>C                            <span class="token keyword">if</span> X <span class="token keyword">in</span> node_pa<span class="token punctuation">:</span>                                <span class="token keyword">for</span> A <span class="token keyword">in</span> node_pa<span class="token punctuation">[</span>X<span class="token punctuation">]</span><span class="token punctuation">:</span>                                    node_rules<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span>A<span class="token punctuation">)</span>                                    TMP <span class="token operator">=</span> LSTM<span class="token punctuation">[</span><span class="token punctuation">(</span>split<span class="token punctuation">,</span> end<span class="token punctuation">,</span> C<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>add_input<span class="token punctuation">(</span>embdding<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> split<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                                    p <span class="token operator">=</span> TMP<span class="token punctuation">.</span>output<span class="token punctuation">(</span><span class="token punctuation">)</span>                                    s_pcfg <span class="token operator">=</span> math_log<span class="token punctuation">(</span>probs<span class="token punctuation">[</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> score_pcfg<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> split<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> score_pcfg<span class="token punctuation">[</span><span class="token punctuation">(</span>split<span class="token punctuation">,</span> end<span class="token punctuation">,</span> C<span class="token punctuation">)</span><span class="token punctuation">]</span>                                    s <span class="token operator">=</span> score_calc<span class="token punctuation">(</span>d<span class="token punctuation">,</span> W<span class="token punctuation">,</span> p<span class="token punctuation">,</span> b<span class="token punctuation">,</span> lamda<span class="token punctuation">,</span> s_pcfg<span class="token punctuation">)</span>                                    <span class="token comment" spellcheck="true"># print (d * (W * p + b) * 100).value(), s_pcfg</span>                                    <span class="token keyword">if</span> <span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token keyword">in</span> score <span class="token operator">or</span> s<span class="token punctuation">.</span>value<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> score<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>value<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                                        LSTM<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> TMP                                        score<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> s                                        score_pcfg<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> s_pcfg                                        embdding<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> p                                        backPointers<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>split<span class="token punctuation">,</span> B<span class="token punctuation">,</span> C<span class="token punctuation">)</span>        t<span class="token punctuation">,</span> s <span class="token operator">=</span> backtrack<span class="token punctuation">(</span>sent<span class="token punctuation">,</span> backPointers<span class="token punctuation">,</span> terminals<span class="token punctuation">,</span> score<span class="token punctuation">)</span>        result <span class="token operator">=</span> None        <span class="token keyword">if</span> t <span class="token operator">!=</span> None<span class="token punctuation">:</span>            result <span class="token operator">=</span> t<span class="token punctuation">.</span>dostr<span class="token punctuation">(</span><span class="token punctuation">)</span>        golds_e<span class="token punctuation">,</span> golds<span class="token punctuation">,</span> golds_pcfg<span class="token punctuation">,</span> lstm<span class="token punctuation">,</span> S <span class="token operator">=</span> cal_gold<span class="token punctuation">(</span>gold<span class="token punctuation">,</span> d<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span>        cnt <span class="token operator">=</span> cal_loss<span class="token punctuation">(</span>result<span class="token punctuation">,</span> gold<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># loss = dy.abs(s - golds) + cnt * k</span>        loss <span class="token operator">=</span> dy<span class="token punctuation">.</span>abs<span class="token punctuation">(</span>s <span class="token operator">-</span> golds<span class="token punctuation">)</span> <span class="token operator">+</span> cnt <span class="token operator">*</span> k <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>dy<span class="token punctuation">.</span>l2_norm<span class="token punctuation">(</span>W<span class="token punctuation">)</span> <span class="token operator">+</span> dy<span class="token punctuation">.</span>l2_norm<span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token operator">+</span> dy<span class="token punctuation">.</span>l2_norm<span class="token punctuation">(</span>d<span class="token punctuation">)</span><span class="token punctuation">)</span>        sumloss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>value<span class="token punctuation">(</span><span class="token punctuation">)</span>        batch<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>batch<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">50</span><span class="token punctuation">:</span>            loss <span class="token operator">=</span> dy<span class="token punctuation">.</span>esum<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            trainer<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span>            dy<span class="token punctuation">.</span>renew_cg<span class="token punctuation">(</span><span class="token punctuation">)</span>            batch <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        eend <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># print "time of sent ", idx, ": ", eend - sstart</span>        <span class="token keyword">if</span> idx <span class="token operator">></span> <span class="token number">0</span> <span class="token operator">and</span> idx <span class="token operator">%</span> <span class="token number">500</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">print</span> <span class="token string">"time of 500 sent: "</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>eend <span class="token operator">-</span> start<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>idx <span class="token operator">/</span> <span class="token number">500</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># print idx, " -------------"</span>            <span class="token comment" spellcheck="true"># print "result: " + result</span>            <span class="token comment" spellcheck="true"># print "gold:   " + gold</span>            <span class="token comment" spellcheck="true"># print "loss: ", loss.value()</span>    end <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    total_time <span class="token operator">+=</span> end <span class="token operator">-</span> start    <span class="token keyword">print</span> <span class="token string">"epoch time: "</span><span class="token punctuation">,</span> end <span class="token operator">-</span> start    <span class="token keyword">print</span> <span class="token string">"epoch loss: "</span><span class="token punctuation">,</span> sumloss <span class="token operator">/</span> num    ff<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'%f\n'</span><span class="token operator">%</span><span class="token punctuation">(</span>sumloss <span class="token operator">/</span> num<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">"total time: "</span><span class="token punctuation">,</span> total_timefh <span class="token operator">=</span> open<span class="token punctuation">(</span>dev_string_file<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span>outfile <span class="token operator">=</span> open<span class="token punctuation">(</span>dev_parse_file<span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span><span class="token keyword">for</span> line <span class="token keyword">in</span> fh<span class="token punctuation">:</span>    sent <span class="token operator">=</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>    origText <span class="token operator">=</span> list<span class="token punctuation">(</span>sent<span class="token punctuation">)</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> word <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>sent<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> word <span class="token operator">not</span> <span class="token keyword">in</span> lexicons<span class="token punctuation">:</span>            sent<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'&lt;unk>'</span>    n <span class="token operator">=</span> len<span class="token punctuation">(</span>sent<span class="token punctuation">)</span>    dy<span class="token punctuation">.</span>renew_cg<span class="token punctuation">(</span><span class="token punctuation">)</span>    d <span class="token operator">=</span> pd<span class="token punctuation">.</span>expr<span class="token punctuation">(</span><span class="token punctuation">)</span>    W <span class="token operator">=</span> pW<span class="token punctuation">.</span>expr<span class="token punctuation">(</span><span class="token punctuation">)</span>    b <span class="token operator">=</span> pb<span class="token punctuation">.</span>expr<span class="token punctuation">(</span><span class="token punctuation">)</span>    terminals <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    embdding <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    score <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>float<span class="token punctuation">)</span>    score_pcfg <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>float<span class="token punctuation">)</span>    backPointers <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    LSTM <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    node_rules <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>        begin <span class="token operator">=</span> i        end <span class="token operator">=</span> i <span class="token operator">+</span> <span class="token number">1</span>        node_rules<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>        word <span class="token operator">=</span> sent<span class="token punctuation">[</span>i<span class="token punctuation">]</span>        <span class="token keyword">for</span> A <span class="token keyword">in</span> nonTerms<span class="token punctuation">:</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>A<span class="token punctuation">,</span> word<span class="token punctuation">)</span> <span class="token keyword">in</span> rules_set1<span class="token punctuation">:</span>                LSTM<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> builder<span class="token punctuation">.</span>initial_state<span class="token punctuation">(</span><span class="token punctuation">)</span>                LSTM<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> LSTM<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>add_input<span class="token punctuation">(</span>WORDS_LOOKUP<span class="token punctuation">[</span>w2i<span class="token punctuation">[</span>sent<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                embdding<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> LSTM<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>output<span class="token punctuation">(</span><span class="token punctuation">)</span>                score_pcfg<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> math_log<span class="token punctuation">(</span>probs<span class="token punctuation">[</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> word<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                score<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> score_calc<span class="token punctuation">(</span>d<span class="token punctuation">,</span> W<span class="token punctuation">,</span> embdding<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> b<span class="token punctuation">,</span> lamda<span class="token punctuation">,</span> probs<span class="token punctuation">[</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> word<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                terminals<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> word                node_rules<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span>A<span class="token punctuation">)</span>    <span class="token keyword">for</span> span <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> begin <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> n <span class="token operator">-</span> span <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            end <span class="token operator">=</span> begin <span class="token operator">+</span> span            node_rules<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">for</span> split <span class="token keyword">in</span> range<span class="token punctuation">(</span>begin <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">for</span> B <span class="token keyword">in</span> node_rules<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> split<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                    <span class="token keyword">for</span> C <span class="token keyword">in</span> node_rules<span class="token punctuation">[</span><span class="token punctuation">(</span>split<span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                        X <span class="token operator">=</span> B<span class="token operator">+</span><span class="token string">" "</span><span class="token operator">+</span>C                        <span class="token keyword">if</span> X <span class="token keyword">in</span> node_pa<span class="token punctuation">:</span>                            <span class="token keyword">for</span> A <span class="token keyword">in</span> node_pa<span class="token punctuation">[</span>X<span class="token punctuation">]</span><span class="token punctuation">:</span>                                node_rules<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span>A<span class="token punctuation">)</span>                                TMP <span class="token operator">=</span> LSTM<span class="token punctuation">[</span><span class="token punctuation">(</span>split<span class="token punctuation">,</span> end<span class="token punctuation">,</span> C<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>add_input<span class="token punctuation">(</span>embdding<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> split<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                                p <span class="token operator">=</span> TMP<span class="token punctuation">.</span>output<span class="token punctuation">(</span><span class="token punctuation">)</span>                                s_pcfg <span class="token operator">=</span> math_log<span class="token punctuation">(</span>probs<span class="token punctuation">[</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> score_pcfg<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> split<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> score_pcfg<span class="token punctuation">[</span><span class="token punctuation">(</span>split<span class="token punctuation">,</span> end<span class="token punctuation">,</span> C<span class="token punctuation">)</span><span class="token punctuation">]</span>                                s <span class="token operator">=</span> score_calc<span class="token punctuation">(</span>d<span class="token punctuation">,</span> W<span class="token punctuation">,</span> p<span class="token punctuation">,</span> b<span class="token punctuation">,</span> lamda<span class="token punctuation">,</span> s_pcfg<span class="token punctuation">)</span>                                <span class="token keyword">if</span> <span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token keyword">in</span> score <span class="token operator">or</span> s<span class="token punctuation">.</span>value<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> score<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>value<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                                    LSTM<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> TMP                                    score<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> s                                    score_pcfg<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> s_pcfg                                    embdding<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> p                                    backPointers<span class="token punctuation">[</span><span class="token punctuation">(</span>begin<span class="token punctuation">,</span> end<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>split<span class="token punctuation">,</span> B<span class="token punctuation">,</span> C<span class="token punctuation">)</span>    t<span class="token punctuation">,</span> s <span class="token operator">=</span> backtrack<span class="token punctuation">(</span>sent<span class="token punctuation">,</span> backPointers<span class="token punctuation">,</span> terminals<span class="token punctuation">,</span> score<span class="token punctuation">)</span>    <span class="token keyword">if</span> t <span class="token operator">==</span> None<span class="token punctuation">:</span>        outfile<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">"None\n"</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        result <span class="token operator">=</span> t<span class="token punctuation">.</span>dostr<span class="token punctuation">(</span><span class="token punctuation">)</span>        outfile<span class="token punctuation">.</span>write<span class="token punctuation">(</span>result<span class="token operator">+</span><span class="token string">"\n"</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第3课（递归式转化为求和求解）</title>
      <link href="/2018/03/12/concrete-math-3/"/>
      <url>/2018/03/12/concrete-math-3/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=539941039&auto=1&height=66"></iframe></div><p>今天讲了一种将递归式转化为求和的方法。</p><p>考虑如下递归式：<br>\[{a_n}{T_n} = {b_n}{T_{n - 1}} + {c_n}\]<br>两边同时乘以$s_n$得到：<br>\[{s_n}{a_n}{T_n} = {s_n}{b_n}{T_{n - 1}} + {s_n}{c_n}\]<br>要想转化成可以求和的递归式，那么必须有：<br>\[{s_n}{b_n} = {s_{n - 1}}{a_{n - 1}}\]<br>也就是：<br>\[{s_n} = \frac { { {a_{n - 1}}{a_{n - 2}} \cdots {a_1}}}{ { {b_n}{b_{n - 1}} \cdots {b_2}}}\]<br>这时令<br>\[{S_n} = {s_n}{a_n}{T_n}\]<br>得到：<br>\[{S_n} = {S_{n - 1}} + {s_n}{c_n}\]<br>这时就可以转化为求和了，解出：<br>\[{S_n} = {s_0}{a_0}{T_0} + \sum\limits_{k = 1}^n { {s_k}{c_k}} \]<br>所以<br>\[{T_n} = \frac{1}{ { {s_n}{a_n}}}({s_0}{a_0}{T_0} + \sum\limits_{k = 1}^n { {s_k}{c_k}} )\]</p><h1 id="例题1"><a href="#例题1" class="headerlink" title="例题1"></a>例题1</h1><hr><p>设$n$个数快速排序的操作次数为$C_n$，那么有<br>\[\begin{array}{l}{C_0} = 0\\{C_n} = n + 1 + \frac{2}{n}\sum\limits_{k = 0}^{n - 1} { {C_k}} ,n &gt; 0\end{array}\]<br>用$n-1$取代$n$可以得到<br>\[{C_{n - 1}} = n + \frac{2}{ {n - 1}}\sum\limits_{k = 0}^{n - 2} { {C_k}} ,n &gt; 1\]<br>两式相减可以得到<br>\[\begin{array}{l}{C_0} = 0\\n{C_n} = (n + 1){C_{n - 1}} + 2n,n &gt; 0\end{array}\]<br>由上面方法可以得到<br>\[{a_n} = n,{b_n} = n + 1,{c_n} = 2n\]<br>所以<br>\[{s_n} = \frac{2}{ {n(n + 1)}}\]<br>进而可以求出<br>\[{C_n} = 2(n + 1)\sum\limits_{k = 1}^n {\frac{1}{ {k + 1}}} \]<br>这里介绍一个概念叫做调和级数：<br>\[{H_n} = 1 + \frac{1}{2} +  \cdots  + \frac{1}{n} = \sum\limits_{k = 1}^n {\frac{1}{k}} \]<br>所以<br>\[{C_n} = 2(n + 1){H_n} - 2n\]</p><h1 id="求和三大定律"><a href="#求和三大定律" class="headerlink" title="求和三大定律"></a>求和三大定律</h1><hr><p>结合律、分配率、交换律。这里就不展开说了，相信你们都知道的。<br>来两题简单的例题说明一下。</p><h1 id="例题2"><a href="#例题2" class="headerlink" title="例题2"></a>例题2</h1><hr><p>求<br>\[S = \sum\limits_{0 \le k \le n} {(a + bk)} \]<br>普通的方法每个人应该都会，等差数列嘛。这里用求和定律来做一做。<br>用$n-k$取代$k$，得到<br>\[S = \sum\limits_{0 \le n - k \le n} {(a + b(n - k))} \]<br>即<br>\[S = \sum\limits_{0 \le k \le n} {(a + b(n - k))} \]<br>两式相加得到<br>\[2S = \sum\limits_{0 \le k \le n} {(2a + bn)}  = (2a + bn)\sum\limits_{0 \le k \le n} 1  = (2a + bn)(n + 1)\]<br>所以<br>\[S = (2a + bn)(n + 1)/2\]</p><h1 id="例题3"><a href="#例题3" class="headerlink" title="例题3"></a>例题3</h1><hr><p>求<br>\[S = \sum\limits_{0 \le k \le n} {k{x^k}} \]<br>这里用到另一种求和的方法。<br>两边同时加上第$n+1$项，得到<br>\[\begin{array}{l}S + (n + 1){x^{n + 1}}\\ = \sum\limits_{0 \le k \le n + 1} {k{x^k}} \\ = \sum\limits_{1 \le k \le n + 1} {k{x^k}} \\ = \sum\limits_{0 \le k \le n} {(k + 1){x^{k + 1}}} \\ = x\sum\limits_{0 \le k \le n} {(k{x^k} + {x^k})} \\ = xS + x\sum\limits_{0 \le k \le n} { {x^k}} \\ = xS + x\frac{ {1 - {x^{n + 1}}}}{ {1 - x}}\end{array}\]<br>所以<br>\[S = \frac{ {x - (n + 1){x^{n + 1}} + n{x^{n + 2}}}}{ { { {(1 - x)}^2}}}\]<br>这里介绍另一种方法来求解。<br>令<br>\[f(x) = \sum\limits_{0 \le k \le n} { {x^k}}  = \frac{ {1 - {x^{n + 1}}}}{ {1 - x}}\]<br>求导得到<br>\[f’(x) = \sum\limits_{0 \le k \le n} {k{x^{k - 1}}}  = \frac{1}{x}S\]<br>所以<br>\[\frac{1}{x}S = \frac{ {\partial f}}{ {\partial x}}(\frac{ {1 - {x^{n + 1}}}}{ {1 - x}}) = \frac{ {1 - (n + 1){x^n} + n{x^{n + 1}}}}{ { { {(1 - x)}^2}}}\]<br>同样可以得到<br>\[S = \frac{ {x - (n + 1){x^{n + 1}} + n{x^{n + 2}}}}{ { { {(1 - x)}^2}}}\]</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu16.04下Nvidia+Cuda8.0+Dynet安装教程</title>
      <link href="/2018/03/09/nvidia-cuda-dynet/"/>
      <url>/2018/03/09/nvidia-cuda-dynet/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=28936057&auto=1&height=66"></iframe></div><p>之前也在笔记本上装过几次cuda，均以失败告终，网上的教程都没有完全能拿来用的，多多少少都会出现一些问题。<br>这次终于完完全全安装成功了，可喜可贺。。。说起来都是泪。</p><p>注意显卡驱动安装最新版就行了，但是cuda最好还是别安装最新版了，装个8.0版本吧，不然都是泪。</p><p>最终版本为ubuntu16.04 + cuda8.0 + gcc5.4。</p><h1 id="NVIDIA驱动安装"><a href="#NVIDIA驱动安装" class="headerlink" title="NVIDIA驱动安装"></a>NVIDIA驱动安装</h1><hr><p>进入<a href="http://www.nvidia.com/Download/index.aspx?lang=en-us" target="_blank" rel="noopener">NVIDIA官网</a>，选择适合自己显卡的驱动，下载后是一个.run文件。</p><p>清除之前安装过的NVIDIA：<code>sudo apt-get remove --purge nvidia*</code></p><p>禁止nouveau等驱动：<code>sudo gedit /etc/modprob.d/blacklist.conf</code></p><p>加入下列语句并保存：</p><pre><code>blacklist vga16fbblacklist nouveaublacklist rivafbblacklist nvidiafbblacklist rivatv</code></pre><p>执行<code>sudo update-initramfs -u</code>并重启<code>reboot</code>。</p><p>按<code>ctrl+alt+f1</code>，登录命令行界面。</p><p>执行<code>sudo service lightdm stop</code></p><p>进入NVIDIA.run目录，运行<code>sudo sh ./NVIDIA.run –no-x-check –no-nouveau-check –no-opengl-files</code></p><p>安装过程中会报错，直接无视。会问你要不要自动更新X配置文件，选择是就行了。</p><p>重启。输入<code>sudo nvidia-smi</code>或者<code>nvidia-settings</code>，如果显示显卡信息，那么恭喜你安装成功了。</p><h1 id="CUDA安装"><a href="#CUDA安装" class="headerlink" title="CUDA安装"></a>CUDA安装</h1><hr><p>进入<a href="https://developer.nvidia.com/cuda-toolkit" target="_blank" rel="noopener">cuda官网</a>，根据自己版本下载cuda.run文件。</p><p>执行<code>sudo sh cuda.run</code>，注意中间问你要不要安装驱动程序，选择no</p><p>执行<code>sudo gedit /etc/profile</code><br>添加下列语句并保存：</p><pre><code>export PATH=/usr/local/cuda-8.0/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64$LD_LIBRARY_PATH</code></pre><p>重启，打开<code>/NVIDIA_CUDA-8.0_Samples/1_Utilities/deviceQuery</code></p><p>执行<code>sudo make</code>和<code>./deviceQuery</code>，如果出现显卡信息，那么cuda安装成功了。</p><h1 id="Dynet安装"><a href="#Dynet安装" class="headerlink" title="Dynet安装"></a>Dynet安装</h1><hr><p>首先安装Anaconda，过程就不说了，直接运行shell脚本就行了。</p><p>然后重要的地方来了，创建虚拟环境，在虚拟环境里安装dynet！！！</p><p>执行</p><pre><code>conda create --name python2 python=2 cython numpysource activate python2</code></pre><p>然后CPU版本的话很简单，直接执行<code>pip install dynet</code>就行了。</p><p>GPU版本执行<code>BACKEND=cuda pip install git+https://github.com/clab/dynet#egg=dynet -i https://pypi.tuna.tsinghua.edu.cn/simple</code>。</p><p>然后就成功啦，但是运行dynet程序的时候还是会报错，找不到libcudart库，这时执行下面三条语句就行了：</p><pre><code>sudo cp /usr/local/cuda-8.0/lib64/libcudart.so.8.0 /usr/local/lib/libcudart.so.8.0 &amp;&amp; sudo ldconfigsudo cp /usr/local/cuda-8.0/lib64/libcublas.so.8.0 /usr/local/lib/libcublas.so.8.0 &amp;&amp; sudo ldconfigsudo cp /usr/local/cuda-8.0/lib64/libcurand.so.8.0 /usr/local/lib/libcurand.so.8.0 &amp;&amp; sudo ldconfig</code></pre><p>然后终于可以运行了，感动哭了。。。</p>]]></content>
      
      
      <categories>
          
          <category> 软件安装与配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dynet </tag>
            
            <tag> Cuda </tag>
            
            <tag> Ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第一章作业解答</title>
      <link href="/2018/03/05/concrete-math-hw1/"/>
      <url>/2018/03/05/concrete-math-hw1/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=371362&auto=1&height=66"></iframe></div><p>这是我的第一章作业答案，只有四题，标准答案太精简了。</p><p>总体做下来，只有最后一题的第二小问一开始算错了，正如题目所说，的确有点难想。我看了标准答案的式子，想了一会儿才想出来的。。。</p><p><img src="1.jpg" alt><br><img src="2.jpg" alt><br><img src="3.jpg" alt><br><img src="4.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第2课（成套方法求解递归式）</title>
      <link href="/2018/03/05/concrete-math-2/"/>
      <url>/2018/03/05/concrete-math-2/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=316275&auto=1&height=66"></iframe></div><p>今天主要讲了关于递推式和求和的一些方法，主要是成套方法。</p><h1 id="约瑟夫环推广"><a href="#约瑟夫环推广" class="headerlink" title="约瑟夫环推广"></a>约瑟夫环推广</h1><p>上一节课说到，约瑟夫环问题的解是<br>\[f(n) = 2l + 1\]<br>其中$n = {2^m} + l$<br>将$n$写成二进制可以发现，$f(n)$就是$n$的二进制循环左移1位。<br>现在做一下推广，求解如下递推式：<br>\[\begin{array}{l}f(1) = \alpha \\f(2n) = 2f(n) + \beta \\f(2n + 1) = 2f(n) + \gamma \end{array}\]<br>可以设<br>\[f(n) = A(n)\alpha  + B(n)\beta  + C(n)\gamma \]<br>同样，令$n = {2^m} + l$<br>可以解出<br>\[\begin{array}{l}A(n) = {2^m}\\B(n) = {2^m} - 1 - l\\C(n) = l\end{array}\]<br>再从二进制角度理解一下，将递推式继续推广：<br>\[\begin{array}{l}f(j) = {\alpha _j},1 \le j &lt; d\\f(dn + j) = cf(n) + {\beta _j},0 \le j \le d,n \ge 1\end{array}\]<br>可以得到解为<br>\[f({({b_m}{b_{m - 1}} \ldots {b_1}{b_0})_d}) = {({\alpha _{ {b_m}}}{\beta _{ {b_{m - 1}}}}{\beta _{ {b_{m - 2}}}} \ldots {\beta _{ {b_1}}}{\beta _{ {b_0}}})_c}\]</p><h1 id="递推式求和"><a href="#递推式求和" class="headerlink" title="递推式求和"></a>递推式求和</h1><p>求解如下递推式：<br>\[\begin{array}{l}{R_0} = \alpha \\{R_n} = {R_{n - 1}} + \beta n + \gamma \end{array}\]<br>用成套方法求解，设<br>\[{R_n} = A(n)\alpha  + B(n)\beta  + C(n)\gamma \]<br>首先令${R_n} = 1$，可以得到$\alpha  = 1,\beta  = 0,\gamma  = 0$，所以$A(n) = 1$。<br>再令${R_n} = n$，可以得到$\alpha  = 0,\beta  = 0,\gamma  = 1$，所以$C(n) = n$。<br>最后令${R_n} = {n^2}$，可以得到$\alpha  = 0,\beta  = 2,\gamma  =  - 1$，所以$2B(n) - C(n) = {n^2}$，所以$B(n) = ({n^2} + n)/2$</p><p>再来一个更复杂的递推式：<br>\[\begin{array}{l}{R_0} = \alpha \\{R_n} = 2{R_{n - 1}} + \beta n + \gamma \end{array}\]<br>同样的方法，设<br>\[{R_n} = A(n)\alpha  + B(n)\beta  + C(n)\gamma \]<br>首先令${R_n} = 1$，可以得到$\alpha  = 1,\beta  = 0,\gamma  = -1$，所以$A(n) - C(n) = 1$。<br>再令${R_n} = n$，可以得到$\alpha  = 0,\beta  = -1,\gamma  = 2$，所以$2C(n) - B(n) = n$。<br>这时候能不能令${R_n} = {n^2}$呢？答案是不能，因为如果${R_n} = {n^2}$，那么<br>\[{n^2} = 2{(n - 1)^2} + \beta n + \gamma \]显然不可能成立。<br>观察系数，可以令${R_n} = 2^n$，可以得到$\alpha  = 1,\beta  = 0,\gamma  = 0$，所以$A(n) = 2^n$。<br>所以<br>\[A(n) = {2^n},B(n) = {2^{n + 1}} - n + 2,C(n) = {2^n} + 1\]</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>具体数学-第1课（递归求解实际问题）</title>
      <link href="/2018/02/27/concrete-math-1/"/>
      <url>/2018/02/27/concrete-math-1/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=496870798&auto=1&height=66"></iframe></div><p>这学期提前选修了研究生的课程：具体数学、人工智能前沿、NLP讨论班，就随便记记具体数学每一节课所学的东西吧。</p><p>第一节课讲的都是一些很简单的东西，这里就一带而过了。</p><h1 id="汉诺塔问题"><a href="#汉诺塔问题" class="headerlink" title="汉诺塔问题"></a>汉诺塔问题</h1><hr><p>这是个老生常谈的问题了，n个盘子，3个柱子的汉诺塔问题，最少移动次数记为$T(n)$。<br>那么\[T(n)=2T(n-1)+1\]<br>边界条件为$T(0)=0$。<br>解出\[T(n)=2^n-1\]<br>验证可以采用数学归纳法，这里就不多说了。</p><h1 id="直线分割平面问题"><a href="#直线分割平面问题" class="headerlink" title="直线分割平面问题"></a>直线分割平面问题</h1><hr><p>这也是个高中问题了，n条直线最多分割平面为几部分，记为$L(n)$。<br>那么\[L(n)=L(n-1)+n\]<br>边界条件为$L(0)=1$。<br>解出\[L(n)=n(n+1)/2+1\]</p><p>这题有个扩展，n个V型最多分割平面为几部分？<br>解决思路如下：<br><img src="1.jpg" alt><br>如上图所示，将V型补全（红色虚线部分），那么就转化为了$2n$条直线划分平面数，那么n个V型划分数只要减去$2n$就行了，所以答案为：<br>\[Z(n)=L(2n)-2n=2n^2-n+1\]</p><h1 id="约瑟夫环问题"><a href="#约瑟夫环问题" class="headerlink" title="约瑟夫环问题"></a>约瑟夫环问题</h1><hr><p>这个问题暴力求解的话模拟就行了，复杂度是$O(n^2)$的，这里探索一种直接求解的方法。<br>分两种情况讨论：<br>当有$2n$个人时，踢掉$n$个人之后，情况如下图所示<br><img src="2.jpg" alt><br>观察对应关系可以得出<br>\[J(2n)=2J(n)-1\]<br>同理，当有$2n+1$个人时，踢掉$n+1$个人之后，情况如下图所示<br><img src="3.jpg" alt><br>观察对应关系可以得出<br>\[J(2n+1)=2J(n)+1\]<br>边界条件为<br>\[J(1)=1\]<br>这个递推式很难求解，但是枚举出前面几项可以发现，如果令$n=2^m+l$，其中$2^m$是小于等于$n$的最大2的幂，那么<br>\[J(n)=2l+1\]<br>正确性可以通过数学归纳法求证。</p><p>第一节课就讲了这么多，约瑟夫环还有很多问题值得探讨，下节课继续。。。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 具体数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>毕业论文代码实现思路</title>
      <link href="/2018/02/26/biyelunwen/"/>
      <url>/2018/02/26/biyelunwen/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=526307800&auto=1&height=66"></iframe></div><p>一个寒假就写了个基本的代码，难受啊，整理一下思路吧，好久不看代码头都大了。</p><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><hr><p>首先使用的是PTB数据集，原始的数据是长这样的：<br>(S (NP-SBJ (NNP Ms.) (NNP Haag) ) (VP (VBZ plays) (NP (NNP Elianti) )) (. .) )</p><p>因为不一定是二叉树，所以要先预处理成二叉树，这里全部借用了github上别人的代码来进行预处理，二叉化之后变成了这样：<br>(S (S*^. (NP-SBJ (NNP Ms.) (NNP Haag)) (VP (VBZ plays) (NP_NNP Elianti))) (. .))</p><p>然后将整个数据集中出现次数过小的单词替换为unk：<br>(S (S*^. (NP-SBJ (NNP Ms.) (NNP Haag)) (VP (VBZ plays) (NP_NNP &lt; unk&gt;))) (. .))</p><p>最后还需要将每个训练数据对应的句子单独提取出来，方便训练时用，比如上面的例子提取出来就是：<br>Ms. Haag plays Elianti . </p><h1 id="文法规则提取"><a href="#文法规则提取" class="headerlink" title="文法规则提取"></a>文法规则提取</h1><hr><p>接着还是利用现成代码将数据集中出现的所有文法生成规则提取出来，保存到文件pcfg中。</p><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><hr><p>这部分大部分代码是自己写的，也有一部分是从CKY算法代码修改得到的。</p><p>首先将数据集中出现的所有单词和词向量数组下标一一映射，词性类别也做一个映射吧。</p><p>然后初始化神经网络的各个参数，要训练的权值矩阵一共有两个，$W$和$d$，接下来就是训练了。</p><p>训练过程是这样的，采用了动态规划的思想，用三个维度$i$，$j$，$A$来表示这个句子从第$i$个位置到第$j$个位置且类别为$A$的信息。<br>用$e(i,j,A)$作为每个结点的向量表示，然后对他的儿子的所有情况进行遍历：<br>\[e(i,j,A) = \tanh (W \cdot [e(i,k,B),e(k,j,C)]) + type[A]\]<br>然后计算这个节点的分数：<br>\[s(i,j,A) = d \cdot e(i,j,A)\]<br>找出能使得分数最大的那个分割点和对应的类别，保存下来。</p><p>这样动态规划做好之后一棵树实际上就已经建好了，再回溯生成这棵树就好了。</p><p>但是这棵树很不准确的，刚开始就是随意生成的，所以要和标准树进行对比，计算出损失函数进行反向传播。</p><p>所以接下来用当前的权值矩阵计算出标准树的分数，然后对刚刚生成的结果和这个标准括号序列进行对比，我就直接粗暴统计出两个字符串有多少位置不同，记为$cnt$。</p><p>总的损失函数就是：<br>\[\left| {result - gold + cnt} \right|\]<br>然后进行反向传播就行了。</p><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><hr><p>在测试集上面直接照搬训练过程代码就行了，按照动态规划生成一棵树就行了。</p><p>然后用现成的代码和标准结果进行对比，得出F1值。</p><h1 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h1><hr><p>总的来说，大体就是这样的了，但是还有很多问题没有解决。</p><ul><li>动态规划结合一个节点的两个子节点的时候，现在还只是直接连接的，准备将其改成LSTM的结点函数。</li><li>改成LSTM的话就有左右结点的顺序问题，准备再加一个维度，0和1分别表示左右儿子的顺序。</li><li>我在$e(i,j,A) = \tanh (W \cdot [e(i,k,B),e(k,j,C)]) + type[A]$加了一个$type[A]$，其实原来没有这个的，但是不加会出现一个很大的问题，就是会出现A1-&gt;B,C和A2-&gt;B,C这两种情况，但是不加的话两种情况算出的分数是一样的，先入为主，后算的那种就永远不会考虑了。所以我强行加上了父节点类型向量，来区别这两种情况。但是具体怎么加还没有个说法，我只是随便试试。</li><li>关于损失函数，论文里写的就是$result - gold + cnt$，但是这样并不能保证非负，我就强行加上了一个绝对值。貌似也可以收敛了，到底应该怎么搞不清楚。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二零一七年终总结</title>
      <link href="/2018/01/22/2017-conclusion/"/>
      <url>/2018/01/22/2017-conclusion/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=407679465&auto=1&height=66"></iframe></div><p>时间过得很快，终于等到放寒假了，虽然这几个月没有课，天天和放假也没啥区别呢。细数一下，还有5个月就要毕业了吧，大一刚入学的场景却依然清楚地记得，转眼间就成了老学长了呢。闲来无事，随便写写，有感而发，无病呻吟而已。</p><h1 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h1><hr><p>回顾我的2017，没做什么事，令我能记得就3件大事吧：</p><ul><li>失恋ing</li><li>ACM退役</li><li>顺利保研</li></ul><p><strong>第一件事</strong>就不想过多回忆了，<strong>2014.12.13 ~ 2017.03.01</strong>，曲终人散。<br><img src="749826.jpg" alt><br>最后引用《我的少女时代》里的一句话吧。</p><blockquote><p>每人都有一颗林真心，遇见是最美好的小幸运，谢谢你出现在我的青春里。</p></blockquote><p><img src="1.jpg" alt><br><strong>第二件事</strong>其实也是黯淡退出吧，大三下开始课程繁忙，也就没怎么训练了，再加上暑假考驾照，于是乎就退役了。回顾三年来，从大一入学时电脑都没怎么碰过的小白，到现在算法也略有所知，也是付出过很大的努力吧，毕竟当年每天刷题，为了一个bug而熬夜到凌晨。最后也算是混了个水水的金牌，奖项不算耀眼。但最重要的是从这段经历中，学到了拼搏、坚持的一种精神，这对以后的研究生涯想必也有很大帮助。<br><img src="2.jpg" alt><br><strong>第三件事</strong>也是意料之中吧，没有什么波折。纠结了很多，虽然<strong>专业第一</strong>，但是最后还是选择保了本校。要问原因，也许是校园情怀，也许是导师人很好，也许是为了方便更早研究，也许就是懒吧。现在尘埃落定，靠人靠天不如靠自己，继续努力吧。<br><img src="3.jpg" alt></p><p>过去的一年，学业未有很大长进，看着同学们整页的4.0绩点，心里倒也没有什么不平衡了。下学期保了研之后选了一门研究生的文本挖掘课，也马马虎虎读了几十篇论文，也算是对自然语言处理和深度学习入了个门，最后的presentation做的还算满意。</p><h1 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h1><hr><p>今年最重要的大事莫过于毕业论文了，因为以后要做的方向是句法分析，所以导师给我的毕业论文安排的就是《基于循环神经网络的成分句法分析》。虽然说是基于ACL2013的一篇论文改编的，但是目前为止，我还没有发现有人做和这个完全一样的。也许最后写的好的话可以直接发paper了。</p><p>但是目前基本的框架还没完全搭建起来吧，代码还不是很熟练，现在只写了一个最基础的动态规划+RNN。最近有如下计划：</p><ul><li>准备试一下动态规划+LSTM。</li><li>然后动态规划扩增一个维度，用来保存左右结点的head结点。</li><li>如果这个写好了，就可以和我github找到的PCFG+CYK代码融合了，准备加上每个结点的POS。</li><li>最后加入预训练词向量应该就基本完成了。</li></ul><p>希望能顺利毕业吧，前一段时间一直对一些实现细节有些困惑，代码还写错了，还以为理论错了。也不知道最后出来的结果会怎么样，希望能不错。</p><p>生活方面，最近半年越来越懒了，极少出门，睡得晚，起的也晚。最近买了把尤克里里，也算是陶冶陶冶情操吧，不至于一直盯着电脑。现在也小有长进，能弹一点点了。</p><p>现在能聊天的人越来越少了，QQ微信放那一天也不一定会有人来找，就算有人也多半是咨询问题的，等一个可以交心的人吧。有时我也想过，我是不是太像中央空调了，对所有人都这么有耐心，到头来却还是一个人，付出那么多最后还是一无所有。<br><img src="4.jpg" alt><br>最后还是祝自己2018年顺利吧，希望毕业顺利，研究生涯小有收获，最后等一个有缘人吧。</p><blockquote><p>我遇见谁，会有怎样的对白。<br>我等的人，她在多远的未来。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Top-down Tree Long Short-Term Memory Networks</title>
      <link href="/2018/01/15/paperdaily-4/"/>
      <url>/2018/01/15/paperdaily-4/</url>
      
        <content type="html"><![CDATA[<p>昨天又鸽了一天，由于水平有限，最主要还是懒，一篇paper看了两天才看了个大概。最近很颓废啊，白天啥都不想干一天就这么过去了，明天开始还是写写毕设代码吧，再好好研究研究。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>这次介绍的仍然是树状LSTM，但是这次是在依存句法树上做的LSTM。主要功能就是给定一个句子的依存句法树，预测这个句子的生成概率。实验主要是在Microsoft Sentence Completion Challenge上面进行的，取得了不错的效果。不仅如此，这个模型还可以对依存句法分析产生的依存句法树进行重排序，从而提升依存句法分析的效果。（PS. 又让我联想到了我的毕业论文，用SU-RNN对PCFG产生的成分句法树进行重排序。。。。。。都是泪，代码还没开始动。）</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><hr><p>首先介绍几个概念。</p><h3 id="依存路径"><a href="#依存路径" class="headerlink" title="依存路径"></a>依存路径</h3><p><img src="1.png" alt><br>如上图所示，虚线箭头就是依存树中的箭头，其中$w_0$就是$w_1$到$w_n$的head结点。那么$w_1$就是$w_0$左边的第一个结点，边$(w_0,w_1)$类型叫做$LEFT$边，而继续向左，例如边$(w_{k-1},w_k)$类型叫做$NX-LEFT$边。同理，向右的边也有两种类型$RIGHT$和$NX-RIGHT$。</p><p>那么依存路径$\mathcal D(w)$定义为从$ROOT$结点到$w$结点的路径，注意不是原来依存树的路径哦。具体计算方式如下：<br><img src="2.png" alt><br>以上图为例，原来$w_0$到$w_n$的路径为${w_0} \to {w_n}$，而现在变成了${w_0} \to {w_1} \to {w_2} \to  \ldots  \to {w_n}$。</p><p>那么给定依存树$T$，句子$S$的概率可以表示为<br><img src="3.png" alt><br>由于每个句子都有$ROOT$，所以就不需要计算它的概率了。$w$按照树$T$的宽度优先搜索顺序访问。</p><h3 id="树状LSTMs"><a href="#树状LSTMs" class="headerlink" title="树状LSTMs"></a>树状LSTMs</h3><p>那么问题就是如何计算$P(w|\mathcal D(w))$了。我们定义4种LSTM：GEN-L,GEN-R,GEN-NX-L,GEN-NX-R，分别用来表示上文中提到的四种类型的边：LEFT,RIGHT,NX-LEFT,NX-RIGHT。</p><p>每个结点的表示如下计算：</p><p><img src="4.png" alt><br>概率表示为：<br><img src="5.png" alt></p><p>注意这里为了简化计算，省略了全部的偏移向量。</p><p>这里用了深层LSTM的内部结点函数，具体直接看公式吧，有点晕。。。<br><img src="6.png" alt><br><img src="7.png" alt><br>直接附上原文解释：<br><img src="8.png" alt><br><img src="9.png" alt></p><h3 id="左依赖树状LSTMs"><a href="#左依赖树状LSTMs" class="headerlink" title="左依赖树状LSTMs"></a>左依赖树状LSTMs</h3><p>上面的方法忽略了同一个结点向左向右依赖之间的联系，举个例子，The car factory sold cars，如果只根据向右的依赖，由sold是无法推出cars的，而加上左依赖The car factory之后就能推出了，所以就提出了这种改进。结构如下：<br><img src="10.png" alt></p><p>也就是计算向右依赖的第一个结点之前，先计算完向左依赖的所有结点（上图绿色箭头部分），然后将最后一个隐含层输出作为向右依赖的第一个结点的输入。<br>首先是左边依赖的表示计算，注意和之前的向左计算方向是反的：<br><img src="11.png" alt><br>然后是向右依赖的计算：<br><img src="12.png" alt></p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>定义两种损失函数，分别对应小规模数据和大规模数据。<br><img src="14.png" alt><br><img src="15.png" alt></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><p>我就只关注了这个模型的附属品————句法分析上的性能。<br><img src="13.png" alt><br>看起来左依赖树状LSTMs相比树状LSTM基本没有提升，可能在其他任务上不一样吧。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>这个模型看了我两天，感觉以前没见过，还挺新奇的（事实是我孤陋寡闻了）。而且我也不知道搞这么复杂究竟能有多大的性能提升，感觉上训练时间会很长？性价比不是很高？</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> NAACL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Head-Lexicalized Bidirectional Tree LSTMs</title>
      <link href="/2018/01/13/paperdaily-3/"/>
      <url>/2018/01/13/paperdaily-3/</url>
      
        <content type="html"><![CDATA[<p>首先给大家说明一下，前两天因为新入手了一个ukulele（就是下图这玩意），所以痴迷于学习弹奏，没有更新博客。照这个节奏下去，PaperDaily恐怕是要变成PaperWeekly了。（囧。。。）寒假一定要学会《小幸运》，嗯。<br><img src="ukulele.jpg" alt></p><p>好了，今天开始恢复吧（说不准过两天我又鸽了，嘻嘻嘻）。</p><p>今天要讲的这篇是TACL2017的，是关于树状LSTM的。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>前两年已经有人提出了树状LSTM的概念，之前一直不知道是个啥高大上的结构。其实就是递归神经网络中的结点单元替换成LSTM的结点单元。那有人要问了，LSTM不是有$x$输入吗，还有一个$h_{t-1}$输入，那这树状的哪来这两个输入？其实很简单，只要把这两个输入替换成左右儿子的输出表示就行了。瞬间感觉也没啥意思，就是换了结点函数，使得树状LSTM具有了遗忘功能，从而能够处理很长的句子罢了。</p><p>今天介绍的这篇paper就是在这基础上做了两点改进。</p><ul><li>除了左右儿子作为输入之外，还增加了$x$输入。$x$是啥呢？就是左右儿子中的头结点，头结点的话传统方法是根据规则来判断谁是头结点的，这里省去了这些复杂的步骤，直接将头结点的判断丢进神经网络中训练。</li><li>增加了反向的树状LSTM，也就是top-down的LSTM。那有人就很好奇了，一个结点分解成两个结点，怎么可能？其实从头结点到任意一个其他结点的路径都可以看成一个独立的LSTM，如果向左参数就是$U_L$，否则就是$U_R$。</li></ul><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><hr><p><img src="1.png" alt><br>图1是序列LSTM和树状LSTM的结构区别，这个树状LSTM是之前传统的树状LSTM，不带head结点的。</p><p>之前的基本的树状LSTM的结点单元的具体公式如下：<br><img src="5.png" alt><br>具体我就不解释了（懒。。。），自行类比序列LSTM。</p><p>加入head结点之后，公式区别如下（加粗所示）：<br><img src="2.png" alt><br><img src="3.png" alt><br><img src="4.png" alt></p><p>那么top-down是怎么做的呢？<br><img src="6.png" alt><br>这里可以看出来，向左向右是用的两套不同的参数。注意到，top-down方向的LSTM前提是一定要用head机制作为支撑！不然头结点的$x$算不出来的话是没有办法计算的哦。</p><p>具体训练过程等等就不再阐述了。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><p>这个模型主要是用在了文本分类和情感分类上，我在想能不能用在我的毕设上面。</p><p>我觉得head机制可以加进去，但是反向LSTM貌似是不可行的，因为这里的短语结构树全部是给定的，所以向下计算知道什么时候停止。但是我是做句法分析任务的，没有给定句法树，向下计算无法知道什么时候停止扩展，情况有无数种！向上计算倒是无所谓，最多卡特兰数级别，加上动态规划，可以缩小到$n\log n$级别。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> TACL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Parsing with Compositional Vector Grammars</title>
      <link href="/2018/01/10/paperdaily-2/"/>
      <url>/2018/01/10/paperdaily-2/</url>
      
        <content type="html"><![CDATA[<p>今天也没看新的paper，就讲讲我的毕设的paper吧，估计等我文本挖掘这门课上完，也不会再看太多序列标注相关的了，重点要转移到parsing了。毕竟序列标注效果也已经很好了，迁移学习方面也暂时不想弄，以后研究重点还是可能在parsing吧。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>这篇paper名字叫做基于成分向量文法的句法分析，那么这是个什么东西呢？大家都知道（也许不知道？我就默认知道了( ╯□╰ )）概率上下文无关文法（PCFG）吧，这是基于传统方法的短语结构句法分析，也叫成分句法分析，还有一种叫做依存句法分析，现在大多数是做这个的。但是传统的成分句法分析无法解决歧义的问题，因为PCFG是基于上下文无关的独立性假设的，但是自然语言是一种上下文有关文法，必然会产生歧义。</p><p>那么如何消除这种歧义呢，Socher提出了SU-RNN的模型，引入了短语的语义表示，具体是什么样子的呢？</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><hr><p>这个模型概括起来是这样的：首先用PCFG产生k-best句法树，比如产生概率最大的20棵句法树。然后对这k-best棵句法树每棵树都跑一遍SU-RNN，计算出每棵树的得分，然后综合PCFG得分和SU-RNN得分，对他们进行重排序，然后得到排名最高的句法树。</p><p>那么怎么通过SU-RNN计算一棵树的得分呢？</p><p>首先上一张图，看看SU-RNN是个什么结构：<br><img src="1.png" alt><br>可以看出，每个节点不仅含有它的类别表示，还有一个向量表示它的语义信息。而SU-RNN与之前提出过的RNN不一样的是，这里的每个节点的$W$权值矩阵全部是不同的，依赖于它的子节点的类别。<br>每个节点的语义表示向量计算方法如下：<br>\[{p^{(1)}} = f\left( { {W^{(B,C)}}\left[ {\begin{array}{*{20}{c}}b\\c\end{array}} \right]} \right)\]而这个节点的得分表示为<br>\[s({p^{(1)}}) = {({v^{(B,C)}})^T}{p^{(1)}} + \log P({P_1} \to B{\rm{ }}C)\]最后一整棵树的得分就是<br>\[s(CVG(\theta ,x,\hat y)) = \sum\limits_{d \in N(\hat y)} {s({p^d})} \]这样就可以枚举所有的句法树，然后计算得到得分最高的那棵树就是最终的句法树了。</p><p>但是这样枚举的话复杂度太高了，要知道一个长度为$n$的句子，可能的句法树有$Catalan(n)$种。而且是无法用动态规划算法来计算最优句法树的，因为SU-RNN破坏了上下文无关的独立性假设（因为反向传播？其实我也不是太懂。。。）。所以就要用到之前所说的先用PCFG得到k-best棵句法树，然后用SU-RNN重排序了。</p><p>那么用SU-RNN计算完得到最优树之后，怎么计算它与gold-tree之间的差异，从而得到loss呢？</p><p>本文计算两棵树差异的公式如下：<br>\[\Delta ({y_i},\hat y) = \sum\limits_{d \in N(\hat y)} {\kappa 1\{ d \notin N({y_i})\} } \]最终的损失函数定义为：<br>\[J(\theta ) = \frac{1}{m}\sum\limits_{i = 1}^m { {r_i}(\theta )}  + \frac{\lambda }{2}{\left| \theta  \right|^2}\]其中<br>\[{r_i}(\theta ) = \mathop {\max }\limits_{\hat y \in Y({x_i})} (s(CVG({x_i},\hat y)) + \Delta ({y_i},\hat y)) - s(CVG({x_i},{y_i}))\]也就是要尽量最大化标准树的得分，减小预测树的得分。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><hr><p><img src="2.png" alt><br>可以看出，SU-RNN结果比以往的结果都要好，但是没有最后两行的好。。。最后两个具体是啥我也没去细看。</p><h1 id="我的毕设任务"><a href="#我的毕设任务" class="headerlink" title="我的毕设任务"></a>我的毕设任务</h1><hr><p>其实我的任务不用PCFG，看起来减少了工作量？嘿嘿，其实貌似麻烦的一笔啊。。。我的模型主要的思想就是直接用SU-RNN训练出句法分析树！那枚举复杂度太高了怎么办？用动态规划啊！不是不能用吗？没事，假装它能用，要是效果好强行解释一波就行了。。。而且原模型的RNN是递归神经网络哦，这次我改成了循环神经网络，用LSTM来计算得分。看起来貌似挺麻烦的，纠结了好几天。LSTM每个节点总得有一个$x$输入，一个$h$隐层输入吧，所以可能还要给每两个节点指定一个作为head。。。</p><p>感觉心态炸了哦，一堆非主流写法？也不知道最后能不能写出来，也不知道写出来结果怎么样。。。说不定要延毕了？哈哈，自嘲一波吧，暑假好好研究研究了，python基础还是不行，写起来太累了。。。</p>]]></content>
      
      
      <categories>
          
          <category> 句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACL </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 句法分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Empower Sequence Labeling with Task-Aware Neural Language Model</title>
      <link href="/2018/01/09/paperdaily-1/"/>
      <url>/2018/01/09/paperdaily-1/</url>
      
        <content type="html"><![CDATA[<p>自从这学期没课以来，一直过着非正常人的生活，作息时间比正常人推迟了3个小时：<strong>3点睡觉、12点起床、15点吃午饭、21点吃晚饭。</strong>因此决定不再如此颓废，每日泛读一篇顶会paper，了解其大概思想即可，然后大概将思想发出来，美其名曰：PaperDaily，就从今天开始吧。</p><p>今天要讲的这篇是昨天偶然看到的，AAAI18的paper，正好与我文本挖掘课大作业主题一样，所以就看了一下。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>这篇paper提出的模型叫做LM-LSTM-CRF，看起来和前面讲过的加入语言模型的两篇没有大区别，事实是区别的确不是很大。之前讲过的transfer模型基本都是共享一部分模型（底层模型），上层模型都是每个任务有各自独立的模型。然而这篇paper的模型所有部分全部共享，这就会带来许多表示上面的问题。于是这篇paper和以往最大的区别就是在character level LSTM之上加入了一个highway layer，用来将LSTM产生的字符表示映射到不同的表示空间，这样语言模型（这里的语言模型是基于字符层面的）和序列标注模型就可以共享character level LSTM。</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><hr><p>不多说，直接上图：<br><img src="1.png" alt></p><h3 id="character-level-LSTM"><a href="#character-level-LSTM" class="headerlink" title="character level LSTM"></a>character level LSTM</h3><p>这里和传统的差不多，只是改每个token单独训练一个LSTM为所有字符联合训练上下文表示（为了语言模型共用嘛），但是只在两个tokens之间输出token表示。</p><h3 id="highway-layer"><a href="#highway-layer" class="headerlink" title="highway layer"></a>highway layer</h3><p>其实就是对输出做了线性变换+门操作，具体表示如下：<br>\[\begin{array}{l}m = H(n) = t \odot g({W_H}n + {b_H}) + (1 - t) \odot n\\t = \sigma ({W_T}n + {b_T})\end{array}\]最终一共产生四个highway输出，分别是前后向序列标注表示和前后向语言模型表示。而序列标注的LSTM输入共有三个，分别是词向量、前后向序列标注表示。</p><h3 id="word-level-LSTM"><a href="#word-level-LSTM" class="headerlink" title="word level LSTM"></a>word level LSTM</h3><p>和传统的没什么不同。。。</p><h3 id="CRF-layer"><a href="#CRF-layer" class="headerlink" title="CRF layer"></a>CRF layer</h3><p>没什么不一样。。。</p><h3 id="joint-training"><a href="#joint-training" class="headerlink" title="joint training"></a>joint training</h3><p>总的损失函数就是语言模型损失函数加上序列标注损失函数，系数这里设置为1:1。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><hr><p>直接上图，这里和之前我看过的几篇paper都进行了比较，还是非常bang的！<br><img src="2.png" alt><br>注意到，他们的结果和<code>Peter 2017.</code>比较还是差了一点，因为<code>Peter 2017.</code>虽然也使用了语言模型作为辅助训练，但是他们语言模型是在大量无标注数据下进行训练的，而且花费时间特别长！而本文根本不需要任何辅助数据，少量标注数据？足够了！就在他们上面联合训练一个语言模型就行了，花费时间大大缩短。</p><p>其实我个人认为，这零点几的提升意义并不是很大，时间大大缩短倒是挺不错的，毕竟<code>Peter 2017.</code>那篇32个GPU都要训练半个月。。。<br>更远一步思考，也许可以将语言模型和序列标注独立开来训练，先用语言模型来训练character level LSTM，再用它产生每个token的表示，直接输入到序列标注的LSTM中，当然highway layer还是必要的，毕竟表示空间是不同的。这样可以利用大量的无标注数据了，但是训练时间也会大大加长，而且感觉和<code>Peter 2017.</code>的模型区别貌似不大了？只是联合训练了一个character level LSTM而已。</p><p>以上都是我的拙见，毕竟这篇也就粗略读了一下没仔细看，各位有什么想法也欢迎和我讨论。</p>]]></content>
      
      
      <categories>
          
          <category> 序列标注 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 序列标注 </tag>
            
            <tag> 迁移学习 </tag>
            
            <tag> AAAI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>词性标注+语言模型简易实现</title>
      <link href="/2018/01/01/pos-lm/"/>
      <url>/2018/01/01/pos-lm/</url>
      
        <content type="html"><![CDATA[<p>文本挖掘课的project，实现的是词性标注，增加了语言模型表示。<br>语言模型在小数据下会过拟合，但是大数据训练要三个星期。。。所以还是放弃了，不添加语言模型好了。<br>词性标注的话如果添加CRF效果反而会下降，也是很奇怪哦。。。如果直接用最裸的BiLSTM+charRNN的话，F1也能有97%左右，知足了，就这样吧。<br>数据和完整程序下载地址：<a href="https://github.com/godweiyang/text-mining" target="_blank" rel="noopener">传送门</a></p><p>下面是两组实验结果，由于速度太慢了，一组要1小时训练时间，所以就没有加语言模型，而且只训练了10轮。</p><p>第一组：</p><pre><code>DEBUG = FalseHASLM = FalseLM_EPOCH = 5TAG_EPOCH = 10MAX_LIK_ITERS = 3--------Language Model Training----------------Sequence Tagger Training--------epoch 0 finishedtotal loss:  0.29375948742total F1:  0.949073958671 0.395180722892epoch 1 finishedtotal loss:  0.132068497052total F1:  0.954682553531 0.427710843373epoch 2 finishedtotal loss:  0.110233872966total F1:  0.960266221303 0.483734939759epoch 3 finishedtotal loss:  0.0115048246573total F1:  0.944312884812 0.367469879518epoch 4 finishedtotal loss:  0.00533642838205total F1:  0.947378916669 0.375903614458epoch 5 finishedtotal loss:  0.00460870711354total F1:  0.945584166314 0.34156626506epoch 6 finishedtotal loss:  0.00420810207526total F1:  0.931001819677 0.269277108434epoch 7 finishedtotal loss:  0.00402948848795total F1:  0.943490290899 0.321084337349epoch 8 finishedtotal loss:  0.00390113119154total F1:  0.952813021911 0.431325301205epoch 9 finishedtotal loss:  0.00367663722034total F1:  0.938579654511 0.31265060241if SCONJyou PRONcould AUXsee VERBthat SCONJi PRONam AUXthe DETone NOUNwho PRONunderstands VERByou PRON. PUNC</code></pre><p>第二组：</p><pre><code>DEBUG = FalseHASLM = FalseLM_EPOCH = 5TAG_EPOCH = 10MAX_LIK_ITERS = 10--------Language Model Training----------------Sequence Tagger Training--------epoch 0 finishedtotal loss:  0.304520357251total F1:  0.948201510582 0.387951807229epoch 1 finishedtotal loss:  0.133941903738total F1:  0.957175262358 0.457228915663epoch 2 finishedtotal loss:  0.111774144948total F1:  0.959019866889 0.455421686747epoch 3 finishedtotal loss:  0.100073265445total F1:  0.960814617245 0.475301204819epoch 4 finishedtotal loss:  0.0922900494867total F1:  0.962310242541 0.487951807229epoch 5 finishedtotal loss:  0.0862275558798total F1:  0.963681232395 0.485542168675epoch 6 finishedtotal loss:  0.0811706444901total F1:  0.963706159484 0.492168674699epoch 7 finishedtotal loss:  0.0776693911075total F1:  0.962808784306 0.484939759036epoch 8 finishedtotal loss:  0.0741868944795total F1:  0.9630331281 0.495180722892epoch 9 finishedtotal loss:  0.0714286559878total F1:  0.963407034424 0.486144578313if SCONJyou PRONcould AUXsee VERBthat SCONJi PRONam VERBthe DETone NOUNwho PRONunderstands VERByou PRON. PUNC</code></pre><p>可以看出来，加了CRF（第一组）效果反而差了一点点，对最后例句的词性标注唯一的区别在于”am”是助动词AUX还是动词VERB，我发现训练集里两种都有，区别也不大。</p><p>完整代码：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> collections <span class="token keyword">import</span> Counter<span class="token punctuation">,</span> defaultdict<span class="token keyword">from</span> itertools <span class="token keyword">import</span> count<span class="token keyword">import</span> random<span class="token keyword">import</span> dynet <span class="token keyword">as</span> dy<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npDEBUG <span class="token operator">=</span> <span class="token boolean">False</span>HASLM <span class="token operator">=</span> <span class="token boolean">False</span>LM_EPOCH <span class="token operator">=</span> <span class="token number">5</span>TAG_EPOCH <span class="token operator">=</span> <span class="token number">10</span><span class="token comment" spellcheck="true"># CRF parameters</span>MAX_LIK_ITERS <span class="token operator">=</span> <span class="token number">3</span>SMALL_NUMBER <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1e10</span>MARGIN <span class="token operator">=</span> <span class="token number">0</span>lm_train_file <span class="token operator">=</span> <span class="token string">"LM_TRAIN"</span>lm_test_file <span class="token operator">=</span> <span class="token string">"LM_DEV"</span>train_file <span class="token operator">=</span> <span class="token string">"TAG_TRAIN"</span>dev_file <span class="token operator">=</span> <span class="token string">"TAG_DEV"</span><span class="token keyword">if</span> DEBUG<span class="token punctuation">:</span>    lm_train_file <span class="token operator">+=</span> <span class="token string">"_SMALL"</span>    lm_test_file <span class="token operator">+=</span> <span class="token string">"_SMALL"</span>    train_file <span class="token operator">+=</span> <span class="token string">"_SMALL"</span>    dev_file <span class="token operator">+=</span> <span class="token string">"_SMALL"</span><span class="token comment" spellcheck="true"># Language Model</span><span class="token keyword">print</span> <span class="token string">"--------Language Model Training--------"</span><span class="token keyword">def</span> <span class="token function">read_lm</span><span class="token punctuation">(</span>fname<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">with</span> file<span class="token punctuation">(</span>fname<span class="token punctuation">)</span> <span class="token keyword">as</span> fh<span class="token punctuation">:</span>        <span class="token keyword">for</span> line <span class="token keyword">in</span> fh<span class="token punctuation">:</span>            sent <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>            sent<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"&lt;s>"</span><span class="token punctuation">)</span>            <span class="token keyword">yield</span> sentlm_train <span class="token operator">=</span> list<span class="token punctuation">(</span>read_lm<span class="token punctuation">(</span>lm_train_file<span class="token punctuation">)</span><span class="token punctuation">)</span>lm_test <span class="token operator">=</span> list<span class="token punctuation">(</span>read_lm<span class="token punctuation">(</span>lm_test_file<span class="token punctuation">)</span><span class="token punctuation">)</span>lm_words <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> sent <span class="token keyword">in</span> lm_train<span class="token punctuation">:</span>    <span class="token keyword">for</span> w <span class="token keyword">in</span> sent<span class="token punctuation">:</span>        lm_words<span class="token punctuation">.</span>append<span class="token punctuation">(</span>w<span class="token punctuation">)</span>lm_words<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"_UNK_"</span><span class="token punctuation">)</span>lm_w2i <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>count<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>next<span class="token punctuation">)</span><span class="token keyword">for</span> word <span class="token keyword">in</span> lm_words<span class="token punctuation">:</span>    lm_w2i<span class="token punctuation">[</span>word<span class="token punctuation">]</span>lm_i2w <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span>w <span class="token keyword">for</span> w<span class="token punctuation">,</span> i <span class="token keyword">in</span> lm_w2i<span class="token punctuation">.</span>iteritems<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>lm_nwords <span class="token operator">=</span> len<span class="token punctuation">(</span>lm_w2i<span class="token punctuation">)</span>lm_model <span class="token operator">=</span> dy<span class="token punctuation">.</span>Model<span class="token punctuation">(</span><span class="token punctuation">)</span>lm_trainer <span class="token operator">=</span> dy<span class="token punctuation">.</span>AdamTrainer<span class="token punctuation">(</span>lm_model<span class="token punctuation">)</span>lm_WORDS_LOOKUP <span class="token operator">=</span> lm_model<span class="token punctuation">.</span>add_lookup_parameters<span class="token punctuation">(</span><span class="token punctuation">(</span>lm_nwords<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span>lm_RNN <span class="token operator">=</span> dy<span class="token punctuation">.</span>LSTMBuilder<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> lm_model<span class="token punctuation">)</span>lm_pW <span class="token operator">=</span> lm_model<span class="token punctuation">.</span>add_parameters<span class="token punctuation">(</span><span class="token punctuation">(</span>lm_nwords<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">)</span>lm_pb <span class="token operator">=</span> lm_model<span class="token punctuation">.</span>add_parameters<span class="token punctuation">(</span>lm_nwords<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">calc_lm_loss</span><span class="token punctuation">(</span>sent<span class="token punctuation">)</span><span class="token punctuation">:</span>    dy<span class="token punctuation">.</span>renew_cg<span class="token punctuation">(</span><span class="token punctuation">)</span>    W <span class="token operator">=</span> dy<span class="token punctuation">.</span>parameter<span class="token punctuation">(</span>lm_pW<span class="token punctuation">)</span>    b <span class="token operator">=</span> dy<span class="token punctuation">.</span>parameter<span class="token punctuation">(</span>lm_pb<span class="token punctuation">)</span>    f_init <span class="token operator">=</span> lm_RNN<span class="token punctuation">.</span>initial_state<span class="token punctuation">(</span><span class="token punctuation">)</span>    wids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> w <span class="token keyword">in</span> sent<span class="token punctuation">:</span>        <span class="token keyword">if</span> w <span class="token keyword">in</span> lm_words<span class="token punctuation">:</span>            wids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>lm_w2i<span class="token punctuation">[</span>w<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            wids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>lm_w2i<span class="token punctuation">[</span><span class="token string">"_UNK_"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    s <span class="token operator">=</span> f_init<span class="token punctuation">.</span>add_input<span class="token punctuation">(</span>lm_WORDS_LOOKUP<span class="token punctuation">[</span>wids<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> wid <span class="token keyword">in</span> wids<span class="token punctuation">:</span>        score <span class="token operator">=</span> W <span class="token operator">*</span> s<span class="token punctuation">.</span>output<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> b        loss <span class="token operator">=</span> dy<span class="token punctuation">.</span>pickneglogsoftmax<span class="token punctuation">(</span>score<span class="token punctuation">,</span> wid<span class="token punctuation">)</span>        losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>        s <span class="token operator">=</span> s<span class="token punctuation">.</span>add_input<span class="token punctuation">(</span>lm_WORDS_LOOKUP<span class="token punctuation">[</span>wid<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> dy<span class="token punctuation">.</span>esum<span class="token punctuation">(</span>losses<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">calc_lm_embdding</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">:</span>    dy<span class="token punctuation">.</span>renew_cg<span class="token punctuation">(</span><span class="token punctuation">)</span>    f_init <span class="token operator">=</span> lm_RNN<span class="token punctuation">.</span>initial_state<span class="token punctuation">(</span><span class="token punctuation">)</span>    wids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> w <span class="token keyword">in</span> words<span class="token punctuation">:</span>        <span class="token keyword">if</span> w <span class="token keyword">in</span> lm_words<span class="token punctuation">:</span>            wids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>lm_w2i<span class="token punctuation">[</span>w<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            wids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>lm_w2i<span class="token punctuation">[</span><span class="token string">"_UNK_"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    wids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>lm_w2i<span class="token punctuation">[</span><span class="token string">"&lt;s>"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    s <span class="token operator">=</span> f_init<span class="token punctuation">.</span>add_input<span class="token punctuation">(</span>lm_WORDS_LOOKUP<span class="token punctuation">[</span>wids<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> wid <span class="token keyword">in</span> wids<span class="token punctuation">:</span>        outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>s<span class="token punctuation">.</span>output<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>value<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        s <span class="token operator">=</span> s<span class="token punctuation">.</span>add_input<span class="token punctuation">(</span>lm_WORDS_LOOKUP<span class="token punctuation">[</span>wid<span class="token punctuation">]</span><span class="token punctuation">)</span>     <span class="token keyword">return</span> outputs<span class="token keyword">if</span> HASLM<span class="token punctuation">:</span>    <span class="token keyword">for</span> ITER <span class="token keyword">in</span> xrange<span class="token punctuation">(</span>LM_EPOCH<span class="token punctuation">)</span><span class="token punctuation">:</span>        lm_num_tagged <span class="token operator">=</span> lm_cum_loss <span class="token operator">=</span> <span class="token number">0</span>        random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>lm_train<span class="token punctuation">)</span>        i <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">for</span> s <span class="token keyword">in</span> lm_train<span class="token punctuation">:</span>            loss_exp <span class="token operator">=</span> calc_lm_loss<span class="token punctuation">(</span>s<span class="token punctuation">)</span>            lm_cum_loss <span class="token operator">+=</span> loss_exp<span class="token punctuation">.</span>scalar_value<span class="token punctuation">(</span><span class="token punctuation">)</span>            lm_num_tagged <span class="token operator">+=</span> len<span class="token punctuation">(</span>s<span class="token punctuation">)</span>            loss_exp<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            lm_trainer<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> DEBUG <span class="token operator">==</span> <span class="token boolean">False</span><span class="token punctuation">:</span>                i <span class="token operator">+=</span> <span class="token number">1</span>                <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                    <span class="token keyword">print</span> <span class="token string">"train loss "</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token string">": "</span><span class="token punctuation">,</span> lm_cum_loss <span class="token operator">/</span> lm_num_tagged        dev_loss <span class="token operator">=</span> dev_words <span class="token operator">=</span> <span class="token number">0</span>        i <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">for</span> sent <span class="token keyword">in</span> lm_test<span class="token punctuation">:</span>            loss_exp <span class="token operator">=</span> calc_lm_loss<span class="token punctuation">(</span>sent<span class="token punctuation">)</span>            dev_loss <span class="token operator">+=</span> loss_exp<span class="token punctuation">.</span>scalar_value<span class="token punctuation">(</span><span class="token punctuation">)</span>            dev_words <span class="token operator">+=</span> len<span class="token punctuation">(</span>sent<span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># if DEBUG == False:</span>            <span class="token comment" spellcheck="true">#     i += 1</span>            <span class="token comment" spellcheck="true">#     if i % 100 == 0:</span>            <span class="token comment" spellcheck="true">#         print "dev loss ", i, ": ", dev_loss / dev_words</span>        <span class="token keyword">print</span> <span class="token string">"epoch %r finished"</span> <span class="token operator">%</span> ITER        <span class="token keyword">print</span> <span class="token string">"total train loss: "</span><span class="token punctuation">,</span> lm_cum_loss <span class="token operator">/</span> lm_num_tagged        <span class="token keyword">print</span> <span class="token string">"total dev loss: "</span><span class="token punctuation">,</span> dev_loss <span class="token operator">/</span> dev_words<span class="token comment" spellcheck="true"># Tagger</span><span class="token keyword">print</span> <span class="token string">"--------Sequence Tagger Training--------"</span><span class="token keyword">def</span> <span class="token function">read</span><span class="token punctuation">(</span>fname<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">with</span> file<span class="token punctuation">(</span>fname<span class="token punctuation">)</span> <span class="token keyword">as</span> fh<span class="token punctuation">:</span>        <span class="token keyword">for</span> line <span class="token keyword">in</span> fh<span class="token punctuation">:</span>            line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>            sent <span class="token operator">=</span> <span class="token punctuation">[</span>tuple<span class="token punctuation">(</span>x<span class="token punctuation">.</span>rsplit<span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> line<span class="token punctuation">]</span>            <span class="token keyword">yield</span> senttrain <span class="token operator">=</span> list<span class="token punctuation">(</span>read<span class="token punctuation">(</span>train_file<span class="token punctuation">)</span><span class="token punctuation">)</span>dev <span class="token operator">=</span> list<span class="token punctuation">(</span>read<span class="token punctuation">(</span>dev_file<span class="token punctuation">)</span><span class="token punctuation">)</span>words <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>tags <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>chars <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>wc <span class="token operator">=</span> Counter<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> sent <span class="token keyword">in</span> train<span class="token punctuation">:</span>    <span class="token keyword">for</span> w<span class="token punctuation">,</span> p <span class="token keyword">in</span> sent<span class="token punctuation">:</span>        words<span class="token punctuation">.</span>append<span class="token punctuation">(</span>w<span class="token punctuation">)</span>        tags<span class="token punctuation">.</span>append<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        chars<span class="token punctuation">.</span>update<span class="token punctuation">(</span>w<span class="token punctuation">)</span>        wc<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>words<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"_UNK_"</span><span class="token punctuation">)</span>words<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"_S_"</span><span class="token punctuation">)</span>tags<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"_S_"</span><span class="token punctuation">)</span>chars<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token string">"&lt;*>"</span><span class="token punctuation">)</span>w2i <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>count<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>next<span class="token punctuation">)</span><span class="token keyword">for</span> word <span class="token keyword">in</span> words<span class="token punctuation">:</span>    w2i<span class="token punctuation">[</span>word<span class="token punctuation">]</span>i2w <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span>w <span class="token keyword">for</span> w<span class="token punctuation">,</span> i <span class="token keyword">in</span> w2i<span class="token punctuation">.</span>iteritems<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>t2i <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>count<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>next<span class="token punctuation">)</span><span class="token keyword">for</span> tag <span class="token keyword">in</span> tags<span class="token punctuation">:</span>    t2i<span class="token punctuation">[</span>tag<span class="token punctuation">]</span>i2t <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span>w <span class="token keyword">for</span> w<span class="token punctuation">,</span> i <span class="token keyword">in</span> t2i<span class="token punctuation">.</span>iteritems<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>c2i <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>count<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>next<span class="token punctuation">)</span><span class="token keyword">for</span> char <span class="token keyword">in</span> chars<span class="token punctuation">:</span>    c2i<span class="token punctuation">[</span>char<span class="token punctuation">]</span>i2c <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span>w <span class="token keyword">for</span> w<span class="token punctuation">,</span> i <span class="token keyword">in</span> c2i<span class="token punctuation">.</span>iteritems<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>UNK <span class="token operator">=</span> w2i<span class="token punctuation">[</span><span class="token string">"_UNK_"</span><span class="token punctuation">]</span>S_W <span class="token operator">=</span> w2i<span class="token punctuation">[</span><span class="token string">"_S_"</span><span class="token punctuation">]</span>S_T <span class="token operator">=</span> t2i<span class="token punctuation">[</span><span class="token string">"_S_"</span><span class="token punctuation">]</span>nwords <span class="token operator">=</span> len<span class="token punctuation">(</span>w2i<span class="token punctuation">)</span>ntags  <span class="token operator">=</span> len<span class="token punctuation">(</span>t2i<span class="token punctuation">)</span>nchars  <span class="token operator">=</span> len<span class="token punctuation">(</span>c2i<span class="token punctuation">)</span>model <span class="token operator">=</span> dy<span class="token punctuation">.</span>Model<span class="token punctuation">(</span><span class="token punctuation">)</span>trainer <span class="token operator">=</span> dy<span class="token punctuation">.</span>AdamTrainer<span class="token punctuation">(</span>model<span class="token punctuation">)</span>WORDS_LOOKUP <span class="token operator">=</span> model<span class="token punctuation">.</span>add_lookup_parameters<span class="token punctuation">(</span><span class="token punctuation">(</span>nwords<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">)</span>CHARS_LOOKUP <span class="token operator">=</span> model<span class="token punctuation">.</span>add_lookup_parameters<span class="token punctuation">(</span><span class="token punctuation">(</span>nchars<span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span>TRANS_LOOKUP <span class="token operator">=</span> model<span class="token punctuation">.</span>add_lookup_parameters<span class="token punctuation">(</span><span class="token punctuation">(</span>ntags<span class="token punctuation">,</span> ntags<span class="token punctuation">)</span><span class="token punctuation">)</span>pH <span class="token operator">=</span> model<span class="token punctuation">.</span>add_parameters<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>pO <span class="token operator">=</span> model<span class="token punctuation">.</span>add_parameters<span class="token punctuation">(</span><span class="token punctuation">(</span>ntags<span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fwdRNN <span class="token operator">=</span> dy<span class="token punctuation">.</span>LSTMBuilder<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> model<span class="token punctuation">)</span>bwdRNN <span class="token operator">=</span> dy<span class="token punctuation">.</span>LSTMBuilder<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> model<span class="token punctuation">)</span><span class="token keyword">if</span> HASLM<span class="token punctuation">:</span>    fwdRNN <span class="token operator">=</span> dy<span class="token punctuation">.</span>LSTMBuilder<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> model<span class="token punctuation">)</span>    bwdRNN <span class="token operator">=</span> dy<span class="token punctuation">.</span>LSTMBuilder<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> model<span class="token punctuation">)</span>cFwdRNN <span class="token operator">=</span> dy<span class="token punctuation">.</span>LSTMBuilder<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> model<span class="token punctuation">)</span>cBwdRNN <span class="token operator">=</span> dy<span class="token punctuation">.</span>LSTMBuilder<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> model<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">word_rep</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> cf_init<span class="token punctuation">,</span> cb_init<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> wc<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">5</span><span class="token punctuation">:</span>        w_index <span class="token operator">=</span> w2i<span class="token punctuation">[</span>w<span class="token punctuation">]</span>        <span class="token keyword">return</span> WORDS_LOOKUP<span class="token punctuation">[</span>w_index<span class="token punctuation">]</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        pad_char <span class="token operator">=</span> c2i<span class="token punctuation">[</span><span class="token string">"&lt;*>"</span><span class="token punctuation">]</span>        char_ids <span class="token operator">=</span> <span class="token punctuation">[</span>pad_char<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>c2i<span class="token punctuation">[</span>c<span class="token punctuation">]</span> <span class="token keyword">for</span> c <span class="token keyword">in</span> w<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>pad_char<span class="token punctuation">]</span>        char_embs <span class="token operator">=</span> <span class="token punctuation">[</span>CHARS_LOOKUP<span class="token punctuation">[</span>cid<span class="token punctuation">]</span> <span class="token keyword">for</span> cid <span class="token keyword">in</span> char_ids<span class="token punctuation">]</span>        fw_exps <span class="token operator">=</span> cf_init<span class="token punctuation">.</span>transduce<span class="token punctuation">(</span>char_embs<span class="token punctuation">)</span>        bw_exps <span class="token operator">=</span> cb_init<span class="token punctuation">.</span>transduce<span class="token punctuation">(</span>reversed<span class="token punctuation">(</span>char_embs<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> dy<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span> fw_exps<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bw_exps<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">build_tagging_graph</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">:</span>    lm_wembs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">if</span> HASLM<span class="token punctuation">:</span>        lm_wembs <span class="token operator">=</span> calc_lm_embdding<span class="token punctuation">(</span>words<span class="token punctuation">)</span>    dy<span class="token punctuation">.</span>renew_cg<span class="token punctuation">(</span><span class="token punctuation">)</span>    H <span class="token operator">=</span> dy<span class="token punctuation">.</span>parameter<span class="token punctuation">(</span>pH<span class="token punctuation">)</span>    O <span class="token operator">=</span> dy<span class="token punctuation">.</span>parameter<span class="token punctuation">(</span>pO<span class="token punctuation">)</span>    f_init <span class="token operator">=</span> fwdRNN<span class="token punctuation">.</span>initial_state<span class="token punctuation">(</span><span class="token punctuation">)</span>    b_init <span class="token operator">=</span> bwdRNN<span class="token punctuation">.</span>initial_state<span class="token punctuation">(</span><span class="token punctuation">)</span>    cf_init <span class="token operator">=</span> cFwdRNN<span class="token punctuation">.</span>initial_state<span class="token punctuation">(</span><span class="token punctuation">)</span>    cb_init <span class="token operator">=</span> cBwdRNN<span class="token punctuation">.</span>initial_state<span class="token punctuation">(</span><span class="token punctuation">)</span>    wembs <span class="token operator">=</span> <span class="token punctuation">[</span>word_rep<span class="token punctuation">(</span>w<span class="token punctuation">,</span> cf_init<span class="token punctuation">,</span> cb_init<span class="token punctuation">)</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> words<span class="token punctuation">]</span>    <span class="token keyword">if</span> HASLM<span class="token punctuation">:</span>        wembs1 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> lmw<span class="token punctuation">,</span> w <span class="token keyword">in</span> zip<span class="token punctuation">(</span>lm_wembs<span class="token punctuation">,</span> wembs<span class="token punctuation">)</span><span class="token punctuation">:</span>            wv <span class="token operator">=</span> w<span class="token punctuation">.</span>value<span class="token punctuation">(</span><span class="token punctuation">)</span>            wv<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>lmw<span class="token punctuation">)</span>            wembs1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>wv<span class="token punctuation">)</span>        wembs <span class="token operator">=</span> <span class="token punctuation">[</span>dy<span class="token punctuation">.</span>inputTensor<span class="token punctuation">(</span>w<span class="token punctuation">)</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> wembs1<span class="token punctuation">]</span>    wembs <span class="token operator">=</span> <span class="token punctuation">[</span>dy<span class="token punctuation">.</span>noise<span class="token punctuation">(</span>we<span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> we <span class="token keyword">in</span> wembs<span class="token punctuation">]</span>    fw_exps <span class="token operator">=</span> f_init<span class="token punctuation">.</span>transduce<span class="token punctuation">(</span>wembs<span class="token punctuation">)</span>    bw_exps <span class="token operator">=</span> b_init<span class="token punctuation">.</span>transduce<span class="token punctuation">(</span>reversed<span class="token punctuation">(</span>wembs<span class="token punctuation">)</span><span class="token punctuation">)</span>    bi_exps <span class="token operator">=</span> <span class="token punctuation">[</span>dy<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>f<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> f<span class="token punctuation">,</span> b <span class="token keyword">in</span> zip<span class="token punctuation">(</span>fw_exps<span class="token punctuation">,</span> reversed<span class="token punctuation">(</span>bw_exps<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    exps <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> x <span class="token keyword">in</span> bi_exps<span class="token punctuation">:</span>        r_t <span class="token operator">=</span> O <span class="token operator">*</span> <span class="token punctuation">(</span>dy<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>H <span class="token operator">*</span> x<span class="token punctuation">)</span><span class="token punctuation">)</span>        exps<span class="token punctuation">.</span>append<span class="token punctuation">(</span>r_t<span class="token punctuation">)</span>    <span class="token keyword">return</span> exps<span class="token keyword">def</span> <span class="token function">viterbi_decoding</span><span class="token punctuation">(</span>vecs<span class="token punctuation">,</span> gold_tags <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># Initialize</span>    init_prob <span class="token operator">=</span> <span class="token punctuation">[</span>SMALL_NUMBER<span class="token punctuation">]</span> <span class="token operator">*</span> ntags    init_prob<span class="token punctuation">[</span>S_T<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>    for_expr <span class="token operator">=</span> dy<span class="token punctuation">.</span>inputVector<span class="token punctuation">(</span>init_prob<span class="token punctuation">)</span>    best_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    trans_exprs <span class="token operator">=</span> <span class="token punctuation">[</span>TRANS_LOOKUP<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token keyword">for</span> tid <span class="token keyword">in</span> range<span class="token punctuation">(</span>ntags<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># Perform the forward pass through the sentence</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> vec <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>vecs<span class="token punctuation">)</span><span class="token punctuation">:</span>        my_best_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        my_best_exprs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> next_tag <span class="token keyword">in</span> range<span class="token punctuation">(</span>ntags<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Calculate vector for single next tag</span>            next_single_expr <span class="token operator">=</span> for_expr <span class="token operator">+</span> trans_exprs<span class="token punctuation">[</span>next_tag<span class="token punctuation">]</span>            next_single <span class="token operator">=</span> next_single_expr<span class="token punctuation">.</span>npvalue<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># Find and save the best score</span>            my_best_id <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>next_single<span class="token punctuation">)</span>            my_best_ids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>my_best_id<span class="token punctuation">)</span>            my_best_exprs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>dy<span class="token punctuation">.</span>pick<span class="token punctuation">(</span>next_single_expr<span class="token punctuation">,</span> my_best_id<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Concatenate the scores for all vectors together</span>        for_expr <span class="token operator">=</span> dy<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span>my_best_exprs<span class="token punctuation">)</span> <span class="token operator">+</span> vec        <span class="token comment" spellcheck="true"># Give a bonus to all but the correct tag if using margin</span>        <span class="token keyword">if</span> MARGIN <span class="token operator">!=</span> <span class="token number">0</span> <span class="token operator">and</span> len<span class="token punctuation">(</span>gold_tags<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>            adjust <span class="token operator">=</span> <span class="token punctuation">[</span>MARGIN<span class="token punctuation">]</span> <span class="token operator">*</span> ntags            adjust<span class="token punctuation">[</span>t2i<span class="token punctuation">[</span>gold_tags<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>            for_expr <span class="token operator">=</span> for_expr <span class="token operator">+</span> dy<span class="token punctuation">.</span>inputVector<span class="token punctuation">(</span>adjust<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Save the best ids</span>        best_ids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>my_best_ids<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Perform the final step to the sentence terminal symbol</span>    next_single_expr <span class="token operator">=</span> for_expr <span class="token operator">+</span> trans_exprs<span class="token punctuation">[</span>S_T<span class="token punctuation">]</span>    next_single <span class="token operator">=</span> next_single_expr<span class="token punctuation">.</span>npvalue<span class="token punctuation">(</span><span class="token punctuation">)</span>    my_best_id <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>next_single<span class="token punctuation">)</span>    best_expr <span class="token operator">=</span> dy<span class="token punctuation">.</span>pick<span class="token punctuation">(</span>next_single_expr<span class="token punctuation">,</span> my_best_id<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Perform the reverse pass</span>    best_path <span class="token operator">=</span> <span class="token punctuation">[</span>i2t<span class="token punctuation">[</span>my_best_id<span class="token punctuation">]</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> my_best_ids <span class="token keyword">in</span> reversed<span class="token punctuation">(</span>best_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>        my_best_id <span class="token operator">=</span> my_best_ids<span class="token punctuation">[</span>my_best_id<span class="token punctuation">]</span>        best_path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i2t<span class="token punctuation">[</span>my_best_id<span class="token punctuation">]</span><span class="token punctuation">)</span>    best_path<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># Remove final &lt;s></span>    best_path<span class="token punctuation">.</span>reverse<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Return the best path and best score as an expression</span>    <span class="token keyword">return</span> best_path<span class="token punctuation">,</span> best_expr<span class="token keyword">def</span> <span class="token function">forced_decoding</span><span class="token punctuation">(</span>vecs<span class="token punctuation">,</span> tags<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># Initialize</span>    for_expr <span class="token operator">=</span> dy<span class="token punctuation">.</span>scalarInput<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    for_tag <span class="token operator">=</span> S_T    <span class="token comment" spellcheck="true"># Perform the forward pass through the sentence</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> vec <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>vecs<span class="token punctuation">)</span><span class="token punctuation">:</span>         my_tag <span class="token operator">=</span> t2i<span class="token punctuation">[</span>tags<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span>        for_expr <span class="token operator">=</span> for_expr <span class="token operator">+</span> dy<span class="token punctuation">.</span>pick<span class="token punctuation">(</span>TRANS_LOOKUP<span class="token punctuation">[</span>my_tag<span class="token punctuation">]</span><span class="token punctuation">,</span> for_tag<span class="token punctuation">)</span> <span class="token operator">+</span> vec<span class="token punctuation">[</span>my_tag<span class="token punctuation">]</span>        for_tag <span class="token operator">=</span> my_tag    for_expr <span class="token operator">=</span> for_expr <span class="token operator">+</span> dy<span class="token punctuation">.</span>pick<span class="token punctuation">(</span>TRANS_LOOKUP<span class="token punctuation">[</span>S_T<span class="token punctuation">]</span><span class="token punctuation">,</span> for_tag<span class="token punctuation">)</span>    <span class="token keyword">return</span> for_expr<span class="token keyword">def</span> <span class="token function">viterbi_sent_loss</span><span class="token punctuation">(</span>words<span class="token punctuation">,</span> tags<span class="token punctuation">)</span><span class="token punctuation">:</span>    vecs <span class="token operator">=</span> build_tagging_graph<span class="token punctuation">(</span>words<span class="token punctuation">)</span>    viterbi_tags<span class="token punctuation">,</span> viterbi_score <span class="token operator">=</span> viterbi_decoding<span class="token punctuation">(</span>vecs<span class="token punctuation">,</span> tags<span class="token punctuation">)</span>    <span class="token keyword">if</span> viterbi_tags <span class="token operator">!=</span> tags<span class="token punctuation">:</span>        reference_score <span class="token operator">=</span> forced_decoding<span class="token punctuation">(</span>vecs<span class="token punctuation">,</span> tags<span class="token punctuation">)</span>        <span class="token keyword">return</span> viterbi_score <span class="token operator">-</span> reference_score    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> dy<span class="token punctuation">.</span>scalarInput<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">sent_loss</span><span class="token punctuation">(</span>words<span class="token punctuation">,</span> tags<span class="token punctuation">)</span><span class="token punctuation">:</span>    vecs <span class="token operator">=</span> build_tagging_graph<span class="token punctuation">(</span>words<span class="token punctuation">)</span>    errs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> v<span class="token punctuation">,</span>t <span class="token keyword">in</span> zip<span class="token punctuation">(</span>vecs<span class="token punctuation">,</span>tags<span class="token punctuation">)</span><span class="token punctuation">:</span>        tid <span class="token operator">=</span> t2i<span class="token punctuation">[</span>t<span class="token punctuation">]</span>        err <span class="token operator">=</span> dy<span class="token punctuation">.</span>pickneglogsoftmax<span class="token punctuation">(</span>v<span class="token punctuation">,</span> tid<span class="token punctuation">)</span>        errs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>err<span class="token punctuation">)</span>    <span class="token keyword">return</span> dy<span class="token punctuation">.</span>esum<span class="token punctuation">(</span>errs<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">tag_sent</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">:</span>    vecs <span class="token operator">=</span> build_tagging_graph<span class="token punctuation">(</span>words<span class="token punctuation">)</span>    vecs <span class="token operator">=</span> <span class="token punctuation">[</span>dy<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>v<span class="token punctuation">)</span> <span class="token keyword">for</span> v <span class="token keyword">in</span> vecs<span class="token punctuation">]</span>    probs <span class="token operator">=</span> <span class="token punctuation">[</span>v<span class="token punctuation">.</span>npvalue<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> v <span class="token keyword">in</span> vecs<span class="token punctuation">]</span>    tags <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> prb <span class="token keyword">in</span> probs<span class="token punctuation">:</span>        tag <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>prb<span class="token punctuation">)</span>        tags<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i2t<span class="token punctuation">[</span>tag<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> tags<span class="token keyword">for</span> ITER <span class="token keyword">in</span> xrange<span class="token punctuation">(</span>TAG_EPOCH<span class="token punctuation">)</span><span class="token punctuation">:</span>    num_tagged <span class="token operator">=</span> cum_loss <span class="token operator">=</span> <span class="token number">0</span>    random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>train<span class="token punctuation">)</span>    i <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> s <span class="token keyword">in</span> train<span class="token punctuation">:</span>        words <span class="token operator">=</span> <span class="token punctuation">[</span>w <span class="token keyword">for</span> w<span class="token punctuation">,</span> t <span class="token keyword">in</span> s<span class="token punctuation">]</span>        golds <span class="token operator">=</span> <span class="token punctuation">[</span>t <span class="token keyword">for</span> w<span class="token punctuation">,</span> t <span class="token keyword">in</span> s<span class="token punctuation">]</span>        <span class="token keyword">if</span> ITER <span class="token operator">&lt;</span> MAX_LIK_ITERS<span class="token punctuation">:</span>            loss_exp <span class="token operator">=</span>  sent_loss<span class="token punctuation">(</span>words<span class="token punctuation">,</span> golds<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            loss_exp <span class="token operator">=</span>  viterbi_sent_loss<span class="token punctuation">(</span>words<span class="token punctuation">,</span> golds<span class="token punctuation">)</span>        cum_loss <span class="token operator">+=</span> loss_exp<span class="token punctuation">.</span>scalar_value<span class="token punctuation">(</span><span class="token punctuation">)</span>        num_tagged <span class="token operator">+=</span> len<span class="token punctuation">(</span>golds<span class="token punctuation">)</span>        loss_exp<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        trainer<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> DEBUG <span class="token operator">==</span> <span class="token boolean">False</span><span class="token punctuation">:</span>            i <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">1000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                <span class="token keyword">print</span> <span class="token string">"train loss "</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token string">": "</span><span class="token punctuation">,</span> cum_loss <span class="token operator">/</span> num_tagged    good_sent <span class="token operator">=</span> bad_sent <span class="token operator">=</span> good <span class="token operator">=</span> bad <span class="token operator">=</span> <span class="token number">0.0</span>    i <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> sent <span class="token keyword">in</span> dev<span class="token punctuation">:</span>        words <span class="token operator">=</span> <span class="token punctuation">[</span>w <span class="token keyword">for</span> w<span class="token punctuation">,</span> t <span class="token keyword">in</span> sent<span class="token punctuation">]</span>        golds <span class="token operator">=</span> <span class="token punctuation">[</span>t <span class="token keyword">for</span> w<span class="token punctuation">,</span> t <span class="token keyword">in</span> sent<span class="token punctuation">]</span>        <span class="token keyword">if</span> ITER <span class="token operator">&lt;</span> MAX_LIK_ITERS<span class="token punctuation">:</span>            tags <span class="token operator">=</span> tag_sent<span class="token punctuation">(</span>words<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            vecs <span class="token operator">=</span> build_tagging_graph<span class="token punctuation">(</span>words<span class="token punctuation">)</span>            tags<span class="token punctuation">,</span> loss_exp <span class="token operator">=</span> viterbi_decoding<span class="token punctuation">(</span>vecs<span class="token punctuation">)</span>        <span class="token keyword">if</span> tags <span class="token operator">==</span> golds<span class="token punctuation">:</span> good_sent <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">else</span><span class="token punctuation">:</span> bad_sent <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">for</span> go<span class="token punctuation">,</span> gu <span class="token keyword">in</span> zip<span class="token punctuation">(</span>golds<span class="token punctuation">,</span> tags<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> go <span class="token operator">==</span> gu<span class="token punctuation">:</span> good <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token keyword">else</span><span class="token punctuation">:</span> bad <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># if DEBUG == False:</span>        <span class="token comment" spellcheck="true">#     i += 1</span>        <span class="token comment" spellcheck="true">#     if i % 1000 == 0:</span>        <span class="token comment" spellcheck="true">#         print "F1 ", i, ": ", good / (good + bad)</span>    <span class="token keyword">print</span> <span class="token string">"epoch %r finished"</span> <span class="token operator">%</span> ITER    <span class="token keyword">print</span> <span class="token string">"total loss: "</span><span class="token punctuation">,</span> cum_loss <span class="token operator">/</span> num_tagged    <span class="token keyword">print</span> <span class="token string">"total F1: "</span><span class="token punctuation">,</span> good <span class="token operator">/</span> <span class="token punctuation">(</span>good <span class="token operator">+</span> bad<span class="token punctuation">)</span><span class="token punctuation">,</span> good_sent <span class="token operator">/</span> <span class="token punctuation">(</span>good_sent <span class="token operator">+</span> bad_sent<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">tagging</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>    words <span class="token operator">=</span> sentence<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> TAG_EPOCH <span class="token operator">&lt;=</span> MAX_LIK_ITERS<span class="token punctuation">:</span>        tags <span class="token operator">=</span> tag_sent<span class="token punctuation">(</span>words<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        vecs <span class="token operator">=</span> build_tagging_graph<span class="token punctuation">(</span>words<span class="token punctuation">)</span>        tags<span class="token punctuation">,</span> loss_exp <span class="token operator">=</span> viterbi_decoding<span class="token punctuation">(</span>vecs<span class="token punctuation">)</span>    <span class="token keyword">for</span> w<span class="token punctuation">,</span> t <span class="token keyword">in</span> zip<span class="token punctuation">(</span>words<span class="token punctuation">,</span> tags<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span> w<span class="token punctuation">,</span> t<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    sentence <span class="token operator">=</span> <span class="token string">"if you could see that i am the one who understands you ."</span>    tagging<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 序列标注 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACL </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Dynet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sequence Tagging with Little Labeled Data</title>
      <link href="/2017/12/30/text-minning-ppt/"/>
      <url>/2017/12/30/text-minning-ppt/</url>
      
        <content type="html"><![CDATA[<p>历经几个星期的磨难，文本挖掘课的presentation课件初稿基本完成了，1月中下旬开讲，这次讲的是基于少量标注数据的序列标注，下面是我的综述。</p><h1 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h1><hr><ul><li>Sequence Tagging</li><li>Semi-supervised Learning</li><li>Transfer Learning</li><li>Conclusions</li><li>References</li></ul><h1 id="Sequence-Tagging"><a href="#Sequence-Tagging" class="headerlink" title="Sequence Tagging"></a>Sequence Tagging</h1><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><strong>Definition</strong><br>Sequence tagging is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values.<br><strong>Significance</strong><br>Sequence tagging is one of the first stages in most natural language processing applications, such as part-of-speech tagging, chunking and named entity recognition.<br><strong>Approaches</strong></p><ul><li><del>Traditional models</del><ul><li><del>Hidden Markov Models</del></li><li><del>Conditional Random Fields</del></li></ul></li><li>Neural network models<ul><li>RNN, LSTM, GRU</li></ul></li></ul><h3 id="Neural-Network-Model"><a href="#Neural-Network-Model" class="headerlink" title="Neural Network Model"></a>Neural Network Model</h3><p><img src="1.png" alt></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="3.png" alt></p><h3 id="Sequence-Tagging-with-Little-Labeled-Data"><a href="#Sequence-Tagging-with-Little-Labeled-Data" class="headerlink" title="Sequence Tagging with Little Labeled Data"></a>Sequence Tagging with Little Labeled Data</h3><p><strong>Backgrounds</strong><br>Although recent neural networks obtain state-of-the-art performance on several sequence tagging tasks, they can’t be used for tasks with little labeled data.<br><strong>Approaches</strong></p><ul><li><del>Self-taught learning</del></li><li><del>Active learning</del></li><li><del>Transductive learning</del></li><li>Semi-supervised learning</li><li>Transfer learning</li></ul><h1 id="Semi-supervised-Learning"><a href="#Semi-supervised-Learning" class="headerlink" title="Semi-supervised Learning"></a>Semi-supervised Learning</h1><hr><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p><strong>Language Models Added</strong></p><ul><li>Semi-supervised Multitask Learning for Sequence Labeling. Marek Rei. ACL17.</li><li>Semi-supervised Sequence Tagging with Bidirectional Language Models. Matthew et al. ACL17.</li></ul><p><strong>Graph-based</strong></p><ul><li>Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models. Subramanya et al. EMNLP10.</li><li>Scientific Information Extraction with Semi-supervised Neural Tagging. Luan et al. EMNLP17.</li><li>Graph-based Semi-supervised Acoustic Modeling in DNN-based Speech Recognition. Liu et al. IEEE SLT14.</li></ul><h3 id="Language-Models-Added"><a href="#Language-Models-Added" class="headerlink" title="Language Models Added"></a>Language Models Added</h3><p><img src="2.png" alt></p><h3 id="Language-Modeling-Objective"><a href="#Language-Modeling-Objective" class="headerlink" title="Language Modeling Objective"></a>Language Modeling Objective</h3><p>\[\begin{array}{l}\overrightarrow { {m_t}}  = \tanh (\overrightarrow { {W_m}} \overrightarrow { {h_t}} )\\\overleftarrow { {m_t}}  = \tanh (\overleftarrow { {W_m}} \overleftarrow { {h_t}} )\\P({w_{t + 1}}|\overrightarrow { {m_t}} ) = {\rm{softmax}}(\overrightarrow { {W_q}} \overrightarrow { {m_t}} )\\P({w_{t - 1}}|\overleftarrow { {m_t}} ) = {\rm{softmax}}(\overleftarrow { {W_q}} \overleftarrow { {m_t}} )\\\overrightarrow E  =  - \sum\limits_{t = 1}^{T - 1} {\log (P({w_{t + 1}}|\overrightarrow { {m_t}} ))} \\\overleftarrow E  =  - \sum\limits_{t = 2}^T {\log (P({w_{t - 1}}|\overleftarrow { {m_t}} ))} \\E = E + \gamma (\overrightarrow E  + \overleftarrow E )\end{array}\]</p><h3 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h3><p><img src="4.png" alt><br><img src="5.png" alt></p><h3 id="Language-Models-Added-1"><a href="#Language-Models-Added-1" class="headerlink" title="Language Models Added"></a>Language Models Added</h3><p><img src="6.png" alt></p><h3 id="Bidirectional-Language-Model"><a href="#Bidirectional-Language-Model" class="headerlink" title="Bidirectional Language Model"></a>Bidirectional Language Model</h3><p>\[\begin{array}{l}h_k^{LM} = [\overrightarrow {h_k^{LM}} ;\overleftarrow {h_k^{LM}} ]\\{h_{k,1}} = [\overrightarrow { {h_{k,1}}} ;\overleftarrow { {h_{k,1}}} ;h_k^{LM}]\end{array}\]<br><strong>Alternative</strong></p><ul><li>Replace $[\overrightarrow { {h_{k,1}}} ;\overleftarrow { {h_{k,1}}} ;h_k^{LM}]$ with $f([\overrightarrow { {h_{k,1}}} ;\overleftarrow { {h_{k,1}}} ;h_k^{LM}])$.</li><li>Concatenate the LM embeddings at different locations in the baseline sequence tagger.</li><li>Decrease the number of parameters in the second RNN layer.</li></ul><h3 id="Results-2"><a href="#Results-2" class="headerlink" title="Results"></a>Results</h3><p><img src="7.png" alt><br><img src="8.png" alt></p><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>The language model transfer across domains.</li><li>The model is robust even when trained on a large number of labeled data.</li><li>Training the sequence tagging model and language model together increases performance.</li></ul><h3 id="Graph-based"><a href="#Graph-based" class="headerlink" title="Graph-based"></a>Graph-based</h3><ul><li>Steps<ul><li>Construct a graph of tokens based on their semantic similarity.</li><li>Use the CRF marginal as a regularization term to do label propagation on the graph.</li><li>The smoothed posterior is then used to either interpolate with the CRF marginal or as an additional feature to the neural network.</li></ul></li><li>Graph Construction<ul><li>${w_{uv}} = {d_e}(u,v)$ if $v \in K(u)$ or $u \in K(v)$.</li></ul></li><li>Label Propagation<br><img src="9.png" alt></li><li>Uncertain Label Marginalizing<br>\[\mathcal{Y}({x_t}) = \left\{ {\begin{array}{*{20}{c}}{\{ {y_t}\} }&amp;{ {\rm{if \ }}p({y_t}|x;\theta ) &gt; \eta }\\{ {\rm{All \ label \ types}}}&amp;{ {\rm{otherwise}}}\end{array}} \right.\]</li><li>Score<br>\[\phi (y;x,\theta ) = \sum\limits_{t = 0}^n { {T_{ {y_t},{y_{t + 1}}}}}  + \sum\limits_{t = 1}^n { {P_{t,{y_t}}}} \]</li><li>Probability<br>\[{p_\theta }(\mathcal{Y}({x^k})|{x^k}) = \frac{ {\sum\nolimits_{ {y^k} \in \mathcal{Y}({x^k})} {\exp (\phi ({y^k};{x^k},\theta ))} }}{ {\sum\nolimits_{y’ \in Y} {\exp (\phi (y’;x,\theta ))} }}\]<br><img src="10.png" alt></li></ul><h3 id="Results-3"><a href="#Results-3" class="headerlink" title="Results"></a>Results</h3><p><img src="11.png" alt></p><h3 id="Conclusions-1"><a href="#Conclusions-1" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>In-domain data performs better than cross-domain data.</li><li>The combination of in-domain data and ULM algorithms performs well.</li><li>We can add language models into the model in the future to capture the context information.</li></ul><h1 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h1><hr><h3 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h3><p><strong>Cross-domain Transfer</strong></p><ul><li>Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks. Yang et al. ICLR17.</li><li>Improving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learning. Peng et al. ACL16.</li><li>Multi-task Domain Adaptation for Sequence Tagging. Peng et al. Workshop17.</li></ul><p><strong>Cross-lingual Transfer</strong></p><ul><li>Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks. Yang et al. ICLR17.</li><li>Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources. Kim et al. EMNLP17.</li></ul><h3 id="Cross-domain-Transfer"><a href="#Cross-domain-Transfer" class="headerlink" title="Cross-domain Transfer"></a>Cross-domain Transfer</h3><ul><li>Label mapping exist<br><img src="13.png" alt></li><li>Disparate label sets<br><img src="12.png" alt></li></ul><p><img src="20.png" alt><br><img src="15.png" alt></p><h3 id="Domain-Projections"><a href="#Domain-Projections" class="headerlink" title="Domain Projections"></a>Domain Projections</h3><ul><li>Domain Masks<br>\[\begin{array}{l}{m_1} = [\overrightarrow 1 ,\overrightarrow 1 ,\overrightarrow 0 ],{m_2} = [\overrightarrow 1 ,\overrightarrow 0 ,\overrightarrow 1 ]\\\hat h = {m_d} \odot h\end{array}\]</li><li>Linear Projection<br>\[\hat h = {T_d}h\]</li></ul><h3 id="Results-4"><a href="#Results-4" class="headerlink" title="Results"></a>Results</h3><p><img src="16.png" alt></p><h3 id="Conclusions-2"><a href="#Conclusions-2" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>Multi-task learning can help domain adaptation.</li><li>The number of shared parameters has great impact on the performance.</li><li>We may use other domain adaptation methods besides parameter sharing and representation learning.</li></ul><h3 id="Cross-lingual-Transfer"><a href="#Cross-lingual-Transfer" class="headerlink" title="Cross-lingual Transfer"></a>Cross-lingual Transfer</h3><p><img src="17.png" alt><br><img src="18.png" alt></p><ul><li>Sequence Tagging Loss<br>\[{\mathcal{L}_p} =  - \sum\limits_{i = 1}^S {\sum\limits_{j = 1}^N { {p_{i,j}}\log ({ {\hat p}_{i,j}})} }\]</li><li>Language Classifier Loss<br>\[{\mathcal{L}_a} =  - \sum\limits_{i = 1}^S { {l_i}\log ({ {\hat l}_i})}\]</li><li>Bidirectional Language Model Loss<br>\[{\mathcal{L}_l} =  - \sum\limits_{i = 1}^S {\sum\limits_{j = 1}^N {\log (P({w_{j + 1}}|{f_j})) + \log (P({w_{j - 1}}|{b_j}))} }\]</li><li>Total Loss<br>\[\mathcal{L} = {w_s}({\mathcal{L}_p} + \lambda {\mathcal{L}_a} + \lambda {\mathcal{L}_l})\]</li></ul><h3 id="Results-5"><a href="#Results-5" class="headerlink" title="Results"></a>Results</h3><p><img src="19.png" alt></p><h3 id="Conclusions-3"><a href="#Conclusions-3" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>The language classifier can train the common LSTM to be language-agnostic.</li><li>Either too many or too little labeled data decrease the performance.</li><li>Multiple source languages can be used to increase the performance.</li></ul><h1 id="Conclusions-4"><a href="#Conclusions-4" class="headerlink" title="Conclusions"></a>Conclusions</h1><hr><h3 id="Semi-supervised-Learning-vs-Transfer-Learning"><a href="#Semi-supervised-Learning-vs-Transfer-Learning" class="headerlink" title="Semi-supervised Learning vs Transfer Learning"></a>Semi-supervised Learning vs Transfer Learning</h3><ul><li>It seems that semi-supervised learning is better than transfer learning on some tasks.</li><li>Semi-supervised learning is not always useful for the lack of unlabeled data in the same domain.</li><li>Andrew Ng had said that transfer learning is an important research direction in the next five years.</li></ul><h3 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h3><ul><li>Semi-supervised learning and transfer learning can be combined to increase performance.</li><li>Other methods like active learning can be added.</li></ul><h1 id="References-2"><a href="#References-2" class="headerlink" title="References"></a>References</h1><hr><p>Xuezhe Ma and Eduard Hovy. (2016).<br><strong>End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.</strong><br><em>In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1064–1074, Berlin, Germany, August 7-12, 2016.</em></p><p>Marek Rei. (2017).<br><strong>Semi-supervised Multitask Learning for Sequence Labeling.</strong><br><em>In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2121–2130, Vancouver, Canada, July 30 - August 4, 2017.</em></p><p>Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power. (2017).<br><strong>Semi-supervised Sequence Tagging with Bidirectional Language Models.</strong><br><em>In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1756–1765, Vancouver, Canada, July 30 - August 4, 2017.</em></p><p>Yi Luan, Mari Ostendorf, Hannaneh Hajishirzi. (2017).<br><strong>Scientific Information Extraction with Semi-supervised Neural Tagging.</strong><br><em>In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2631–2641, Copenhagen, Denmark, September 7–11, 2017.</em></p><p>Zhilin Yang, Ruslan Salakhutdinov, William W. Cohen. (2017).<br><strong>Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks.</strong><br><em>In ICLR 2017.</em></p><p>Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, Eric Fosler-Lussier. (2017).<br><strong>Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources.</strong><br><em>In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2822–2828, Copenhagen, Denmark, September 7–11, 2017.</em></p><p>Nanyun Peng, Mark Dredze. (2017).<br><strong>Multi-task Domain Adaptation for Sequence Tagging.</strong><br><em>In Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 91–100, Vancouver, Canada, August 3, 2017.</em></p><p>Amarnag Subramanya, Slav Petrov, Fernando Pereira. (2010).<br><strong>Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models.</strong><br><em>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 167–176, MIT, Massachusetts, USA, 9-11 October 2010.</em></p><p>Yuzong Liu, Katrin Kirchhoff. (2014).<br><strong>Graph-based Semi-supervised Acoustic Modeling in DNN-based Speech Recognition.</strong><br><em>In IEEE SLT 2014.</em></p><p>Nanyun Peng, Mark Dredze. (2016).<br><strong>Improving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learning.</strong><br><em>In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 149–155, Berlin, Germany, August 7-12, 2016.</em></p>]]></content>
      
      
      <categories>
          
          <category> 序列标注 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 序列标注 </tag>
            
            <tag> 迁移学习 </tag>
            
            <tag> 半监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>华东师范大学LaTeX幻灯片模板</title>
      <link href="/2017/12/29/ecnu-ppt/"/>
      <url>/2017/12/29/ecnu-ppt/</url>
      
        <content type="html"><![CDATA[<p>分享一套自用ppt模板，使用 <code>XeLaTeX</code> 编译<br>链接地址：<a href="https://github.com/godweiyang/ECNU_BeamerTemplate" target="_blank" rel="noopener">传送门</a></p><h1 id="界面示例"><a href="#界面示例" class="headerlink" title="界面示例"></a>界面示例</h1><p><img src="1.png" alt><br>觉得还可以的下载了用哦，欢迎修改的更美观！</p>]]></content>
      
      
      <categories>
          
          <category> 模板 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LaTeX </tag>
            
            <tag> Beamer </tag>
            
            <tag> ppt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scientific Information Extraction with Semi-supervised Neural Tagging</title>
      <link href="/2017/12/19/emnlp17-2/"/>
      <url>/2017/12/19/emnlp17-2/</url>
      
        <content type="html"><![CDATA[<p>论文链接：<a href="https://www.aclweb.org/anthology/D/D17/D17-1279.pdf" target="_blank" rel="noopener">D17-1279</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>这篇论文研究的是科技论文的信息抽取问题，比如给你一篇paper，你要找出其中的Task（任务）、Process（过程方法）、Material（资料数据）三种实体。<br>这个问题可以归类为序列标注问题，但是科技论文的标注数据还是很少的，于是本文提出了一种基于图的半监督序列标注算法。</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><hr><p>背景就是近几年来论文发表数量越来越多，要对这些论文进行信息抽取，概括这篇论文的摘要。<br>由于标注过的论文很少，大量论文那都是无标注的，所以需要一个半监督的方法来进行序列标注。本文有三大贡献：一是结果比以往的更好啦，二是提出一种半监督序列标注方法，使用基于图的标签传播和可信度数据选择，三是探索了不同的利用无标注数据的方法，比如无监督的表示初始化和半监督的模型训练。</p><p>以往的工作大多是基于迁移学习的，本文提出的模型比他们结果都要好（ps.迁移学习和半监督方法都说自己最好。。。）</p><h1 id="2-模型"><a href="#2-模型" class="headerlink" title="2 模型"></a>2 模型</h1><hr><p>基本的模型就不用说了，就是传统的CNN+LSTM+CRF序列标注模型。<br>本文的重点就在于无标注数据的使用上，采用了基于图的标签传播算法，来对无标注数据进行标注，并添加到序列标注模型中进行训练。</p><h1 id="3-半监督学习"><a href="#3-半监督学习" class="headerlink" title="3 半监督学习"></a>3 半监督学习</h1><hr><p>具体是怎么标注的呢？<br>概括起来就是首先计算出每个无标注数据的后验概率，然后改进基本序列标注模型中的CRF，使它能够考虑到标签的不确定性。</p><h3 id="基于图的后验估计"><a href="#基于图的后验估计" class="headerlink" title="基于图的后验估计"></a>基于图的后验估计</h3><p>估计后验概率方法如下：<br>首先基于词的语义相似度构造出一个图，然后使用CRF边际函数作为正则化因子在图上进行标签传播，最后应用到神经网络中。</p><h4 id="图的构造"><a href="#图的构造" class="headerlink" title="图的构造"></a>图的构造</h4><p>图中的结点代表单词，边代表词之间的语义相似性。整个图的结点数量等于标注数据和未标注数据总单词数。<br>单词的表示是用前后共5个单词的词向量、和他最近的动词的词向量、一组离散特征例如词性和大写连接而成，然后用PCA降维到100维。<br>定义两个结点之间的边权重等于欧几里得距离，如果两个点中至少一个点在另一个点的K近邻里。</p>]]></content>
      
      
      <categories>
          
          <category> 序列标注 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 序列标注 </tag>
            
            <tag> EMNLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources</title>
      <link href="/2017/12/19/emnlp17-1/"/>
      <url>/2017/12/19/emnlp17-1/</url>
      
        <content type="html"><![CDATA[<p>论文链接：<a href="https://www.aclweb.org/anthology/D/D17/D17-1302.pdf" target="_blank" rel="noopener">D17-1302</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>本文介绍了一个跨语言的序列标注迁移模型，和以往不同的是，不需要大量的跨语言语料。</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><hr><p>之前正好才看过一篇Yang et al. (2017)的论文Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks，还写了博客笔记<a href="http://godweiyang.com/2017/11/13/ICLR17-1/">[传送门]</a>。然而本文上来就说，他的模型不好！因为他的模型共享了字符层，其他层都是独立的，这样就导致了两种字符没有交集的语言迁移会很困难。<br>于是本文提出了基于一个公有LSTM和一个私有LSTM的模型。不过，我还是没明白这个字符层和前文批判的那个有啥区别。。。</p><h1 id="2-模型"><a href="#2-模型" class="headerlink" title="2 模型"></a>2 模型</h1><hr><p>模型结构如下图所示：<br><img src="1.png" alt></p><h3 id="跨语言训练"><a href="#跨语言训练" class="headerlink" title="跨语言训练"></a>跨语言训练</h3><p>整体训练过程是这样的：首先输入一个句子，对于每个单词中的每个字符，输入到双向LSTM中，取前向后向LSTM的最后一个隐含层输出，连接到一起来作为词的表示。然后输入到公有和私有LSTM中训练，最后的损失函数有三个。<br>图中蓝色部分是公有部分，红色部分是私有部分。紫色部分是不同语言预测出的结果，三个红色方框是三个损失函数。<br>其中softmax层的损失函数定义为：<br><img src="2.png" alt><br>$S$是句子个数，$N$是当前句子单词数，${p_{i,j}}$第$i$个句子的第$j$个标签，${\hat p_{i,j}}$是预测标签。</p><h3 id="语言对抗训练"><a href="#语言对抗训练" class="headerlink" title="语言对抗训练"></a>语言对抗训练</h3><p>为了让公有的LSTM部分变得语言无关，所以要对他进行语言对抗训练。<br>首先对公有LSTM的输出进行CNN/MaxPool编码（其实就是每一个维度取最大值），三个卷积层，得到三个向量，再连接到一起得到最终的向量表示，经过一个梯度反转层，最后输入到语言鉴别器。<br>这个语言鉴别器就是一个单隐含层的全连接的神经网络，激活函数用的是Leaky ReLU，具体就是当x&gt;=0时，f(x)=x；当x&lt;0时，f(x)=ax，这里a取0.2。<br>最后得到的损失函数定义为：<br><img src="3.png" alt><br>其中${l_{i}}$是第i个句子的语言，${\hat l_{i}}$是预测出的语言（原文说的是softmax输出的标签？？？这里不是很理解）。<br>由于经过了梯度反转层，所以这个语言鉴别器并不能鉴别出是哪一种语言，反而对两种语言的界限混淆了，使得它变得越来越语言无关了。</p><h3 id="双向语言模型"><a href="#双向语言模型" class="headerlink" title="双向语言模型"></a>双向语言模型</h3><p>这个就不用多说了，很多地方都用到了，之前的两篇半监督序列标注的论文也都提到了加入语言模型来提升效果，而且注意的是，就算是没有标注的数据也可以加入进语言模型的训练。<br>语言模型的损失函数定义为：<br><img src="4.png" alt><br>其中f和b分别代表前向和后向LSTM输出。<br>最终，三个损失函数合并起来定义一个总的损失函数：<br><img src="5.png" alt><br>其中$\lambda $随着训练进行缓慢的从0上升到1，这是为了增加额外损失函数的训练稳定性。<br>而$w_{s}$是为了给源语言和目标语言不同的权重的，当训练目标语言是，权重为1，否则权重为目标语言训练集大小/源语言训练集大小。</p><h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h1><p>实验针对目标语言训练句子数为1280、320和32分别做了对比。<br>其中1280个句子的结果如下：<br><img src="6.png" alt><br>可以看出加入迁移学习后结果提升还是比较大的，表格中c表示公有LSTM，l表示语言模型，p表示私有LSTM，a表示语言对抗训练。本文的模型(c,p,l+a)结果是最好的。<br>320个句子的结果如下：<br><img src="7.png" alt><br>依然有提升，但是提升很不明显了，而且有几种语言直接采用私有LSTM+语言模型效果反而最好。原因就是目标语言的训练数据太少了，不足以训练出完美的公有和私有模型。<br>32个句子的训练数据就没有放出具体结果，结果依然有略微提升的，但是已经可以忽略不计了。<br>反过来思考，如果用上全部的目标语言训练数据，那么出来的结果会怎么样？事实是反而下降了，还不如直接用私有LSTM。<br>最后还做了一个实验，就是用多种不同的源语言+目标语言来训练模型，发现效果比用单种源语言效果好。</p>]]></content>
      
      
      <categories>
          
          <category> 序列标注 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 序列标注 </tag>
            
            <tag> EMNLP </tag>
            
            <tag> 迁移学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sequence Tagging with Little Labeled Data</title>
      <link href="/2017/11/29/textminning/"/>
      <url>/2017/11/29/textminning/</url>
      
        <content type="html"><![CDATA[<p>文本挖掘课presentation还有一个多月了，依然很迷茫，不知道选什么课题。<br>最近看了一些序列标注相关的paper，暂且就准备挑一个相关的点做了，打算做一个“基于少量标注数据的序列标注”。</p><h1 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h1><hr><p>基于少量标注数据的序列标注，主要有两种方法：迁移学习和半监督学习。<br>代表性的paper分别有：<br><a href="https://arxiv.org/pdf/1703.06345.pdf" target="_blank" rel="noopener">Yang Z, Salakhutdinov R, Cohen W W. Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks. ICLR, 2017.</a><br><a href="https://arxiv.org/pdf/1705.00108.pdf" target="_blank" rel="noopener">Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power. Semi-supervised sequence tagging with bidirectional language models. ACL, 2017.</a><br>具体该讲些什么，该怎么讲，等我慢慢补充吧。<br>其中传统的半监督序列标注模型有基于HMM之类的，这里就不说了。<br>下面说说几个基于双向LSTM的。</p><h3 id="Marek-Rei-Semi-supervised-Multitask-Learning-for-Sequence-Labeling-ACL-2017"><a href="#Marek-Rei-Semi-supervised-Multitask-Learning-for-Sequence-Labeling-ACL-2017" class="headerlink" title="Marek Rei. Semi-supervised Multitask Learning for Sequence Labeling. ACL, 2017."></a>Marek Rei. <em>Semi-supervised Multitask Learning for Sequence Labeling. ACL, 2017.</em></h3><p><img src="1.png" alt><br>这篇论文介绍了一个附加了语言模型的LSTM序列标注模型。<br>就是在传统的序列标注模型隐含层输出上额外附加了一层语言模型输出，总的损失函数也加上了语言模型的损失函数，共同训练。<br><img src="2.png" alt><br><img src="3.png" alt><br><img src="4.png" alt><br><img src="5.png" alt><br>其中E就是原始序列标注模型的损失。<br>当然了，这个模型不需要额外的未标注数据，只要用到少量的标注数据就行了。</p><h3 id="Matthew-E-Peters-Waleed-Ammar-Chandra-Bhagavatula-Russell-Power-Semi-supervised-sequence-tagging-with-bidirectional-language-models-ACL-2017"><a href="#Matthew-E-Peters-Waleed-Ammar-Chandra-Bhagavatula-Russell-Power-Semi-supervised-sequence-tagging-with-bidirectional-language-models-ACL-2017" class="headerlink" title="Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power. Semi-supervised sequence tagging with bidirectional language models. ACL, 2017."></a>Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power. <em>Semi-supervised sequence tagging with bidirectional language models. ACL, 2017.</em></h3><p>这篇论文就是半监督序列标注模型中我看到的效果最好的一个了。<br>和上面一篇相同的是，都加入了语言模型，来对单词上下文信息进行编码。但是不同的是，上文将语言模型和序列标注模型融合在了一起，所以只需要少量的标注数据就行了，无法利用大量的无标注数据。这篇的模型将其分开，预先对大量的无标注数据训练语言模型，然后将训练好的词表示加入到序列标注模型中，和原始的词向量结合，然后进行训练。<br>模型结构如下所示：<br><img src="6.png" alt></p><h3 id="Yi-Luan-Mari-Ostendorf-Hannaneh-Hajishirzi-Scientific-Information-Extraction-with-Semi-supervised-Neural-Tagging-EMNLP-2017"><a href="#Yi-Luan-Mari-Ostendorf-Hannaneh-Hajishirzi-Scientific-Information-Extraction-with-Semi-supervised-Neural-Tagging-EMNLP-2017" class="headerlink" title="Yi Luan, Mari Ostendorf, Hannaneh Hajishirzi. Scientific Information Extraction with Semi-supervised Neural Tagging. EMNLP, 2017."></a>Yi Luan, Mari Ostendorf, Hannaneh Hajishirzi. <em>Scientific Information Extraction with Semi-supervised Neural Tagging. EMNLP, 2017.</em></h3><p>这篇论文任务是科技文章的关键词提取，然后分类为task、process、material三类，可以将其看成一个序列标注任务。<br>这篇论文介绍了一种基于图的半监督方法</p>]]></content>
      
      
      <categories>
          
          <category> 序列标注 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 序列标注 </tag>
            
            <tag> 迁移学习 </tag>
            
            <tag> 半监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编译原理实践期末大作业</title>
      <link href="/2017/11/16/sw-compiler/"/>
      <url>/2017/11/16/sw-compiler/</url>
      
        <content type="html"><![CDATA[<p>这学期编译原理实践期末大作业是编写一个简化swift语法编译器，代码等到学期结束再上传百度云，现在先上传一个测试运行程序，bug很多。<br><a href="https://pan.baidu.com/s/1jHDbtcy" target="_blank" rel="noopener">程序地址</a>，密码是cnfj，在大四上文件夹下。<br>主界面：<br><img src="1.png" alt><br>支持常用编辑以及编译运行和单步调试查看数据栈。<br>目前暂时只做了跳过注释、mod、odd、自增、自减、常量定义、repeat这几个扩展点，其他再说吧，做的很烂，及格就行。</p>]]></content>
      
      
      <categories>
          
          <category> 程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编译原理 </tag>
            
            <tag> 编译器 </tag>
            
            <tag> C# </tag>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks</title>
      <link href="/2017/11/13/iclr17-1/"/>
      <url>/2017/11/13/iclr17-1/</url>
      
        <content type="html"><![CDATA[<p>论文链接：<a href="https://arxiv.org/pdf/1703.06345.pdf" target="_blank" rel="noopener">链接</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>探索一种神经序列标注的迁移学习方法，适用于源任务有大量标注标签，但是目标任务标注标签很少的情况，主要有三个方面：跨领域、跨应用、跨语言。</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><hr><p>序列标注任务的一个难点就是怎样将知识从一个任务迁移到另一个任务上面去，通常被叫做“迁移学习”。<br>这里对三种任务（跨领域、跨应用、跨语言）分别提出了参数共享的神经网络结构，实验结果也很好。即使标注标签很多，结果也比以前的结果更好。</p><h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h1><hr><p>NLP迁移学习通常有两种方法，一种是基于资源的迁移，另一种是基于模型的迁移。<br>基于资源的迁移需要大量的跨语言语料，而且通常只用在跨语言任务了，在跨领域和跨应用方面还没什么应用。<br>基于模型的迁移就不需要大量的额外资源了，只要探索源任务和目标任务之间的相似性和相关性，通过修改模型结构、训练算法、特征表示。<br>这里的模型就是基于模型的迁移。和以往不同的是，这个模型利用了深度递归神经网之间的一般性，可以在跨领域、跨应用、跨语言之间转换。</p><h1 id="3-方法"><a href="#3-方法" class="headerlink" title="3 方法"></a>3 方法</h1><hr><p><img src="1.png" alt><br>上图分别是基本模型、跨领域模型、跨应用模型、跨语言模型。</p><h3 id="3-1-基本模型"><a href="#3-1-基本模型" class="headerlink" title="3.1 基本模型"></a>3.1 基本模型</h3><p>首先字符层输入每个字符表示，输出单词的字符层表示，然后和词向量结合作为每个词的表示，然后经过一个CNN或者RNN，隐含层输出再经过一个CRF得到每个词的标签。</p><h3 id="3-2-迁移学习结构"><a href="#3-2-迁移学习结构" class="headerlink" title="3.2 迁移学习结构"></a>3.2 迁移学习结构</h3><p>跨领域、跨应用、跨语言每种一个迁移结构，和基本模型不同的是，共享的参数不同。</p><h4 id="3-2-1-跨领域迁移"><a href="#3-2-1-跨领域迁移" class="headerlink" title="3.2.1 跨领域迁移"></a>3.2.1 跨领域迁移</h4><p>跨领域迁移有两种情况，一种是两个领域标签存在映射关系，一种是不存在映射关系。<br>第一种情况结构如图b所示，可以共享CRF层以及下面的所有层，最后加一个标签映射的步骤就行了<br>第二种情况结构如图c所示，CRF层独立训练。</p><h4 id="3-2-2-跨应用迁移"><a href="#3-2-2-跨应用迁移" class="headerlink" title="3.2.2 跨应用迁移"></a>3.2.2 跨应用迁移</h4><p>这里假设两种应用是同一种语言，所以和跨领域标签无映射关系类似，用的是图c的结构。</p><h4 id="3-2-3-跨语言迁移"><a href="#3-2-3-跨语言迁移" class="headerlink" title="3.2.3 跨语言迁移"></a>3.2.3 跨语言迁移</h4><p>这里重点放在两种字母表相同的语言上，比如英语和西班牙语。<br>由于字符相同，所以采用图d结构，共享字符层和词向量。</p><h3 id="3-3-训练"><a href="#3-3-训练" class="headerlink" title="3.3 训练"></a>3.3 训练</h3><p>假设从任务$s$迁移到任务$t$，训练集分为$X_s$和$X_t$，参数分为$W_s$和$W_t$。其中模型参数还分为任务特定参数和共享参数：<br>\[{W_s} = {W_{s,spec}} \cup {W_{shared}},{W_t} = {W_{t,spec}} \cup {W_{shared}}\]训练过程如下：<br>每一次迭代，都采用二项分布随机抽取一个任务$s$或$t$，然后训练该任务特定模型参数和共享参数。由于源任务和目标任务收敛速度不一定一样，所以对目标任务提前停止。</p><h3 id="3-4-模型实现"><a href="#3-4-模型实现" class="headerlink" title="3.4 模型实现"></a>3.4 模型实现</h3><p>RNN采用GRU，假设隐含层输出为$h$，输出标签为$y$，定义CRF的目标函数为<br>\[f(h,y) - \log \sum\limits_{y’ \in \gamma (h)} {\exp (f(h,y’) + \cos {\rm{t}}(y,y’))} \]</p><h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h1><p><img src="2.png" alt><br><img src="3.png" alt><br>上图是实验结果，其中跨领域迁移：ade，跨应用迁移：fgh，跨语言迁移：ij，跨领域和应用：b，跨领域、应用和语言：c。<br><img src="4.png" alt><br>上表是各类任务在不同标签率下迁移学习和无迁移学习的性能对比。<br>可以看出三种迁移结构性能提升的顺序是A&gt;B&gt;C，因为结构C比如跨语言模型，源任务和目标任务相似性很小。<br><img src="5.png" alt><br>上图是和其他模型比较的结果。</p><h1 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h1><hr><p>提出一种神经序列标注的迁移学习方法，主要有三个方面：跨领域、跨应用、跨语言。<br>以下三个因素对迁移学习性能有很大影响：目标任务标签数量、源任务和目标任务相关性、能够共享的参数数量。<br>在以后的工作中，可以在跨语言迁移学习中尝试结合基于模型的迁移和基于资源的迁移。</p>]]></content>
      
      
      <categories>
          
          <category> 序列标注 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 序列标注 </tag>
            
            <tag> ICLR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Implicitly-Defined Neural Networks for Sequence Labeling</title>
      <link href="/2017/10/25/acl17-2027/"/>
      <url>/2017/10/25/acl17-2027/</url>
      
        <content type="html"><![CDATA[<p>论文链接：<a href="https://www.aclweb.org/anthology/P/P17/P17-2027.pdf" target="_blank" rel="noopener">P17-2027</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>这篇论文介绍了一种新奇的、隐式定义神经网络，并且描述了计算它的方法。</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><hr><p>传统的双向RNN只能单独计算两个方向的隐含层，现在介绍一种新的机制，将两个方向的信息直接结合起来计算。</p><h1 id="2-INN"><a href="#2-INN" class="headerlink" title="2 INN"></a>2 INN</h1><hr><h3 id="2-1-传统的RNN"><a href="#2-1-传统的RNN" class="headerlink" title="2.1 传统的RNN"></a>2.1 传统的RNN</h3><p>经典的RNN给定一个输入序列$[{\xi _1},{\xi _2}, \ldots ,{\xi _n}]$和初始隐含层状态${h_s}$，然后迭代产生后续的隐含层状态：<br>\[\begin{array}{l}{h_1} = f({\xi _1},{h_s})\\{h_2} = f({\xi _2},{h_1})\\ \cdots \\{h_n} = f({\xi _n},{h_{n - 1}})\end{array}\]LSTM、GRU和其他的相关变体计算方法也都类似，都是像下图这样线性计算，每一时刻的状态只依赖于当前输入和前一时刻的状态。<br><img src="1.png" alt></p><h3 id="2-2-改进结构"><a href="#2-2-改进结构" class="headerlink" title="2.2 改进结构"></a>2.2 改进结构</h3><p>这篇论文中这样计算隐含层状态：<br>\[{h_t} = f({\xi _t},{h_{t - 1}},{h_{t + 1}})\]这样整个隐含层状态序列的等式就是隐式的，记为：<br>\[H = [{h_1},{h_2}, \ldots ,{h_n}]\]在这个神经网络中，定义如下变量：数据$X$、标签$Y$、参数$\theta$，定义如下函数：<br>输入层变换：<br>\[\xi  = g(\theta ,X)\]隐式隐含层：<br>\[H = F(\theta ,\xi ,H)\]损失函数：<br>\[L = \ell (\theta ,H,Y)\]定义${h_s}$和${h_e}$为边界状态，$n$为输入序列长度，$F$函数构造出了一系列非线性等式：<br>\[\begin{array}{l}{h_1} = f({h_s},{h_2},{\xi _1})\\ \cdots \\{h_i} = f({h_{i - 1}},{h_{i + 1}},{\xi _i})\\ \cdots \\{h_n} = f({h_{n - 1}},{h_e},{\xi _n})\end{array}\]INN结构如下图：<br><img src="2.png" alt></p><h3 id="2-3-计算前向传播"><a href="#2-3-计算前向传播" class="headerlink" title="2.3 计算前向传播"></a>2.3 计算前向传播</h3><p>为了计算等式$H = F(H)$，采用拟牛顿法。<br>令$G = H - F(H)$，转化为计算等式$G = 0$。<br>\[\begin{array}{l}{H_{n + 1}} = {H_n} - {({\nabla _H}G)^{ - 1}}G\\{H_{n + 1}} = {H_n} - {(I - {\nabla _H}F)^{ - 1}}({H_n} - F({H_n}))\end{array}\]注意到$(I - {\nabla _H}F)$是一个稀疏矩阵，所以采用Krylov子空间方法，具体是稳定双共轭梯度法(BICG-STAB)算法来计算。</p><h3 id="2-4-梯度"><a href="#2-4-梯度" class="headerlink" title="2.4 梯度"></a>2.4 梯度</h3><p>为了训练模型，采用随机梯度下降，定义损失函数：<br>\[{\nabla _\theta }L = {\nabla _\theta }\ell  + {\nabla _H}\ell {\nabla _\theta }H\]其中<br>\[{\nabla _\theta }H = {\nabla _\theta }F + {\nabla _H}F{\nabla _\theta }H + {\nabla _\xi }F{\nabla _\theta }\xi \]所以<br>\[{\nabla _\theta }H = {(I - {\nabla _H}F)^{ - 1}}({\nabla _\theta }F + {\nabla _\xi }F{\nabla _\theta }\xi )\]所以整个梯度就是<br>\[{\nabla _\theta }L = {\nabla _\theta }\ell  + {\nabla _H}\ell {(I - {\nabla _H}F)^{ - 1}}({\nabla _\theta }F + {\nabla _\xi }F{\nabla _\theta }\xi )\]</p><h3 id="2-5-转换函数"><a href="#2-5-转换函数" class="headerlink" title="2.5 转换函数"></a>2.5 转换函数</h3><p>回忆在GRU中，有如下转换函数：<br>\[\begin{array}{l}{h_t} = (1 - {z_t}){ {\hat h}_t} + {z_t}{ {\tilde h}_t}\\{ {\tilde h}_t} = \tanh (W{x_t} + U({r_t}{ {\hat h}_t}) + \tilde b)\\{z_t} = \sigma ({W_z}{x_t} + {U_z}{ {\hat h}_t} + {b_z})\\{r_t} = \sigma ({W_r}{x_t} + {U_r}{ {\hat h}_t} + {b_r})\end{array}\]其中在GRU中${ {\hat h}_t} = {h_{t - 1}}$，在INN中做一个替代：<br>\[\begin{array}{l}{ {\hat h}_t} = s{h_{t - 1}} + (1 - s){h_{t + 1}}\\s = \frac{ { {s_p}}}{ { {s_p} + {s_n}}}\\{s_p} = \sigma ({W_p}{x_t} + {U_p}{h_{t - 1}} + {b_p})\\{s_n} = \sigma ({W_n}{x_t} + {U_n}{h_{t + 1}} + {b_n})\end{array}\]</p><h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h1><hr><h3 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h3><p>如下图所示，这个模型的效果甚至比标准的序列标注器还要好！<br><img src="3.png" alt></p><h1 id="4-结论"><a href="#4-结论" class="headerlink" title="4 结论"></a>4 结论</h1><hr><p>介绍了一种隐式定义神经网络，应用到了序列标注任务上，效果比双向LSTM、双向GRU等还要好。<br>还有一些工作可以改进，比如可以在双向LSTM上面改造INN，加速计算${(I - {\nabla _H}F)^{ - 1}}$等等。</p>]]></content>
      
      
      <categories>
          
          <category> 序列标注 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACL </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 序列标注 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semi-supervised sequence tagging with bidirectional language models</title>
      <link href="/2017/10/03/acl17-1161/"/>
      <url>/2017/10/03/acl17-1161/</url>
      
        <content type="html"><![CDATA[<p>论文链接：<a href="https://www.aclweb.org/anthology/P/P17/P17-1161.pdf" target="_blank" rel="noopener">P17-1161</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>从未标注文本中学习到的预训练词向量已经成为NLP任务神经网络结构的重要组成部分。<br>但是大多数情况下，现在的循环神经网络还是从极少的标注数据中学习上下文相关的表示。<br>所以这篇论文研究一种通用的半监督学习方法，将从双向语言模型中预训练出来的词向量加到NLP系统中，把它应用到序列标注任务中。<br>我们在两个NLP任务上做实验：NER和chunking。</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><hr><p>这篇论文我们探讨一种半监督学习方法，不需要额外的标注数据。<br>我们使用一个神经语言模型，在大量未标注数据上训练，计算出每个位置上下文的编码，然后应用到半监督的标注模型中。<br>我们第一个主要贡献是证明了语言模型训练出的上下文相关表示在半监督标注模型中是很有用的。<br>第二个贡献是多使用一个后向的语言模型效果更好。<br>同时我们发现没必要针对某个领域数据来专门训练。</p><h1 id="2-语言模型增强的序列标注"><a href="#2-语言模型增强的序列标注" class="headerlink" title="2 语言模型增强的序列标注"></a>2 语言模型增强的序列标注</h1><hr><h3 id="2-1-概览"><a href="#2-1-概览" class="headerlink" title="2.1 概览"></a>2.1 概览</h3><p>这个模型的主要结构如图所示：<br><img src="1.jpg" alt><br>主要过程可以分为3步：</p><ul><li>首先在大量的未标注数据上训练词向量和一个神经语言模型</li><li>然后提取一个句子中每个单词的词向量表示与语言模型表示</li><li>最后将它们应用到监督序列标注模型中。</li></ul><p>具体的结构如下图所示：<br><img src="2.jpg" alt></p><h3 id="2-2-基本的序列标注模型"><a href="#2-2-基本的序列标注模型" class="headerlink" title="2.2 基本的序列标注模型"></a>2.2 基本的序列标注模型</h3><p>我们用到的基本的序列标注模型是一个分层的神经序列标注模型，如上图左半部分所示。<br>给定一个句子$({t_1},{t_2}, \ldots ,{t_N})$，首先对于每个单词${t_k}$产生一个表示${x_k}$，其中${x_k}$是由这个单词基于字符的表示${c_k}$和词向量表示${w_k}$连接而成：<br>\[\begin{array}{l}{c_k} = C({t_k};{\theta _c})\\{w_k} = E({t_k};{\theta _w})\\{x_k} = [{c_k};{w_k}]\end{array}\]字符表示${c_k}$捕获的是这个单词的形态信息，可以用CNN或者RNN来实现。<br>词向量表示${w_k}$是从预训练的词向量表中直接提取的。<br>为了学习到上下文相关的表示，我们采用多层双向RNN。<br>对于每个单词${x_k}$，第${i}$层隐含层${h_{k,i}}$是由前向隐含层状态${ {\vec h}_{k,i}}$和后向隐含层状态${ {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over h} }_{k,i}}$连接而成。<br>对于第一层隐含层，${h_{k,1}}$由${x_k}$经过如下运算获得：<br>\[\begin{array}{l}{ {\vec h}_{k,1}} = { {\vec R}_1}({x_k},{ {\vec h}_{k - 1,1}};{\theta _{ { {\vec R}_1}}})\\{ {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over h} }_{k,1}} = { {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over R} }_1}({x_k},{ {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over h} }_{k + 1,1}};{\theta _{ { {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over R} }_1}}})\\{h_{k,1}} = [{ {\vec h}_{k,1}};{ {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over h} }_{k,1}}]\end{array}\]这个实验中我们使用2层隐含层，并且使用GRU或者LSTM作为双向RNN。<br>最后，最后一层隐含层的输出${h_{k,L}}$被输出到一个稠密层，用来预测每个标签的评分。<br>由于在我们的序列标注任务中，连续的标签之间是有依赖性的，所以联合起来预测一整个句子的标签比单独预测每个单词的标签更好。<br>因此，我们增加了额外的一层来计算相邻两个标签之间的条件随机场损失，然后用Viterbi算法来寻找概率最大的标签序列。</p><h3 id="2-3-双向语言模型"><a href="#2-3-双向语言模型" class="headerlink" title="2.3 双向语言模型"></a>2.3 双向语言模型</h3><p>一个语言模型是用来计算一个句子$({t_1},{t_2}, \ldots ,{t_N})$的概率：<br>\[p({t_1},{t_2}, \ldots ,{t_N}) = \prod\limits_{k - 1}^N {p({t_k}|{t_1},{t_2}, \ldots ,{t_{k - 1}})} \]之前的研究将每个单词的字符表示或者词向量表示送到多层LSTM中，用$({t_1},{t_2}, \ldots ,{t_k})$来求出隐含层${ {\vec h}^{LM}}_k$，这就是第$k$个单词的前向语言模型表示，同时也是语言模型LSTM层最顶端的输出。最后用softmax层来预测${t_{k + 1}}$的概率。<br>当然再加上一个后向语言模型表示效果就更好了：<br>\[p({t_1},{t_2}, \ldots ,{t_N}) = \prod\limits_{k - 1}^N {p({t_k}|{t_{k + 1}},{t_{k + 2}}, \ldots ,{t_N})} \]后向语言模型表示实现方式和前向相似，产生输出${ {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over h} }^{LM}}_k$<br>注意到在这个模型中，前向后向语言模型是独立的，不共享任何参数。</p><h3 id="2-4-结合语言模型和序列模型"><a href="#2-4-结合语言模型和序列模型" class="headerlink" title="2.4 结合语言模型和序列模型"></a>2.4 结合语言模型和序列模型</h3><p>我们结合的模型TagLM是将语言模型的词表示当作额外的输入传送到序列标注模型中。<br>在实验中，我们发现将语言模型表示和序列模型第一层隐含层输出结合效果最好。表示如下：<br>\[{h_{k,1}} = [{ {\vec h}_{k,1}};{ {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over h} }_{k,1}};{h_k}^{LM}]\]有许多方法可以结合语言模型表示和序列模型第一层隐含层输出，比如用一个非线性函数来结合：<br>\[{h_{k,1}} = f([{ {\vec h}_{k,1}};{ {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over h} }_{k,1}};{h_k}^{LM}])\]另一种可能的方法是用类似注意力模型的机制，给每个单词的语言模型表示加上权重，然后再加进序列模型中。<br>本次实验中直接结合效果已经很好了，所以没有尝试其他方法。</p><h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h1><hr><p>我们在两个NLP任务上做实验：NER和chunking，使用F1评价指标和BIOES标注体系。<br>我们对数据做了预处理，对所有字母小写处理，将所有数字替换成0。</p><h4 id="CoNLL-2003-NER"><a href="#CoNLL-2003-NER" class="headerlink" title="CoNLL 2003 NER"></a>CoNLL 2003 NER</h4><p>CoNLL 2003 NER任务包含了路透社RCV1语料库，它是由4种不同的实体类型标注的：PER、LOC、ORG、MISC，包含了标准的训练集、验证集和测试集。<br>我们的序列模型的字符表示使用了80个隐含层和25维字符表示的双向GRU。上面的序列层使用了两个300个隐含层的双向GRU。为了正则化，每个GRU的输入都添加了25%的dropout。</p><h4 id="CoNLL-2000-chunking"><a href="#CoNLL-2000-chunking" class="headerlink" title="CoNLL 2000 chunking"></a>CoNLL 2000 chunking</h4><p>CoNLL 2000 chunking任务使用华尔街日报第15~18章训练，第20章测试。定义了11种句法分块类型，我们从训练集随机标记出1000个句子作为验证集。<br>序列模型字符表示使用了30维字符表示和带有30个宽度为3字符滤波器的CNN。上面的序列层使用了两个200个隐含层的双向GRU。每个GRU的输入都添加了50%的dropout。</p><h4 id="预训练语言模型"><a href="#预训练语言模型" class="headerlink" title="预训练语言模型"></a>预训练语言模型</h4><p>我们在1B Word Benchmark上面训练语言模型，包含了8亿个单词。<br>我们使用两个2048个单元，512维的LSTM，在4个GPU上进行参数的同步更新，在10轮训练后就停止训练。</p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>所有的实验都采用Adam优化器在5.0处进行梯度截断。<br>提前结束训练来防止过拟合，采用以下方法来决定什么时候停止训练：开始时学习率设为0.001，观察验证集每一轮的性能，当验证集上的性能达到最高时，将学习率降低一个数量级，再训练5次，再降低一个数量级，再训练5次，最后停止训练。</p><h3 id="3-1-综合系统结果"><a href="#3-1-综合系统结果" class="headerlink" title="3.1 综合系统结果"></a>3.1 综合系统结果</h3><p>表1和表2比较的是TagLM和其他没有额外标注数据的模型结果。<br>表3和表4比较的是TagLM和其他包含额外标注数据的模型结果。<br><img src="3.jpg" alt><br><img src="4.jpg" alt></p><ul><li>增加外部标注数据<br>尽管我们没有使用外部标注数据，但是我们效果依然比其他模型要好。表3和表4还可以看出这个模型加了语言模型后的提升是最大的。</li></ul><h3 id="3-2-分析"><a href="#3-2-分析" class="headerlink" title="3.2 分析"></a>3.2 分析</h3><p>为了解释我们的TagLM的特性，我们在CoNLL 2003 NER上做了许多额外的实验。</p><h4 id="怎样使用语言模型表示？"><a href="#怎样使用语言模型表示？" class="headerlink" title="怎样使用语言模型表示？"></a>怎样使用语言模型表示？</h4><p>在这个实验中，我们将语言模型产生的表示连接到序列模型的不同位置：</p><ul><li>连接到第一个RNN的输入层：<br>\[{x_k} = [{c_k};{w_k};{h_k}^{LM}]\]</li><li>连接到第一个RNN的输出层：<br>\[{h_{k,1}} = [{ {\vec h}_{k,1}};{ {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over h} }_{k,1}};{h_k}^{LM}]\]</li><li>连接到第二个RNN的输出层：<br>\[{h_{k,2}} = [{ {\vec h}_{k,2}};{ {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\leftarrow$}} \over h} }_{k,2}};{h_k}^{LM}]\]</li></ul><p>表5显示出第二种位置是效果最好的，我们猜测原因可能是因为第二层RNN可以捕获第一层RNN产生的任务特定的上下文和语言模型产生的通用的上下文之间的互相联系。<br><img src="5.jpg" alt></p><h4 id="用哪一种语言模型重要吗？"><a href="#用哪一种语言模型重要吗？" class="headerlink" title="用哪一种语言模型重要吗？"></a>用哪一种语言模型重要吗？</h4><p>从表6可以看出，前向传播使用CNN-BIG-LSTM，后向传播使用LSTM-2048-512效果是最好的，但是我们没有测试后向也是CNN-BIG-LSTM的，那样效果估计会更好。<br><img src="6.jpg" alt></p><h4 id="任务特定RNN的重要性"><a href="#任务特定RNN的重要性" class="headerlink" title="任务特定RNN的重要性"></a>任务特定RNN的重要性</h4><p>我们把任务特定的RNN去掉了，只用语言模型和稠密层和CRF来预测输出标签，结果非常的差。说明还是需要任务特定RNN来对标注数据编码产生必要的信息的。</p><h4 id="数据集的大小"><a href="#数据集的大小" class="headerlink" title="数据集的大小"></a>数据集的大小</h4><p>通过在大数据和小数据上做实验，得出如下结论：<br>以往的模型在小数据上从无语言模型到有语言模型提升都是很大的，但是在大数据上提升就非常的少了。<br>而我们的TagLM不论是小数据还是大数据性能提升都非常的大。</p><h4 id="参数个数"><a href="#参数个数" class="headerlink" title="参数个数"></a>参数个数</h4><p>由于第二层RNN的输入加入了语言模型表示，所以维数增加了，但是对实验效果几乎没有影响的。<br>我们通过两个实验来验证：</p><ul><li>增加不包含语言模型的序列模型的第二层RNN维数。</li><li>减少TagLM的第二层RNN维数。</li></ul><p>性能提升都非常的少，而且还说明了TagLM增加的参数对性能是有略微削弱的。</p><h4 id="语言模型要跟随语料库领域而改变吗？"><a href="#语言模型要跟随语料库领域而改变吗？" class="headerlink" title="语言模型要跟随语料库领域而改变吗？"></a>语言模型要跟随语料库领域而改变吗？</h4><p>答案是不需要，之前都是在新闻语料上做的训练，我们直接把它应用到了科技语料库上，性能依然有很大提升。</p><h1 id="4-相关工作"><a href="#4-相关工作" class="headerlink" title="4 相关工作"></a>4 相关工作</h1><hr><h4 id="未标注数据"><a href="#未标注数据" class="headerlink" title="未标注数据"></a>未标注数据</h4><h4 id="神经语言模型"><a href="#神经语言模型" class="headerlink" title="神经语言模型"></a>神经语言模型</h4><h4 id="解释RNN状态"><a href="#解释RNN状态" class="headerlink" title="解释RNN状态"></a>解释RNN状态</h4><h4 id="其他序列标注模型"><a href="#其他序列标注模型" class="headerlink" title="其他序列标注模型"></a>其他序列标注模型</h4><h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5 总结"></a>5 总结</h1><hr><ul><li>提出了一种简单、通用的半监督方法，使用预训练的神经语言模型，来给序列标注模型增加上下文表示。</li><li>我们的方法在NER和chunking任务上比其他的方法都要好。</li><li>多使用一个后向的语言模型效果更好。</li><li>即使语言模型在不同领域的语料库上训练，或者序列模型在大数据量的标注数据上训练，效果依然有很大提升。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 序列标注 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACL </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 序列标注 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sublime Text安装与配置教程</title>
      <link href="/2017/10/02/sublime/"/>
      <url>/2017/10/02/sublime/</url>
      
        <content type="html"><![CDATA[<p>Sublime Text是我一直使用的代码编辑器，我喜爱它的原因就是好看啊！当然打开速度毋庸置疑啦，毕竟不是IDE。这里我把我的安装与配置步骤教给大家，如有未尽之处，大家自己摸索咯，也欢迎与我交流。<br>先附上一张美图：<br><img src="sublime.png" alt></p><h1 id="安装Sublime-Text-3"><a href="#安装Sublime-Text-3" class="headerlink" title="安装Sublime Text 3"></a>安装Sublime Text 3</h1><hr><p>下载地址请点击<a href="https://download.sublimetext.com/Sublime%20Text%20Build%203143%20x64%20Setup.exe" target="_blank" rel="noopener">这里</a>。<br>安装过程就不多说了，一直点<code>next</code>就行了。</p><h1 id="配置C-运行环境"><a href="#配置C-运行环境" class="headerlink" title="配置C++运行环境"></a>配置C++运行环境</h1><hr><p>装完后可以直接写代码了，但是不能运行C++的哦，还需要配置运行环境。</p><ul><li><p>首先要安装C++的编译器MinGW，可以直接去官网下（<a href="http://www.mingw.org/" target="_blank" rel="noopener">传送门</a>）。不过我自己是直接下的CodeBlocks（<a href="https://downloads.sourceforge.net/project/codeblocks/Binaries/16.01/Windows/codeblocks-16.01mingw-setup.exe" target="_blank" rel="noopener">传送门</a>），然后用的自带的MinGW。</p></li><li><p>装完编译器之后在<code>我的电脑</code>右键，依次点击<code>属性 - 高级系统设置 - 环境变量</code>，在<code>系统变量</code>中找到<code>Path</code>，编辑它，新建一条，添加MinGW路径，以我的为例是<code>E:\software\codeblocks\MinGW\bin</code>。</p></li><li><p>打开Sublime Text，依次点击<code>Tools - Build System - new Build System</code>，粘贴以下代码：</p><pre><code>{  &quot;encoding&quot;: &quot;utf-8&quot;,  &quot;working_dir&quot;: &quot;$file_path&quot;,  &quot;shell_cmd&quot;: &quot;g++ -Wall -std=c++11 \&quot;$file_name\&quot; -o \&quot;$file_base_name\&quot;&quot;,  &quot;file_regex&quot;: &quot;^(..[^:]*):([0-9]+):?([0-9]+)?:? (.*)$&quot;,  &quot;selector&quot;: &quot;source.c++&quot;,  &quot;variants&quot;:   [      {          &quot;name&quot;: &quot;Run&quot;,          &quot;shell_cmd&quot;: &quot;g++ -Wall -std=c++11 \&quot;$file\&quot; -o \&quot;$file_base_name\&quot; &amp;&amp; start cmd /c \&quot;\&quot;${file_path}/${file_base_name}\&quot; &amp; pause\&quot;&quot;      }  ]}</code></pre></li><li><p>然后<code>ctrl+s</code>保存，命名为c++11。</p></li></ul><p>这时候随便写一个C++代码，然后<code>Tools - Build System</code>选择<code>c++11</code>，然后按<code>ctrl+b</code>就可以运行啦。<br>我这配置的是控制台运行的C++，所以支持输入数据的哦！</p><h1 id="配置Java运行环境"><a href="#配置Java运行环境" class="headerlink" title="配置Java运行环境"></a>配置Java运行环境</h1><hr><ul><li>首先下载Java的编译器jdk（<a href="http://download.oracle.com/otn-pub/java/jdk/9+181/jdk-9_windows-x64_bin.exe" target="_blank" rel="noopener">传送门</a>），安装过程就不说了。</li><li>环境变量应该不用自己添加了，安装过程会帮你自动添加。</li><li>打开Sublime Text，依次点击<code>Tools - Build System - new Build System</code>，粘贴以下代码：<pre><code>{  &quot;cmd&quot;: [&quot;javac&quot;,&quot;-d&quot;,&quot;.&quot;,&quot;$file&quot;],  &quot;file_regex&quot;: &quot;^(...*?):([0-9]*):?([0-9]*)&quot;,  &quot;selector&quot;: &quot;source.java&quot;,  &quot;encoding&quot;:&quot;cp936&quot;,  //执行完上面的命令就结束  // 下面的命令需要按Ctrl+Shift+b来运行  &quot;variants&quot;:  [      {          &quot;name&quot;: &quot;Run&quot;,          &quot;shell&quot;: true,          &quot;cmd&quot; : [&quot;start&quot;,&quot;cmd&quot;,&quot;/c&quot;, &quot;java ${file_base_name} &amp;echo. &amp; pause&quot;],           //c是执行完命令后关闭cmd窗口,          //k是执行完命令后不关闭cmd窗口。          // echo. 相当于输入一个回车          // pause命令使cmd窗口按任意键后才关闭          &quot;working_dir&quot;: &quot;${file_path}&quot;,          &quot;encoding&quot;:&quot;cp936&quot;      }  ]}</code></pre></li><li>然后<code>ctrl+s</code>保存，命名为JavaC。</li></ul><p>这时候随便写一个Java代码，然后<code>Tools - Build System</code>选择<code>JavaC</code>，然后按<code>ctrl+b</code>就可以运行啦。<br>我这配置的是控制台运行的Java，所以支持输入数据的哦！</p><h1 id="配置Python运行环境"><a href="#配置Python运行环境" class="headerlink" title="配置Python运行环境"></a>配置Python运行环境</h1><hr><ul><li>强烈推荐配合Python发行版本Anaconda使用，下载地址（<a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">传送门</a>），下载速度有点慢，推荐使用迅雷下载。安装过程就不多说了。一定要记得安装过程中有一步添加系统变量一定要勾上！</li><li>然后…就没有然后了，Python运行环境安装就是这么简单，直接按<code>ctrl+b</code>就能运行了，但是不支持输入数据哦，想要输入数据的话要安装<code>Sublime REPL</code>插件，请看后面的教程。</li></ul><h1 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h1><p>Sublime Text的强大之处就是可以安装各种插件满足你的需求。<br>安装过程很简单：</p><ul><li>首先要安装插件管理工具<code>Package Control</code>，按<code>ctrl+shift+p</code>，输入<code>Install Package</code>，按回车，等待安装完毕。</li><li>然后<code>Preferences</code>选项菜单就会出现<code>Package Control</code>子菜单。</li><li>然后按<code>ctrl+shift+p</code>，输入各种插件名称就能安装啦。</li></ul><p>下面推荐几个我使用的插件，其他的可以自行百度搜索。</p><ul><li>Anaconda<br>这个需要你先安装了Anaconda，然后可以提供各种强大的功能，比如代码提示、函数文档查询、代码风格纠正等等。</li><li>SublimeREPL<br>这个是为了Python输入数据准备的插件，装完之后点击<code>Preferences - Key Bindings</code>，在<code>User</code>文件里粘贴以下代码：<pre><code>  [       { &quot;keys&quot;: [&quot;f5&quot;],          &quot;caption&quot;: &quot;SublimeREPL: Python - RUN current file&quot;,          &quot;command&quot;: &quot;run_existing_window_command&quot;, &quot;args&quot;:          {              &quot;id&quot;: &quot;repl_python_run&quot;,              &quot;file&quot;: &quot;config/Python/Main.sublime-menu&quot;          }      }  ]</code></pre>  然后运行Python代码时直接按<code>F5</code>就行啦！<br>下面两个随意装。</li><li>SublimeHighLight<br>装完之后选中你要复制的代码，右键<code>Copy as RTF</code>，然后粘贴到Word里就会保留代码格式，很漂亮的啊！</li><li>ConvertToUTF8<br>这是为了某些中文显示准备的插件，貌似不怎么用得到，随意装吧。</li></ul><p>我用的就这些啦，Sublime Text写代码还是很方便的，现在基本不用其他的IDE了，能少打开一个软件是一个嘛。</p><h1 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h1><hr><p>直接递上<a href="http://www.jb51.net/softjc/180873.html" target="_blank" rel="noopener">传送门</a>。</p><h1 id="sublime配置"><a href="#sublime配置" class="headerlink" title="sublime配置"></a>sublime配置</h1><p><strong>Settings:</strong></p><pre><code>{    &quot;color_scheme&quot;: &quot;Packages/Color Scheme - Default/Monokai.sublime-color-scheme&quot;,    &quot;font_options&quot;:    [        &quot;gdi&quot;    ],    &quot;font_size&quot;: 14,    &quot;ignored_packages&quot;:    [        &quot;Vintage&quot;    ],    &quot;theme&quot;: &quot;Adaptive.sublime-theme&quot;,    &quot;translate_tabs_to_spaces&quot;: true,    &quot;expand_tabs_on_save&quot;: true,    &quot;tab_size&quot;: 4,}</code></pre><p><strong>Anaconda Settings User:</strong></p><pre><code>{    &quot;python_interpreter&quot;: &quot;E:/software/anaconda/python.exe&quot;,    &quot;suppress_word_completions&quot;: false,    &quot;suppress_explicit_completions&quot;: false,    &quot;complete_parameters&quot;: true,    &quot;complete__all_parameters&quot;: true,    &quot;anaconda_linting&quot;: false,    &quot;swallow_startup_errors&quot;: true,    &quot;auto_formatting&quot;: true,    &quot;enable_docstrings_tooltip&quot;: true,    &quot;enable_signatures_tooltip&quot;: true,    &quot;display_signatures&quot;: true,}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 软件安装与配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sublime </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吴恩达深度学习公开课第二周编程练习2</title>
      <link href="/2017/09/20/dlhw2/"/>
      <url>/2017/09/20/dlhw2/</url>
      
        <content type="html"><![CDATA[<p>这次练习是实现logistic回归模型的神经网络，来预测一张图片是不是一只猫。<br>我把代码整合在了一起，如下：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> h5py<span class="token keyword">import</span> scipy<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image<span class="token keyword">from</span> scipy <span class="token keyword">import</span> ndimage<span class="token keyword">from</span> lr_utils <span class="token keyword">import</span> load_datasettrain_set_x_orig<span class="token punctuation">,</span> train_set_y<span class="token punctuation">,</span> test_set_x_orig<span class="token punctuation">,</span> test_set_y<span class="token punctuation">,</span> classes <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token punctuation">)</span>m_train <span class="token operator">=</span> train_set_x_orig<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>m_test <span class="token operator">=</span> test_set_x_orig<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>num_px <span class="token operator">=</span> train_set_x_orig<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>train_set_x_flatten <span class="token operator">=</span> train_set_x_orig<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>train_set_x_orig<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>Ttest_set_x_flatten <span class="token operator">=</span> test_set_x_orig<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>test_set_x_orig<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>Ttrain_set_x <span class="token operator">=</span> train_set_x_flatten <span class="token operator">/</span> <span class="token number">255</span><span class="token punctuation">.</span>test_set_x <span class="token operator">=</span> test_set_x_flatten <span class="token operator">/</span> <span class="token number">255</span><span class="token punctuation">.</span><span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">:</span>    s <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>z<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> s<span class="token keyword">def</span> <span class="token function">initialize_with_zeros</span><span class="token punctuation">(</span>dim<span class="token punctuation">)</span><span class="token punctuation">:</span>    w <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    b <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">assert</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>shape <span class="token operator">==</span> <span class="token punctuation">(</span>dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">assert</span><span class="token punctuation">(</span>isinstance<span class="token punctuation">(</span>b<span class="token punctuation">,</span> float<span class="token punctuation">)</span> <span class="token operator">or</span> isinstance<span class="token punctuation">(</span>b<span class="token punctuation">,</span> int<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> w<span class="token punctuation">,</span> bdim <span class="token operator">=</span> <span class="token number">2</span>w<span class="token punctuation">,</span> b <span class="token operator">=</span> initialize_with_zeros<span class="token punctuation">(</span>dim<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">propagate</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">:</span>    m <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    A <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>w<span class="token punctuation">.</span>T<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span>    cost <span class="token operator">=</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>Y <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>A<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> Y<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> A<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> m    dw <span class="token operator">=</span> X<span class="token punctuation">.</span>dot<span class="token punctuation">(</span><span class="token punctuation">(</span>A <span class="token operator">-</span> Y<span class="token punctuation">)</span><span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">/</span> m    db <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>A <span class="token operator">-</span> Y<span class="token punctuation">)</span> <span class="token operator">/</span> m    <span class="token keyword">assert</span><span class="token punctuation">(</span>dw<span class="token punctuation">.</span>shape <span class="token operator">==</span> w<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>    <span class="token keyword">assert</span><span class="token punctuation">(</span>db<span class="token punctuation">.</span>dtype <span class="token operator">==</span> float<span class="token punctuation">)</span>    cost <span class="token operator">=</span> np<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>cost<span class="token punctuation">)</span>    <span class="token keyword">assert</span><span class="token punctuation">(</span>cost<span class="token punctuation">.</span>shape <span class="token operator">==</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    grads <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"dw"</span><span class="token punctuation">:</span> dw<span class="token punctuation">,</span>             <span class="token string">"db"</span><span class="token punctuation">:</span> db<span class="token punctuation">}</span>    <span class="token keyword">return</span> grads<span class="token punctuation">,</span> costw<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>grads<span class="token punctuation">,</span> cost <span class="token operator">=</span> propagate<span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">optimize</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> num_iterations<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> print_cost <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    costs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>        grads<span class="token punctuation">,</span> cost <span class="token operator">=</span> propagate<span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span>        dw <span class="token operator">=</span> grads<span class="token punctuation">[</span><span class="token string">"dw"</span><span class="token punctuation">]</span>        db <span class="token operator">=</span> grads<span class="token punctuation">[</span><span class="token string">"db"</span><span class="token punctuation">]</span>        w <span class="token operator">=</span> w <span class="token operator">-</span> learning_rate <span class="token operator">*</span> dw        b <span class="token operator">=</span> b <span class="token operator">-</span> learning_rate <span class="token operator">*</span> db        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            costs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cost<span class="token punctuation">)</span>        <span class="token keyword">if</span> print_cost <span class="token operator">and</span> i <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">"Cost after iteration %i: %f"</span> <span class="token operator">%</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> cost<span class="token punctuation">)</span><span class="token punctuation">)</span>    params <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"w"</span><span class="token punctuation">:</span> w<span class="token punctuation">,</span>              <span class="token string">"b"</span><span class="token punctuation">:</span> b<span class="token punctuation">}</span>    grads <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"dw"</span><span class="token punctuation">:</span> dw<span class="token punctuation">,</span>             <span class="token string">"db"</span><span class="token punctuation">:</span> db<span class="token punctuation">}</span>    <span class="token keyword">return</span> params<span class="token punctuation">,</span> grads<span class="token punctuation">,</span> costsparams<span class="token punctuation">,</span> grads<span class="token punctuation">,</span> costs <span class="token operator">=</span> optimize<span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> num_iterations<span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> learning_rate <span class="token operator">=</span> <span class="token number">0.009</span><span class="token punctuation">,</span> print_cost <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>    m <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    Y_prediction <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>m<span class="token punctuation">)</span><span class="token punctuation">)</span>    w <span class="token operator">=</span> w<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    A <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>w<span class="token punctuation">.</span>T<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> A<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;=</span> <span class="token number">0.5</span><span class="token punctuation">:</span>            Y_prediction<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            Y_prediction<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>      <span class="token keyword">assert</span><span class="token punctuation">(</span>Y_prediction<span class="token punctuation">.</span>shape <span class="token operator">==</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> Y_prediction</code></pre><p>但是这样看起来太乱太复杂了，于是最后一个练习将训练过程合并成了一个model，代码如下：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> Y_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> Y_test<span class="token punctuation">,</span> num_iterations <span class="token operator">=</span> <span class="token number">2000</span><span class="token punctuation">,</span> learning_rate <span class="token operator">=</span> <span class="token number">0.5</span><span class="token punctuation">,</span> print_cost <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    w<span class="token punctuation">,</span> b <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>X_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span>    parameters<span class="token punctuation">,</span> grads<span class="token punctuation">,</span> costs <span class="token operator">=</span> optimize<span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X_train<span class="token punctuation">,</span> Y_train<span class="token punctuation">,</span> num_iterations<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> print_cost<span class="token punctuation">)</span>    w <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">"w"</span><span class="token punctuation">]</span>    b <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">"b"</span><span class="token punctuation">]</span>    Y_prediction_test <span class="token operator">=</span> predict<span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X_test<span class="token punctuation">)</span>    Y_prediction_train <span class="token operator">=</span> predict<span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X_train<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"train accuracy: {} %"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token number">100</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>np<span class="token punctuation">.</span>abs<span class="token punctuation">(</span>Y_prediction_train <span class="token operator">-</span> Y_train<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"test accuracy: {} %"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token number">100</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>np<span class="token punctuation">.</span>abs<span class="token punctuation">(</span>Y_prediction_test <span class="token operator">-</span> Y_test<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    d <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"costs"</span><span class="token punctuation">:</span> costs<span class="token punctuation">,</span>         <span class="token string">"Y_prediction_test"</span><span class="token punctuation">:</span> Y_prediction_test<span class="token punctuation">,</span>          <span class="token string">"Y_prediction_train"</span> <span class="token punctuation">:</span> Y_prediction_train<span class="token punctuation">,</span>          <span class="token string">"w"</span> <span class="token punctuation">:</span> w<span class="token punctuation">,</span>          <span class="token string">"b"</span> <span class="token punctuation">:</span> b<span class="token punctuation">,</span>         <span class="token string">"learning_rate"</span> <span class="token punctuation">:</span> learning_rate<span class="token punctuation">,</span>         <span class="token string">"num_iterations"</span><span class="token punctuation">:</span> num_iterations<span class="token punctuation">}</span>    <span class="token keyword">return</span> dd <span class="token operator">=</span> model<span class="token punctuation">(</span>train_set_x<span class="token punctuation">,</span> train_set_y<span class="token punctuation">,</span> test_set_x<span class="token punctuation">,</span> test_set_y<span class="token punctuation">,</span> num_iterations <span class="token operator">=</span> <span class="token number">2000</span><span class="token punctuation">,</span> learning_rate <span class="token operator">=</span> <span class="token number">0.005</span><span class="token punctuation">,</span> print_cost <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吴恩达深度学习公开课第二周编程练习1</title>
      <link href="/2017/09/20/dlhw1/"/>
      <url>/2017/09/20/dlhw1/</url>
      
        <content type="html"><![CDATA[<p>这次编程练习是吴恩达深度学习公开课第二周的配套练习，地址：<a href="https://www.coursera.org/learn/neural-networks-deep-learning" target="_blank" rel="noopener">coursera</a>。</p><h1 id="1-Building-basic-functions-with-numpy"><a href="#1-Building-basic-functions-with-numpy" class="headerlink" title="1 - Building basic functions with numpy"></a>1 - Building basic functions with numpy</h1><hr><h2 id="1-1-sigmoid-function-np-exp"><a href="#1-1-sigmoid-function-np-exp" class="headerlink" title="1.1 - sigmoid function, np.exp()"></a>1.1 - sigmoid function, np.exp()</h2><p>Exercise: Build a function that returns the sigmoid of a real number x. Use <code>math.exp(x)</code> for the exponential function.</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># GRADED FUNCTION: basic_sigmoid</span><span class="token keyword">import</span> math<span class="token keyword">def</span> <span class="token function">basic_sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Compute sigmoid of x.    Arguments:    x -- A scalar    Return:    s -- sigmoid(x)    """</span>    <span class="token comment" spellcheck="true">### START CODE HERE ### (≈ 1 line of code)</span>    s <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">### END CODE HERE ###</span>    <span class="token keyword">return</span> sbasic_sigmoid<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span></code></pre><p>Exercise: Implement the sigmoid function using <code>numpy</code>.</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># GRADED FUNCTION: sigmoid</span><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np <span class="token comment" spellcheck="true"># this means you can access numpy functions by writing np.function() instead of numpy.function()</span><span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Compute the sigmoid of x    Arguments:    x -- A scalar or numpy array of any size    Return:    s -- sigmoid(x)    """</span>    <span class="token comment" spellcheck="true">### START CODE HERE ### (≈ 1 line of code)</span>    s <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">### END CODE HERE ###</span>    <span class="token keyword">return</span> sx <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><h2 id="1-2-Sigmoid-gradient"><a href="#1-2-Sigmoid-gradient" class="headerlink" title="1.2 - Sigmoid gradient"></a>1.2 - Sigmoid gradient</h2><p>Exercise: Implement the function <code>sigmoid_grad()</code> to compute the gradient of the sigmoid function with respect to its input x. The formula is:<br>\[sigmoid\_derivative(x) = \sigma’(x) = \sigma(x) (1 - \sigma(x))\]</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># GRADED FUNCTION: sigmoid_derivative</span><span class="token keyword">def</span> <span class="token function">sigmoid_derivative</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.    Arguments:    x -- A scalar or numpy array    Return:    ds -- Your computed gradient.    """</span>    <span class="token comment" spellcheck="true">### START CODE HERE ### (≈ 2 lines of code)</span>    s <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    ds <span class="token operator">=</span> s <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> s<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">### END CODE HERE ###</span>    <span class="token keyword">return</span> dsx <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">"sigmoid_derivative(x) = "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>sigmoid_derivative<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="1-3-Reshaping-arrays"><a href="#1-3-Reshaping-arrays" class="headerlink" title="1.3 - Reshaping arrays"></a>1.3 - Reshaping arrays</h2><p>Exercise: Implement <code>image2vector()</code> that takes an input of shape (length, height, 3) and returns a vector of shape (length*height*3, 1). </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># GRADED FUNCTION: image2vector</span><span class="token keyword">def</span> <span class="token function">image2vector</span><span class="token punctuation">(</span>image<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Argument:    image -- a numpy array of shape (length, height, depth)    Returns:    v -- a vector of shape (length*height*depth, 1)    """</span>    <span class="token comment" spellcheck="true">### START CODE HERE ### (≈ 1 line of code)</span>    v <span class="token operator">=</span> image<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>image<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">*</span>image<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">*</span>image<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">### END CODE HERE ###</span>    <span class="token keyword">return</span> v<span class="token comment" spellcheck="true"># This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values</span>image <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.67826139</span><span class="token punctuation">,</span>  <span class="token number">0.29380381</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.90714982</span><span class="token punctuation">,</span>  <span class="token number">0.52835647</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.4215251</span> <span class="token punctuation">,</span>  <span class="token number">0.45017551</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.92814219</span><span class="token punctuation">,</span>  <span class="token number">0.96677647</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.85304703</span><span class="token punctuation">,</span>  <span class="token number">0.52351845</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.19981397</span><span class="token punctuation">,</span>  <span class="token number">0.27417313</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.60659855</span><span class="token punctuation">,</span>  <span class="token number">0.00533165</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.10820313</span><span class="token punctuation">,</span>  <span class="token number">0.49978937</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.34144279</span><span class="token punctuation">,</span>  <span class="token number">0.94630077</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">"image2vector(image) = "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>image2vector<span class="token punctuation">(</span>image<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="1-4-Normalizing-rows"><a href="#1-4-Normalizing-rows" class="headerlink" title="1.4 - Normalizing rows"></a>1.4 - Normalizing rows</h2><p>Exercise: Implement <code>normalizeRows()</code> to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1).</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># GRADED FUNCTION: normalizeRows</span><span class="token keyword">def</span> <span class="token function">normalizeRows</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Implement a function that normalizes each row of the matrix x (to have unit length).    Argument:    x -- A numpy matrix of shape (n, m)    Returns:    x -- The normalized (by row) numpy matrix. You are allowed to modify x.    """</span>    <span class="token comment" spellcheck="true">### START CODE HERE ### (≈ 2 lines of code)</span>    <span class="token comment" spellcheck="true"># Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)</span>    x_norm <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> keepdims <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Divide x by its norm.</span>    x <span class="token operator">=</span> x <span class="token operator">/</span> x_norm    <span class="token comment" spellcheck="true">### END CODE HERE ###</span>    <span class="token keyword">return</span> xx <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"normalizeRows(x) = "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>normalizeRows<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="1-5-Broadcasting-and-the-softmax-function"><a href="#1-5-Broadcasting-and-the-softmax-function" class="headerlink" title="1.5 - Broadcasting and the softmax function"></a>1.5 - Broadcasting and the softmax function</h2><p>Exercise: Implement a <code>softmax</code> function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization.</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># GRADED FUNCTION: softmax</span><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Calculates the softmax for each row of the input x.    Your code should work for a row vector and also for matrices of shape (n, m).    Argument:    x -- A numpy matrix of shape (n,m)    Returns:    s -- A numpy matrix equal to the softmax of x, of shape (n,m)    """</span>    <span class="token comment" spellcheck="true">### START CODE HERE ### (≈ 3 lines of code)</span>    <span class="token comment" spellcheck="true"># Apply exp() element-wise to x. Use np.exp(...).</span>    x_exp <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).</span>    x_sum <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>x_exp<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.</span>    s <span class="token operator">=</span> x_exp <span class="token operator">/</span> x_sum    <span class="token comment" spellcheck="true">### END CODE HERE ###</span>    <span class="token keyword">return</span> sx <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span> <span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"softmax(x) = "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h1 id="2-Vectorization"><a href="#2-Vectorization" class="headerlink" title="2 - Vectorization"></a>2 - Vectorization</h1><hr><h2 id="2-1-Implement-the-L1-and-L2-loss-functions"><a href="#2-1-Implement-the-L1-and-L2-loss-functions" class="headerlink" title="2.1 - Implement the L1 and L2 loss functions"></a>2.1 - Implement the L1 and L2 loss functions</h2><p>Exercise: Implement the numpy vectorized version of the <code>L1</code> loss. You may find the function <code>abs(x)</code> (absolute value of x) useful.</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># GRADED FUNCTION: L1</span><span class="token keyword">def</span> <span class="token function">L1</span><span class="token punctuation">(</span>yhat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    yhat -- vector of size m (predicted labels)    y -- vector of size m (true labels)    Returns:    loss -- the value of the L1 loss function defined above    """</span>    <span class="token comment" spellcheck="true">### START CODE HERE ### (≈ 1 line of code)</span>    loss <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>abs<span class="token punctuation">(</span>y <span class="token operator">-</span> yhat<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">### END CODE HERE ###</span>    <span class="token keyword">return</span> lossyhat <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">)</span>y <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"L1 = "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>L1<span class="token punctuation">(</span>yhat<span class="token punctuation">,</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>Exercise: Implement the numpy vectorized version of the <code>L2</code> loss. There are several way of implementing the <code>L2</code> loss but you may find the function <code>np.dot()</code> useful. As a reminder, if $x = [x_1, x_2, …, x_n]$, then <code>np.dot(x,x)</code> = $\sum_{j=0}^n x_j^{2}$</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># GRADED FUNCTION: L2</span><span class="token keyword">def</span> <span class="token function">L2</span><span class="token punctuation">(</span>yhat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    yhat -- vector of size m (predicted labels)    y -- vector of size m (true labels)    Returns:    loss -- the value of the L2 loss function defined above    """</span>    <span class="token comment" spellcheck="true">### START CODE HERE ### (≈ 1 line of code)</span>    loss <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>y <span class="token operator">-</span> yhat<span class="token punctuation">,</span> y <span class="token operator">-</span> yhat<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">### END CODE HERE ###</span>    <span class="token keyword">return</span> lossyhat <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">)</span>y <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"L2 = "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>L2<span class="token punctuation">(</span>yhat<span class="token punctuation">,</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吴恩达深度学习公开课第二周学习笔记</title>
      <link href="/2017/09/19/dl2/"/>
      <url>/2017/09/19/dl2/</url>
      
        <content type="html"><![CDATA[<p>这周开始将会学到神经网络编程的基础知识。</p><h1 id="2-1-二分分类"><a href="#2-1-二分分类" class="headerlink" title="2.1 二分分类"></a>2.1 二分分类</h1><hr><p><img src="13.jpg" alt></p><p>二分类问题就是给定一个输入$x$，预测它的标签$y$是0还是1。拿预测一张图片是不是猫来举例子，一张图片大小为${\rm{64}} \times {\rm{64}}$，将它转化为三个矩阵分别代表RGB分量。再将三个矩阵合并成一个大小为${\rm{(64}} \times {\rm{64}} \times {\rm{3,1)}}$的矩阵作为输入$x$。</p><p>下面给出一些以后要用到的符号表示。</p><p>一个样本用$(x,y)$表示，其中$x \in {R^{ {n_x}}}$，$y \in \{ 0,1\} $，那么$m$个训练样本就可以用集合$\\{ ({x^{(1)}},{y^{(1)}}),({x^{(2)}},{y^{(2)}}), \ldots ,({x^{(m)}},{y^{(m)}})\\} $来表示。</p><p>我们还可以将所有样本特征值用一个矩阵表示：<br>\[X = \left[ {\begin{array}{*{20}{l}} \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\\\{ {x^{(1)}}}&amp;{ {x^{(2)}}}&amp; \cdots &amp;{ {x^{(m)}}}\\\\ \vdots &amp; \vdots &amp; \cdots &amp; \vdots \end{array}} \right]\]</p><p>其中$X \in {R^{ {n_x} \times m}}$。<br>所有样本标签也可以用一个矩阵表示：<br>\[Y = \left[ {\begin{array}{*{20}{l}}{ {y^{(1)}}}&amp;{ {y^{(2)}}}&amp; \cdots &amp;{ {y^{(m)}}}\end{array}} \right]\]</p><p>其中$Y \in {R^{1 \times m}}$。</p><h1 id="2-2-logistic回归"><a href="#2-2-logistic回归" class="headerlink" title="2.2 logistic回归"></a>2.2 logistic回归</h1><hr><p>logistic回归就是用线性函数来拟合输出标签。具体定义为，输入特征$X \in {R^{n_x}}$，参数$w \in {R^{n_x}}$，$b \in {R}$。那么令输出标签<br>\[{ {\hat y}^{(i)}} = \sigma ({w^T}{x^{^{(i)}}} + b)\]<br>其中\[\sigma ({x^{^{(i)}}}) = \frac{1}{ {1 + {e^{ - {x^{^{(i)}}}}}}}\]</p><h1 id="2-3-logistic回归损失函数"><a href="#2-3-logistic回归损失函数" class="headerlink" title="2.3 logistic回归损失函数"></a>2.3 logistic回归损失函数</h1><hr><p>损失函数衡量的是输出标签${\hat y}$与真实标签$y$之间的差距，有很多种定义方法，下面是常用的两种：<br>\[L({ {\hat y}^{^{(i)}}},{y^{^{(i)}}}) = \frac{1}{2}{({ {\hat y}^{^{(i)}}} - {y^{^{(i)}}})^2}\]和<br>\[L({ {\hat y}^{^{(i)}}},{y^{^{(i)}}}) =  - [{y^{(i)}}\log ({ {\hat y}^{(i)}}) + (1 - {y^{(i)}})\log (1 - { {\hat y}^{(i)}})]\]<br>一般我们使用下面一种损失函数，具体原因最后一节课会讲到，因为它是个凸函数，方便梯度下降。</p><p>如果有$m$个样本，那么总的损失函数就定义为<br>\[J(w,b) = \frac{1}{m}\sum\limits_{i - 1}^m {L({ {\hat y}^{^{(i)}}},{y^{^{(i)}}})}  =  - \frac{1}{m}\sum\limits_{i - 1}^m {[{y^{(i)}}\log ({ {\hat y}^{(i)}}) + (1 - {y^{(i)}})\log (1 - { {\hat y}^{(i)}})]} \]</p><h1 id="2-4-梯度下降法"><a href="#2-4-梯度下降法" class="headerlink" title="2.4 梯度下降法"></a>2.4 梯度下降法</h1><hr><p>通过重复<br>\[w: = w - \alpha \frac{ {\partial J(w,b)}}{ {\partial w}}\]<br>和<br>\[b: = b - \alpha \frac{ {\partial J(w,b)}}{ {\partial b}}\]<br>来不断更新$w$和$b$，使得$w$和$b$接近最优值。</p><h1 id="2-7-计算图"><a href="#2-7-计算图" class="headerlink" title="2.7 计算图"></a>2.7 计算图</h1><hr><p><img src="14.jpg" alt></p><p>如上图所示就是一个计算图，初始结点都是输入值，中间一个结点表示一个运算，最后一个结点就是输出值。</p><h1 id="2-8-计算图的导数计算"><a href="#2-8-计算图的导数计算" class="headerlink" title="2.8 计算图的导数计算"></a>2.8 计算图的导数计算</h1><hr><p><img src="15.jpg" alt></p><p>反向传播的时候只要沿着红色的箭头利用求导链式法则来对每个参数求导就行了。</p><h1 id="2-9-logistic回归中的梯度下降"><a href="#2-9-logistic回归中的梯度下降" class="headerlink" title="2.9 logistic回归中的梯度下降"></a>2.9 logistic回归中的梯度下降</h1><hr><p><img src="16.jpg" alt></p><p>logistic回归的计算图如上图所示，导数如下：<br>\[\frac{ {\partial L(a,y)}}{ {\partial a}} =  - \frac{y}{a} + \frac{ {1 - y}}{ {1 - a}}\]<br>\[\frac{ {\partial L(a,y)}}{ {\partial z}} = \frac{ {\partial L(a,y)}}{ {\partial a}} \cdot \frac{ {\partial a}}{ {\partial z}} = ( - \frac{y}{a} + \frac{ {1 - y}}{ {1 - a}}) \cdot a(1 - a) = a - y\]<br>\[\frac{ {\partial L(a,y)}}{ {\partial {w_1}}} = {x_1} \cdot \frac{ {\partial L(a,y)}}{ {\partial z}} = {x_1}(a - y)\]<br>\[\frac{ {\partial L(a,y)}}{ {\partial {w_2}}} = {x_2} \cdot \frac{ {\partial L(a,y)}}{ {\partial z}} = {x_2}(a - y)\]<br>\[\frac{ {\partial L(a,y)}}{ {\partial b}} = \frac{ {\partial L(a,y)}}{ {\partial z}} = a - y\]</p><h1 id="2-10-m-个样本的梯度下降"><a href="#2-10-m-个样本的梯度下降" class="headerlink" title="2.10 $m$个样本的梯度下降"></a>2.10 $m$个样本的梯度下降</h1><hr><p><img src="17.jpg" alt></p><p>$m$个样本的梯度其实就是每个样本的梯度求和，如图所示的伪代码中，用一层<code>for</code>循环来对梯度进行求和。在后面的课程中我们将会摒弃这种做法，用更快速的向量化方法来进行计算。</p><h1 id="2-11-向量化"><a href="#2-11-向量化" class="headerlink" title="2.11 向量化"></a>2.11 向量化</h1><hr><p>向量化就是将例如${w^T}x$这样的矩阵点乘用python的<code>numpy</code>库函数<code>dot</code>代替普通的<code>for</code>循环。示例代码如下：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npa <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1000000</span><span class="token punctuation">)</span>b <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1000000</span><span class="token punctuation">)</span>c <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token keyword">print</span> c</code></pre><h1 id="2-12-向量化的更多例子"><a href="#2-12-向量化的更多例子" class="headerlink" title="2.12 向量化的更多例子"></a>2.12 向量化的更多例子</h1><hr><p>经验法则是能不使用<code>for</code>循环就尽量不要使用，用向量来代替。更多的向量化例子有<code>np.exp()</code>，<code>np.sum()</code>等等。</p><h1 id="2-13-向量化logistic回归"><a href="#2-13-向量化logistic回归" class="headerlink" title="2.13 向量化logistic回归"></a>2.13 向量化logistic回归</h1><hr><p>之前提到的算法是用<code>for</code>循环来计算所有的${ {\hat y}^{(i)}} = \sigma ({w^T}{x^{^{(i)}}} + b)$。现在可以使用向量化来加快计算速度，只要计算$Y = \sigma ({w^T}X + b)$即可。</p><h1 id="2-14-向量化logistic回归的梯度输出"><a href="#2-14-向量化logistic回归的梯度输出" class="headerlink" title="2.14 向量化logistic回归的梯度输出"></a>2.14 向量化logistic回归的梯度输出</h1><hr><p>logistic回归的梯度用向量可以表示为<br>\[dz = A - Y\]<br>\[dw = \frac{1}{m}Xd{z^T}\]<br>下面两节课都是讲的python的广播和向量的一些说明，在此就不细讲了，大家可以去查看python文档。其中讲到的一个写python程序的好习惯就是用到向量的时候如果不确定维数，那么你就<code>reshape()</code>一下，还有就是加一句<code>assert()</code>语句判断一下维数。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吴恩达深度学习公开课第一周学习笔记</title>
      <link href="/2017/09/19/dl1/"/>
      <url>/2017/09/19/dl1/</url>
      
        <content type="html"><![CDATA[<p>保了研暂时也没什么事干，从今天开始学一学吴恩达最近发布的深度学习课程。顺便在博客上做做笔记，梳理一下。</p><p>coursera上面的课程要钱，还贼贵。所以我就直接在网易公开课上面看免费版的了，免费版没有作业和练习，是一个很大的缺憾。因为这门课很大的一个亮点就是高质量的编程题。顺带附上这门课的地址：<a href="https://mooc.study.163.com/learn/deeplearning_ai-2001281002?tid=2001392029#/learn/content" target="_blank" rel="noopener">吴恩达深度学习公开课</a></p><h1 id="1-2-什么是神经网络？"><a href="#1-2-什么是神经网络？" class="headerlink" title="1.2 什么是神经网络？"></a>1.2 什么是神经网络？</h1><hr><p>这节课是深度学习的入门课，首先给出了深度学习的定义：深度学习就是训练神经网络。那么问题来了，什么是神经网络？这节课就介绍了神经网络的概念。<br><img src="5.jpg" alt></p><p>首先从一个房价预测的例子讲起，已知房子大小和对应的房价，用一条曲线拟合这些点。那么用一条直线就可以了，但是房价不能为负，所以左边恒为0，这个函数就是“线性修正单元”，即ReLU。</p><p><img src="6.jpg" alt></p><p>这就是最基本的神经元，输入一个房子大小也就是$x$，经过一个神经单元运算之后，输出房价也就是$y$。</p><p><img src="7.jpg" alt></p><p>更复杂的情况下，房价还取决于更多的因素，比如房子大小、房间数量、邮编、周边富裕程度。</p><p>我们不会具体指明中间一层的神经单元代表什么，神经网络想怎么算就怎么算。所以上图从左到右三层分别叫做输入层、隐含层、输出层。</p><p>值得注意的是，神经网络只要你喂给了它足够多的$(x,y)$训练数据，那么它就可以训练出一个从$x$到$y$的精准映射函数。</p><p>这就是最基本的神经网络和监督学习的例子，也就是你任意输入一个$x$，神经网络都可以预测出一个对应的$y$。</p><p>下一讲将会更加深入的讲解监督学习的相关算法。</p><h1 id="1-3-用神经网络进行监督学习"><a href="#1-3-用神经网络进行监督学习" class="headerlink" title="1.3 用神经网络进行监督学习"></a>1.3 用神经网络进行监督学习</h1><hr><p>神经网络最近被媒体炒作的沸沸扬扬，其实绝大多数神经网络创造的价值都来自于一种机器学习算法：监督学习。</p><p><img src="8.jpg" alt></p><p>可以看出，监督学习就是输入一个特征$x$，然后学习得到一个输出$y$。监督学习在很多不同领域有重要应用，比如之前提到的房价预测，还有在线广告、计算机视觉、语音识别、机器翻译和自动驾驶等等。</p><p>他们所应用的神经网络结构也都是不同的。其中房价预测和在线广告用的是标准的神经网络。而计算机视觉处理的是空间上的图像，所以用到的是卷积神经网络(CNN)。语音识别和机器翻译因为都是处理的时间相关的序列，所以用到的是循环神经网络(RNN)。而自动驾驶就更加的复杂了，既要处理图像，又要处理雷达信息，所以要用混合神经网络。</p><p><img src="9.jpg" alt></p><p>上图就是三种神经网络的基本结构，CNN擅长处理图像相关的数据，RNN擅长处理序列相关的数据。</p><p><img src="10.jpg" alt></p><p>输入的数据也分为两大类：结构化数据和非结构化数据。结构化数据就是指数据的数据库，每一个数据都是有明确的含义的，比如房子的大小，房间的数量等等。非结构化数据就是指音频、图像、文本之类的数据。</p><p>人类天生就更擅长理解非结构化数据，但是计算机就不行了。近几年来，神经网络的发展让计算机也能非常好的理解非结构化数据了。在实际应用中，最好要将结构化数据和非结构化数据结合起来，才能发挥出最好的性能。</p><p>其实神经网络几十年前就已经提出来了，那为什么最近几年才流行起来呢？下一节吴恩达老师将会给我们带来答案。</p><h1 id="1-4-为什么深度学习会兴起"><a href="#1-4-为什么深度学习会兴起" class="headerlink" title="1.4 为什么深度学习会兴起?"></a>1.4 为什么深度学习会兴起?</h1><hr><p>上节课最后提到，神经网络几十年前就有了，那为什么最近几年才兴起呢？</p><p><img src="11.jpg" alt></p><p>吴恩达通过一张图很好的解释了原因。如上图所示，$x$轴是数据量，$y$轴是机器学习算法的效率，具体来说就是图像识别的准确率等等。</p><p>可以看出，随着数据量的增大，机器学习算法的效率也随之增大。但是传统的机器学习算法比如支持向量机(SVM)在数据量大到一个程度之后会进入“平台期”，不会再上升了。而不同规模的神经网络会一直上升，只要你数据足够多。</p><p>所以在今天要想得到更好的效果，就要训练一个大规模的神经网络，这个大规模指的是两个方面：一个是神经网络的隐含层单元足够多，也就是参数足够多，一个就是数据规模要足够多。但是这最终也是有瓶颈的，因为神经网络太大了之后计算效率就会下降，训练时间会特别长。数据也会耗尽，没有无限的数据。</p><p>从技术上面来说，上图$x$轴的数据代表的是“有标签的数据”，也就是每个输入$x$都有一个标签$y$与之对应，一组数据就是一对$(x,y)$。在以后的课程中，我们都使用$m$来表示数据的规模，也就是$x$轴的数值。</p><p></p><p>在上图中我们还可以看出，在数据量比较小的时候，传统机器学习算法甚至会优于神经网络。因为它的性能基于手工设计组件的优劣和一些算法细节上面的优化，比如特征的提取等等。但是训练数据规模大了以后，神经网络就稳定优于传统机器学习方法了。</p><p><img src="12.jpg" alt></p><p>影响神经网络性能的因素主要有三点：数据规模、计算能力、算法。</p><p>数据规模之前已经提到过了。计算能力的话因为最近几年CPU和GPU的发展，计算能力得到很大的提升，所以训练时间也大大缩短。算法优化也是为了缩短训练时间，课上举了一个算法优化的例子。</p><p></p><p>以前神经网络激活函数都是用的sigmoid函数，图像如上图所示。但是存在一个问题，就是一直训练下去会出现梯度消失的问题，反映在函数图象上就是当输入太小时斜率会趋于0。所以后来换成了ReLU函数，在大于0的时候斜率恒为1。</p><p></p><p>图的右半部分是神经网络训练的流程。首先你要想出一个算法，然后你要实现代码，最后运行结果进行训练。如果训练时间很短，比如10分钟或者一天以内，那么你就可以调整算法，继续优化训练。如果训练时间要一个月，那么你就只能实现很少的想法了。</p><p></p><p>可喜的是，现在数据和硬件还在继续发展着，我们要做的就是继续研究新的算法和进行算法的优化。所以我们坚信，深度学习还会继续发展进步下去。</p><p></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models</title>
      <link href="/2017/09/19/context/"/>
      <url>/2017/09/19/context/</url>
      
        <content type="html"><![CDATA[<p>论文链接：<a href="https://www.aclweb.org/anthology/P/P17/P17-2036.pdf" target="_blank" rel="noopener">P17-2036</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>最近许多研究者都注意到了上下文在对话系统中的重要性，也做了很多的研究，但是没有系统的比较来分析怎么样才能有效地利用上下文。我们做了详细的研究来比较不同的模型，研究上下文在对话系统中的作用。同时，我们提出了度量上下文与查询之间相关性的方法，比其他方法性能都出色。</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><hr><p>对话系统有两种典型的研究设置：单轮和多轮。单轮就是只输入查询$q$，输出答案$r$。但是大多数现实的对话都是要多轮的，就是要结合上下文来做出回答。<br>很多研究者都意识到了上下文的重要性，也提出了很多方法。一种是直接将上下文和查询向量连接到一起，另一种是分层模型。有很多方法来结合上下文和查询，比如池化和连接。但是没有人对它们做过比较。<br>这篇论文里，我们在Seq2Seq的对话系统上对上下文模型做研究。我们关注两个问题：</p><ul><li>我们怎么样才能更好的利用上下文？</li><li>上下文对神经对话系统的影响是什么？</li></ul><p>未分层模型通常使用经典的encode-decoder框架，我们实验中用的是RNN+GRU，decode时beamsearch大小为5。分层模型有三种方法结合上下文和查询：池化、连接、连续整合。<br><img src="hierarchical_model.jpg" alt><br><img src="2.jpg" alt><br>但是我们发现加权后的实验结果还不如直接使用最后一个隐含层的结果，我们猜测是因为这个RNN不是很长，所以对前面的结果保存的比较好，所以我们实验直接使用最后一个隐含层作为输出。<br>衡量上下文和查询的相关程度：<br>\[{s_{ {c_i}}} = sim({c_i},q) = \frac{ { {e_{c_i} } \cdot {e_q}}}{ {\left\| {e_{c_i}} \right\| \cdot \left\| { {e_q}} \right\|}}\]<br>其中：\[{e_{c_i}} = \sum\limits_{w \in {c_i}} {e_w} ,{e_q} = \sum\limits_{w’ \in {c_i}} {e_w’} \]<br>归一化：<br>\[\alpha _{c_i} = \frac{ {\exp ({s_{ {c_i}}})}} { {\sum\nolimits_{j = 0}^n {\exp ({s_{ {c_i}}})}  + \exp ({s_q})}}\]<br>\[{ {\alpha _q} = \frac{ {\exp ({s_q})}}{ {\sum\nolimits_{j = 0}^n {\exp ({s_{ {c_i}}})}  + \exp ({s_q})}}}\]<br>两种连接方法：</p><ul><li>WSeq(sum):<br>\[{v_{enc}} = \sum\limits_{i = 0}^n { {\alpha _{ {c_i}}}{h_{ {c_i}}} + {\alpha _q}{h_q}} \]</li><li>WSeq(concat):<br>\[{v_{enc}} = \left[ { {\alpha _{ {c_0}}}{h_{ {c_0}}}; \ldots ;{\alpha _{ {c_n}}}{h_{ {c_n}}};{\alpha _q}{h_q}} \right]\]</li></ul><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><hr><p>我们在百度贴吧问答数据集上做实验。<br><img src="1.jpg" alt></p><h1 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h1><hr><p>虽然BLEU不适合用来度量对话系统这种开放式的系统。但是我们没有足够的人力物力来对结果一一标注，所以还是采用BLEU。下面回到最开始提出的两个问题。</p><ul><li>我们怎么样才能更好的利用上下文？<br>首先我们发现采用上下文实验结果的确比不采用的更好了。然后分层的模型结果比不分层的更好。我们猜测原因可能是对话系统不同于其他NLP任务，对话系统句子可能出自不同的人。让每个上下文保持独立很重要，而不是简单的池化结合到一起，所以直接连接起来效果更好。上下文和查询相关性对系统有帮助。</li><li>上下文对神经对话系统的影响是什么？<br><img src="3.jpg" alt><br>可以看出，采用上下文的模型能产生更长、更有意义、更多样性的回答。我们还发现了一个有趣的现象：一个encode-decoder模型如果想要生成有意义的回答，必须要足够多的有意义的信息提供给它。这解释了为什么seq2seq在其他NLP任务表现得很好，但是在对话系统表现得不好。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 问答系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACL </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 问答系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>保研第一，我还是选择留在了本校</title>
      <link href="/2017/09/19/baoyan/"/>
      <url>/2017/09/19/baoyan/</url>
      
        <content type="html"><![CDATA[<p>保研去哪？其实我刚开始也纠结过。看到其他同学忙着申报各大名校，我也曾经动摇过，但是现在却没什么感觉了。很多人不理解为什么第一不去试试清北复交，我承认，这些学校是优势很大，毕竟还是高了一个档次的。<br>想当年高考时，我的梦想就是考上清华，但是最后考砸了只能来了华师。现在想来也罢了，或许本来就没有那个实力吧。从小学开始，我就一直是考上了最好的学校却去了低一个档次的学校。那时候都是因为体质不好吧，不让去太远的学校，现在反而习惯了那种拿第一的感觉。<br>暑假开始前，很多人就准备报名夏令营了，我却回家考驾照了，一个也没报。其实当时就已经打算留读本校了，当时考虑是研究生阶段学校影响没有导师和专业强了，而且其他学校导师也都不熟悉，很难找到一个人品好学术好的好导师，于是就决心本校找一个了。其他原因也都是些无关紧要的了，比如本校环境熟悉、还不用大费周折搬家等等，说起来挺幼稚，哪有前途重要。<br>现在也差不多定了吧，我觉得还是成事在人吧。既然自己选择了这条路，不去走更宽敞的路，那就要把它走好。去了好学校不能骄纵，留在差学校也不能自暴自弃吧，就当作是激励自己的动力吧。<br>还是谢谢大家的关心，让你们失望了，我不是没有梦想，也不是懒。只要你足够努力，梦想在哪里都能发芽。<br>看来不能因为保了研而放松自己了，还有那么多为了考研在奋斗的学生。下面一年三大计划：毕业论文、实习、研一的活。不管是保了研的还是还在考研的或者准备工作的，大家一起加油吧。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 保研 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一位长者关于保研的一些人生经验</title>
      <link href="/2017/09/19/recommend/"/>
      <url>/2017/09/19/recommend/</url>
      
        <content type="html"><![CDATA[<p>经过三年的艰苦学习，终于也艰难保上研了，因此在此给学弟学妹们一点人生经验，粗鄙之见，大家随便看看。</p><h1 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h1><hr><ul><li>好好学习自然不用多说，自觉性是最重要的，作业不能拖拉，不要拖到最后一刻才做作业。<br>就拿我自己说吧，我每次上完课都会立即把上课讲的再看一遍，把没听懂的搞懂，不然时间长了会慢慢忘掉的，等到期末就更难搞懂了。还有就是作业，我每次都会当天就开始做，尽量提前很早做完。这样好处多多哦，既可以不至于最后一天很忙，又可以把作业完成的很好，最后自然就拿高分啦。</li><li>图书馆可以多去去的，虽然我这一年都没去过。<br>想当年大一大二在闵行的时候，那时候还是有女票的(&gt;﹏&lt;)，基本上天天都会去图书馆呆着吧，很怀念那时候的日子。</li><li>上课少玩手机，多听讲，这样期末复习起来才轻松一点。<br>不过我自己上课也玩哈哈哈，我也有很多课听不懂或者很无聊的课，都会戳一戳手机解解闷。有时下午太困了还会打个盹。但是一定要记住，课上缺的，课后一定要补回来，就算花再多时间也要弄懂。</li></ul><h1 id="竞赛项目"><a href="#竞赛项目" class="headerlink" title="竞赛项目"></a>竞赛项目</h1><hr><ul><li>有实力和天赋的话搞搞ACM竞赛也是好处极大的，奖学金和保研加分多多的，也极大提升代码能力。<br>想当年我刚高中毕业的那个暑假，还连信息学竞赛都没听说过，但是出于兴趣暑假自学了c语言和基础的算法。于是刚开学就被招进了ACM实验室。不过当时搞的人也少，于是我也就有了出去打比赛的机会。现在这几年人越来越多了，要好好加油了啊。<br>不过我后来太懒了，都没怎么训练，现在光荣的退役咯，老学长只能给你们加油了。</li><li>项目的话我没怎么搞过，真心想搞可以好好弄弄，想水一水的那就别浪费时间了，加分也不多。<br>想当年还有过打算搞搞大夏杯啥的，后来也没了音讯，毕竟ACM太忙了。</li></ul><h1 id="娱乐"><a href="#娱乐" class="headerlink" title="娱乐"></a>娱乐</h1><hr><ul><li>也别老学习啊，多累啊，出去散散步逛逛街玩玩。<br>找个女票啊，带她一起出去玩玩。实在没有的话，和好基友出去玩也行啊！</li><li>日常开黑打打游戏啊，但别耽误了学习做作业。<br>像我就玩英雄联盟、炉石之类的啊，别玩物丧志，通宵玩伤了身体就好。室友之间有个共同爱好挺不错的。</li></ul><h1 id="恋爱"><a href="#恋爱" class="headerlink" title="恋爱"></a>恋爱</h1><hr><ul><li>这方面我很失败，就不谈经验了，谈谈教训吧。</li><li>要好好待人家，别只顾自己学习和玩乐。</li><li>别太自私，毕竟人家为你付出了青春。</li><li>为我自己加油。</li></ul><h1 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h1><hr><ul><li>千万别熬夜了，大一大二你还年轻，熬得起。等到了大三大四，身体吃不消了，你会越来越感觉累的。</li><li>还是那句话，学在平时，别在考试前最后一天通宵学习。</li><li>现在想来，还是多运动运动散散步好，整天呆在宿舍太闷了，恐怕是老了吧。</li></ul><p>最后附上今年保研最终排名，见笑了（好基友非要我给他打码，那我就照办了）：<br><img src="top.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 保研 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EOJ3328. 时空交织的代价</title>
      <link href="/2017/09/19/eoj3328/"/>
      <url>/2017/09/19/eoj3328/</url>
      
        <content type="html"><![CDATA[<h1 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h1><hr><p>给定$n$个点和每个点在$x$轴上面的位置和每个点的权值，求出点对之间的费用总和。其中某两个点$i$，$j$之间的费用定义为$ | p_i - p_j | \times max\{v_i, v_j\} $。</p><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><hr><p>对于每个点，计算它左边权值小于它的点与它点对费用之和：\[{v_i}\sum\limits_j {({p_i} - {p_j})}  = k{v_i}{p_i} - {v_i}\sum\limits_j {p_j} \]其中$k$是左边权值小于它的点的数量，$\sum\limits_j {p_j}$是它们的权值之和，可以用两个树状数组统计。右边同理。<br>首先按照$p$从小到大排序。对于每个点用树状数组分别统计出左右两边$v$比它小的$v$之和与数量，然后直接计算结果即可。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><hr><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;functional></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;algorithm></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iterator></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;sstream></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;fstream></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;numeric></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iomanip></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;climits></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;utility></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;complex></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;cstdlib></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;cstring></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;cassert></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;string></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;vector></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;cctype></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;bitset></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;cstdio></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;cmath></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;ctime></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;deque></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stack></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;queue></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;deque></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;list></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;new></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;map></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;set></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//#pragma comment(linker, "/STACK:102400000,102400000")</span><span class="token keyword">typedef</span> <span class="token keyword">long</span> <span class="token keyword">long</span> LL<span class="token punctuation">;</span><span class="token keyword">typedef</span> pair<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token operator">></span> PII<span class="token punctuation">;</span><span class="token keyword">typedef</span> pair<span class="token operator">&lt;</span>PII<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token operator">></span> PIII<span class="token punctuation">;</span><span class="token keyword">typedef</span> vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> vec<span class="token punctuation">;</span><span class="token keyword">typedef</span> vector<span class="token operator">&lt;</span>vec<span class="token operator">></span> mat<span class="token punctuation">;</span><span class="token macro property">#<span class="token directive keyword">define</span> PB push_back</span><span class="token macro property">#<span class="token directive keyword">define</span> MP(a, b) make_pair(a, b)</span><span class="token macro property">#<span class="token directive keyword">define</span> FI first</span><span class="token macro property">#<span class="token directive keyword">define</span> SE second</span><span class="token macro property">#<span class="token directive keyword">define</span> gcd(x, y) __gcd(x, y)</span><span class="token macro property">#<span class="token directive keyword">define</span> gcd3(x, y, z) __gcd(__gcd(x, y), z)</span><span class="token keyword">const</span> <span class="token keyword">double</span> EPS <span class="token operator">=</span> <span class="token number">1e-15</span><span class="token punctuation">;</span><span class="token keyword">const</span> <span class="token keyword">double</span> PI <span class="token operator">=</span> <span class="token function">acos</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">const</span> <span class="token keyword">int</span> INF <span class="token operator">=</span> <span class="token number">0x3f3f3f3f</span><span class="token punctuation">;</span><span class="token keyword">const</span> LL INFL <span class="token operator">=</span> <span class="token number">0x3f3f3f3f3f3f3f3fLL</span><span class="token punctuation">;</span><span class="token keyword">const</span> <span class="token keyword">int</span> MAXN <span class="token operator">=</span> <span class="token number">200000</span> <span class="token operator">+</span> <span class="token number">10</span><span class="token punctuation">;</span><span class="token keyword">const</span> <span class="token keyword">int</span> MOD <span class="token operator">=</span> <span class="token number">1000000007</span><span class="token punctuation">;</span>LL bit<span class="token punctuation">[</span>MAXN<span class="token punctuation">]</span><span class="token punctuation">,</span> bit1<span class="token punctuation">[</span>MAXN<span class="token punctuation">]</span><span class="token punctuation">,</span> n<span class="token punctuation">;</span>LL <span class="token function">sum</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token punctuation">)</span> <span class="token punctuation">{</span>    LL s <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token punctuation">(</span>s <span class="token operator">+</span><span class="token operator">=</span> bit<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">=</span> MOD<span class="token punctuation">;</span>        i <span class="token operator">-</span><span class="token operator">=</span> i <span class="token operator">&amp;</span> <span class="token operator">-</span>i<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> s<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">add</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token punctuation">,</span> LL x<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;=</span> <span class="token number">10000</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token punctuation">(</span>bit<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span><span class="token operator">=</span> x<span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">=</span> MOD<span class="token punctuation">;</span>        i <span class="token operator">+</span><span class="token operator">=</span> i <span class="token operator">&amp;</span> <span class="token operator">-</span>i<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span>LL <span class="token function">sum1</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token punctuation">)</span> <span class="token punctuation">{</span>    LL s <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token punctuation">(</span>s <span class="token operator">+</span><span class="token operator">=</span> bit1<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">=</span> MOD<span class="token punctuation">;</span>        i <span class="token operator">-</span><span class="token operator">=</span> i <span class="token operator">&amp;</span> <span class="token operator">-</span>i<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> s<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">add1</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token punctuation">,</span> LL x<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;=</span> <span class="token number">10000</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token punctuation">(</span>bit1<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span><span class="token operator">=</span> x<span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">=</span> MOD<span class="token punctuation">;</span>        i <span class="token operator">+</span><span class="token operator">=</span> i <span class="token operator">&amp;</span> <span class="token operator">-</span>i<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">struct</span> node <span class="token punctuation">{</span>    LL p<span class="token punctuation">,</span> v<span class="token punctuation">;</span>    <span class="token keyword">bool</span> <span class="token keyword">operator</span> <span class="token operator">&lt;</span> <span class="token punctuation">(</span><span class="token keyword">const</span> node<span class="token operator">&amp;</span> rhs<span class="token punctuation">)</span> <span class="token keyword">const</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> p <span class="token operator">&lt;</span> rhs<span class="token punctuation">.</span>p<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">;</span>node a<span class="token punctuation">[</span>MAXN<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    cin <span class="token operator">>></span> n<span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%lld"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>p<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%lld"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">sort</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> a <span class="token operator">+</span> n<span class="token punctuation">)</span><span class="token punctuation">;</span>    LL ans <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        LL cnt <span class="token operator">=</span> <span class="token function">sum</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>        LL s <span class="token operator">=</span> <span class="token function">sum1</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">(</span>ans <span class="token operator">+</span><span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span>cnt <span class="token operator">*</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>p <span class="token operator">-</span> s<span class="token punctuation">)</span> <span class="token operator">%</span> MOD <span class="token operator">+</span> MOD<span class="token punctuation">)</span> <span class="token operator">%</span> MOD<span class="token punctuation">)</span> <span class="token operator">*</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">=</span> MOD<span class="token punctuation">;</span>        <span class="token function">add</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">add1</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">,</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>p<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">memset</span><span class="token punctuation">(</span>bit<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span> bit<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">memset</span><span class="token punctuation">(</span>bit1<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span> bit1<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">;</span> <span class="token operator">--</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        LL cnt <span class="token operator">=</span> <span class="token function">sum</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>        LL s <span class="token operator">=</span> <span class="token function">sum1</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">(</span>ans <span class="token operator">+</span><span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span>s <span class="token operator">-</span> cnt <span class="token operator">*</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>p<span class="token punctuation">)</span> <span class="token operator">%</span> MOD <span class="token operator">+</span> MOD<span class="token punctuation">)</span> <span class="token operator">%</span> MOD<span class="token punctuation">)</span> <span class="token operator">*</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">=</span> MOD<span class="token punctuation">;</span>        <span class="token function">add</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">add1</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">,</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>p<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">memset</span><span class="token punctuation">(</span>bit<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span> bit<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">memset</span><span class="token punctuation">(</span>bit1<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span> bit1<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">;</span> <span class="token operator">--</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        LL cnt <span class="token operator">=</span> <span class="token function">sum</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token function">sum</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        LL s <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token function">sum1</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token function">sum1</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">%</span> MOD <span class="token operator">+</span> MOD<span class="token punctuation">)</span> <span class="token operator">%</span> MOD<span class="token punctuation">;</span>        <span class="token punctuation">(</span>ans <span class="token operator">-</span><span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span>s <span class="token operator">-</span> cnt <span class="token operator">*</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>p<span class="token punctuation">)</span> <span class="token operator">%</span> MOD <span class="token operator">+</span> MOD<span class="token punctuation">)</span> <span class="token operator">%</span> MOD<span class="token punctuation">)</span> <span class="token operator">*</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">=</span> MOD<span class="token punctuation">;</span>        <span class="token function">add</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">add1</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>v<span class="token punctuation">,</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>p<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%lld\n"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>ans <span class="token operator">%</span> MOD <span class="token operator">+</span> MOD<span class="token punctuation">)</span> <span class="token operator">%</span> MOD<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> EOJ </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
