---
title: Meta Multi-Task Learning for Sequence Modeling
date: 2018-10-25 22:50:40
top: false
cover: false
password:
toc: true
mathjax: true
summary:
tags:
- AAAI
- 自然语言处理
- 神经网络
- 深度学习
- 迁移学习
categories:
- 元学习
---

<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=32507551&auto=1&height=66"></iframe></div>

> 这篇文章是知识分析课准备讲的论文，随便拿来看一看了，简单介绍一下吧，论文是复旦邱锡鹏老师组写的。

**论文地址：**[Meta Multi-Task Learning for Sequence Modeling](https://arxiv.org/pdf/1802.08969.pdf)

# 介绍
---
多任务学习一般的模型是共享特征表示层，也就是最底层的特征表示层是共享的，上层的神经网络都是随具体任务而不同的。但是这有个问题，比如用LSTM对句子进行建模的时候，不同的短语的组合函数是一样的，比如动词+名词、形容词+名词。但是组合函数应该定义成不同的比较好，于是这篇文章提出了针对不同的任务，不同的时刻产生不同的参数矩阵的动态参数生成方法。

本文主要有如下三个贡献点：
* 不同于以往的特征层的共享，本文模型提出了函数层的共享，也就是针对不同任务动态的生成不同的组合函数。
* 不仅对于多任务，Meta-LSTM对于单任务也有提升，因为是动态生成参数，所以每个时刻的参数都不一样，可以更好地表示不同的短语语义。
* 模型还可以被用作迁移学习，Meta-LSTM在训练完成后可以直接被用于新任务上面作为先验知识，而任务特定的LSTM就作为后验知识。

# 模型
---
## 任务介绍
本文主要在序列标注和文本分类两个任务上面做实验，而且是多任务的，序列标注包括NER和POS tagging，文本分类包括多个不同领域的文本分类。

## 传统模型
传统的多任务模型共享一个私有LSTM特征表示层，用这个私有LSTM学习出句子的表示，然后和词向量拼接共同输入到任务特定的公有LSTM去。具体结构如下图所示：
![](1.jpg)
输出层每个任务都是不共享的，和一般的模型一样，这里就不介绍了。最终的损失函数为所有任务的损失函数加权之和。

多任务模型的训练策略如下所示：
* 首先随机选择一个任务。
* 然后从这个任务的数据集中随机选择一个mini-batch。
* 然后用这个任务的mini-batch数据去训练并更新参数。
* 不断重复以上三个过程。

这样就可以训练出一个适用于所有任务的多任务模型。

## 元多任务学习
传统模型只共享了特征表示层，也就是共享了私有LSTM。本文的模型创新就是通过Meta-LSTM动态生成针对每个任务、每个时刻不同的参数，然后用每个任务特定的Basic-LSTM进行编码。具体结构如下图所示：
![](2.jpg)
其中Basic-LSTM的结构和普通的LSTM基本一样，唯一区别就是每个时刻的参数W和b是通过Meta-LSTM动态生成的，形式化定义如下：
![](3.jpg)
因为W维度过大，计算复杂度太高，并且也容易导致过拟合，所以这里采用了SVD分解：
![](4.jpg)
而这里的$z_t$就是通过Meta-LSTM动态生成的，形式化定义如下：
![](5.jpg)
如果精简的表示出这个LSTM之间的关系，可以写成如下形式：
![](6.jpg)
概括起来就是：**Basic-LSTM上一个时刻的输出$h_{t - 1}$、Meta-LSTM上一个时刻的输出$\hat h_{t - 1}$和当前时刻的单词表示$x_t$作为Meta-LSTM当前时刻的输入，产生的输出$z_t$用来产生Basic-LSTM当前时刻的参数矩阵。**

Meta-LSTM主要有如下两个优点：
* 一个就是每个时刻的参数动态生成。
* 另一个就是比普通的LSTM参数数量更少，因为有SVD分解。

# 实验
---
## 文本分类
文本分类任务是在16个购物网站评论数据集上做的，数据集大小如下所示：
![](7.jpg)
最后在大多数数据集上，Meta-LSTM都能做到最好结果，具体结果如下：
![](8.jpg)

## 序列标注
---
序列标注任务是在三个数据集上面做的，两个是NER数据集，一个是POS tagging数据集，具体结果如下：
![](9.jpg)
只能说比最基础的LSTM+CRF模型高了那么一丢丢吧。

# 总结
---
本文提出了一种function-level的多任务共享机制，即使用Meta-LSTM来动态产生Basic-LSTM每个时刻的参数矩阵。

> 看完后我在想，这个动态参数生成的机制能不能用在成分句法分析上面，例如对于top-down的chart-based模型，可以自顶向下通过Tree-LSTM动态产生每一个树结点的参数矩阵，然后用这个参数矩阵来预测结点的label和split。