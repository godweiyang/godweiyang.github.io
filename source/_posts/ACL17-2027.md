---
title: Implicitly-Defined Neural Networks for Sequence Labeling
date: 2017-10-25 00:50:25
top: false
cover: false
password:
toc: true
mathjax: true
summary:
tags:
- ACL
- 自然语言处理
- 神经网络
- 深度学习
- 序列标注
categories:
- 序列标注
---
论文链接：[P17-2027](https://www.aclweb.org/anthology/P/P17/P17-2027.pdf)

# 摘要
---
这篇论文介绍了一种新奇的、隐式定义神经网络，并且描述了计算它的方法。

# 1 介绍
---
传统的双向RNN只能单独计算两个方向的隐含层，现在介绍一种新的机制，将两个方向的信息直接结合起来计算。

# 2 INN
---
### 2.1 传统的RNN
经典的RNN给定一个输入序列$[{\xi _1},{\xi _2}, \ldots ,{\xi _n}]$和初始隐含层状态${h_s}$，然后迭代产生后续的隐含层状态：
\\[\begin{array}{l}{h_1} = f({\xi _1},{h_s})\\{h_2} = f({\xi _2},{h_1})\\ \cdots \\{h_n} = f({\xi _n},{h_{n - 1}})\end{array}\\]LSTM、GRU和其他的相关变体计算方法也都类似，都是像下图这样线性计算，每一时刻的状态只依赖于当前输入和前一时刻的状态。
![](1.png)

### 2.2 改进结构
这篇论文中这样计算隐含层状态：
\\[{h_t} = f({\xi _t},{h_{t - 1}},{h_{t + 1}})\\]这样整个隐含层状态序列的等式就是隐式的，记为：
\\[H = [{h_1},{h_2}, \ldots ,{h_n}]\\]在这个神经网络中，定义如下变量：数据$X$、标签$Y$、参数$\theta$，定义如下函数：
输入层变换：
\\[\xi  = g(\theta ,X)\\]隐式隐含层：
\\[H = F(\theta ,\xi ,H)\\]损失函数：
\\[L = \ell (\theta ,H,Y)\\]定义${h_s}$和${h_e}$为边界状态，$n$为输入序列长度，$F$函数构造出了一系列非线性等式：
\\[\begin{array}{l}{h_1} = f({h_s},{h_2},{\xi _1})\\ \cdots \\{h_i} = f({h_{i - 1}},{h_{i + 1}},{\xi _i})\\ \cdots \\{h_n} = f({h_{n - 1}},{h_e},{\xi _n})\end{array}\\]INN结构如下图：
![](2.png)

### 2.3 计算前向传播
为了计算等式$H = F(H)$，采用拟牛顿法。
令$G = H - F(H)$，转化为计算等式$G = 0$。
\\[\begin{array}{l}{H_{n + 1}} = {H_n} - {({\nabla _H}G)^{ - 1}}G\\{H_{n + 1}} = {H_n} - {(I - {\nabla _H}F)^{ - 1}}({H_n} - F({H_n}))\end{array}\\]注意到$(I - {\nabla _H}F)$是一个稀疏矩阵，所以采用Krylov子空间方法，具体是稳定双共轭梯度法(BICG-STAB)算法来计算。

### 2.4 梯度
为了训练模型，采用随机梯度下降，定义损失函数：
\\[{\nabla _\theta }L = {\nabla _\theta }\ell  + {\nabla _H}\ell {\nabla _\theta }H\\]其中
\\[{\nabla _\theta }H = {\nabla _\theta }F + {\nabla _H}F{\nabla _\theta }H + {\nabla _\xi }F{\nabla _\theta }\xi \\]所以
\\[{\nabla _\theta }H = {(I - {\nabla _H}F)^{ - 1}}({\nabla _\theta }F + {\nabla _\xi }F{\nabla _\theta }\xi )\\]所以整个梯度就是
\\[{\nabla _\theta }L = {\nabla _\theta }\ell  + {\nabla _H}\ell {(I - {\nabla _H}F)^{ - 1}}({\nabla _\theta }F + {\nabla _\xi }F{\nabla _\theta }\xi )\\]

### 2.5 转换函数
回忆在GRU中，有如下转换函数：
\\[\begin{array}{l}{h_t} = (1 - {z_t}){ {\hat h}_t} + {z_t}{ {\tilde h}_t}\\{ {\tilde h}_t} = \tanh (W{x_t} + U({r_t}{ {\hat h}_t}) + \tilde b)\\{z_t} = \sigma ({W_z}{x_t} + {U_z}{ {\hat h}_t} + {b_z})\\{r_t} = \sigma ({W_r}{x_t} + {U_r}{ {\hat h}_t} + {b_r})\end{array}\\]其中在GRU中${ {\hat h}_t} = {h_{t - 1}}$，在INN中做一个替代：
\\[\begin{array}{l}{ {\hat h}_t} = s{h_{t - 1}} + (1 - s){h_{t + 1}}\\s = \frac{ { {s_p}}}{ { {s_p} + {s_n}}}\\{s_p} = \sigma ({W_p}{x_t} + {U_p}{h_{t - 1}} + {b_p})\\{s_n} = \sigma ({W_n}{x_t} + {U_n}{h_{t + 1}} + {b_n})\end{array}\\]

# 3 实验
---
### 序列标注
如下图所示，这个模型的效果甚至比标准的序列标注器还要好！
![](3.png)

# 4 结论
---
介绍了一种隐式定义神经网络，应用到了序列标注任务上，效果比双向LSTM、双向GRU等还要好。
还有一些工作可以改进，比如可以在双向LSTM上面改造INN，加速计算${(I - {\nabla _H}F)^{ - 1}}$等等。