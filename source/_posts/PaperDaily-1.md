---
title: Empower Sequence Labeling with Task-Aware Neural Language Model
date: 2018-01-09 22:28:40
top: false
cover: false
password:
toc: true
mathjax: true
summary:
tags:
- AAAI
- 自然语言处理
- 神经网络
- 深度学习
- 迁移学习
- 序列标注
categories:
- 序列标注
---

自从这学期没课以来，一直过着非正常人的生活，作息时间比正常人推迟了3个小时：**3点睡觉、12点起床、15点吃午饭、21点吃晚饭。**因此决定不再如此颓废，每日泛读一篇顶会paper，了解其大概思想即可，然后大概将思想发出来，美其名曰：PaperDaily，就从今天开始吧。

今天要讲的这篇是昨天偶然看到的，AAAI18的paper，正好与我文本挖掘课大作业主题一样，所以就看了一下。

# 介绍
---
这篇paper提出的模型叫做LM-LSTM-CRF，看起来和前面讲过的加入语言模型的两篇没有大区别，事实是区别的确不是很大。之前讲过的transfer模型基本都是共享一部分模型（底层模型），上层模型都是每个任务有各自独立的模型。然而这篇paper的模型所有部分全部共享，这就会带来许多表示上面的问题。于是这篇paper和以往最大的区别就是在character level LSTM之上加入了一个highway layer，用来将LSTM产生的字符表示映射到不同的表示空间，这样语言模型（这里的语言模型是基于字符层面的）和序列标注模型就可以共享character level LSTM。

# 模型
---
不多说，直接上图：
![](1.png)
### character level LSTM
这里和传统的差不多，只是改每个token单独训练一个LSTM为所有字符联合训练上下文表示（为了语言模型共用嘛），但是只在两个tokens之间输出token表示。
### highway layer
其实就是对输出做了线性变换+门操作，具体表示如下：
\\[\begin{array}{l}m = H(n) = t \odot g({W_H}n + {b_H}) + (1 - t) \odot n\\t = \sigma ({W_T}n + {b_T})\end{array}\\]最终一共产生四个highway输出，分别是前后向序列标注表示和前后向语言模型表示。而序列标注的LSTM输入共有三个，分别是词向量、前后向序列标注表示。
### word level LSTM
和传统的没什么不同。。。
### CRF layer
没什么不一样。。。
### joint training
总的损失函数就是语言模型损失函数加上序列标注损失函数，系数这里设置为1:1。

# 实验结果
---
直接上图，这里和之前我看过的几篇paper都进行了比较，还是非常bang的！
![](2.png)
注意到，他们的结果和```Peter 2017.```比较还是差了一点，因为```Peter 2017.```虽然也使用了语言模型作为辅助训练，但是他们语言模型是在大量无标注数据下进行训练的，而且花费时间特别长！而本文根本不需要任何辅助数据，少量标注数据？足够了！就在他们上面联合训练一个语言模型就行了，花费时间大大缩短。

其实我个人认为，这零点几的提升意义并不是很大，时间大大缩短倒是挺不错的，毕竟```Peter 2017.```那篇32个GPU都要训练半个月。。。
更远一步思考，也许可以将语言模型和序列标注独立开来训练，先用语言模型来训练character level LSTM，再用它产生每个token的表示，直接输入到序列标注的LSTM中，当然highway layer还是必要的，毕竟表示空间是不同的。这样可以利用大量的无标注数据了，但是训练时间也会大大加长，而且感觉和```Peter 2017.```的模型区别貌似不大了？只是联合训练了一个character level LSTM而已。

以上都是我的拙见，毕竟这篇也就粗略读了一下没仔细看，各位有什么想法也欢迎和我讨论。