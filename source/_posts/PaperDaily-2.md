---
title: Parsing with Compositional Vector Grammars
date: 2018-01-10 20:24:48
top: false
cover: false
password:
toc: true
mathjax: true
summary:
tags:
- ACL
- 自然语言处理
- 神经网络
- 深度学习
- 句法分析
categories:
- 句法分析
---

今天也没看新的paper，就讲讲我的毕设的paper吧，估计等我文本挖掘这门课上完，也不会再看太多序列标注相关的了，重点要转移到parsing了。毕竟序列标注效果也已经很好了，迁移学习方面也暂时不想弄，以后研究重点还是可能在parsing吧。

# 介绍
---
这篇paper名字叫做基于成分向量文法的句法分析，那么这是个什么东西呢？大家都知道（也许不知道？我就默认知道了( ╯□╰ )）概率上下文无关文法（PCFG）吧，这是基于传统方法的短语结构句法分析，也叫成分句法分析，还有一种叫做依存句法分析，现在大多数是做这个的。但是传统的成分句法分析无法解决歧义的问题，因为PCFG是基于上下文无关的独立性假设的，但是自然语言是一种上下文有关文法，必然会产生歧义。

那么如何消除这种歧义呢，Socher提出了SU-RNN的模型，引入了短语的语义表示，具体是什么样子的呢？

# 模型
---
这个模型概括起来是这样的：首先用PCFG产生k-best句法树，比如产生概率最大的20棵句法树。然后对这k-best棵句法树每棵树都跑一遍SU-RNN，计算出每棵树的得分，然后综合PCFG得分和SU-RNN得分，对他们进行重排序，然后得到排名最高的句法树。

那么怎么通过SU-RNN计算一棵树的得分呢？

首先上一张图，看看SU-RNN是个什么结构：
![](1.png)
可以看出，每个节点不仅含有它的类别表示，还有一个向量表示它的语义信息。而SU-RNN与之前提出过的RNN不一样的是，这里的每个节点的$W$权值矩阵全部是不同的，依赖于它的子节点的类别。
每个节点的语义表示向量计算方法如下：
\\[{p^{(1)}} = f\left( { {W^{(B,C)}}\left[ {\begin{array}{*{20}{c}}b\\c\end{array}} \right]} \right)\\]而这个节点的得分表示为
\\[s({p^{(1)}}) = {({v^{(B,C)}})^T}{p^{(1)}} + \log P({P_1} \to B{\rm{ }}C)\\]最后一整棵树的得分就是
\\[s(CVG(\theta ,x,\hat y)) = \sum\limits_{d \in N(\hat y)} {s({p^d})} \\]这样就可以枚举所有的句法树，然后计算得到得分最高的那棵树就是最终的句法树了。

但是这样枚举的话复杂度太高了，要知道一个长度为$n$的句子，可能的句法树有$Catalan(n)$种。而且是无法用动态规划算法来计算最优句法树的，因为SU-RNN破坏了上下文无关的独立性假设（因为反向传播？其实我也不是太懂。。。）。所以就要用到之前所说的先用PCFG得到k-best棵句法树，然后用SU-RNN重排序了。

那么用SU-RNN计算完得到最优树之后，怎么计算它与gold-tree之间的差异，从而得到loss呢？

本文计算两棵树差异的公式如下：
\\[\Delta ({y_i},\hat y) = \sum\limits_{d \in N(\hat y)} {\kappa 1\{ d \notin N({y_i})\} } \\]最终的损失函数定义为：
\\[J(\theta ) = \frac{1}{m}\sum\limits_{i = 1}^m { {r_i}(\theta )}  + \frac{\lambda }{2}{\left\| \theta  \right\|^2}\\]其中
\\[{r_i}(\theta ) = \mathop {\max }\limits_{\hat y \in Y({x_i})} (s(CVG({x_i},\hat y)) + \Delta ({y_i},\hat y)) - s(CVG({x_i},{y_i}))\\]也就是要尽量最大化标准树的得分，减小预测树的得分。

# 实验结果
---
![](2.png)
可以看出，SU-RNN结果比以往的结果都要好，但是没有最后两行的好。。。最后两个具体是啥我也没去细看。

# 我的毕设任务
---
其实我的任务不用PCFG，看起来减少了工作量？嘿嘿，其实貌似麻烦的一笔啊。。。我的模型主要的思想就是直接用SU-RNN训练出句法分析树！那枚举复杂度太高了怎么办？用动态规划啊！不是不能用吗？没事，假装它能用，要是效果好强行解释一波就行了。。。而且原模型的RNN是递归神经网络哦，这次我改成了循环神经网络，用LSTM来计算得分。看起来貌似挺麻烦的，纠结了好几天。LSTM每个节点总得有一个$x$输入，一个$h$隐层输入吧，所以可能还要给每两个节点指定一个作为head。。。

感觉心态炸了哦，一堆非主流写法？也不知道最后能不能写出来，也不知道写出来结果怎么样。。。说不定要延毕了？哈哈，自嘲一波吧，暑假好好研究研究了，python基础还是不行，写起来太累了。。。