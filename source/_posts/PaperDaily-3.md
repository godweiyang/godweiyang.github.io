---
title: Head-Lexicalized Bidirectional Tree LSTMs
date: 2018-01-13 14:41:01
top: false
cover: false
password:
toc: true
mathjax: true
summary:
tags:
- TACL
- 自然语言处理
- 神经网络
- 深度学习
categories:
- 深度学习
---

首先给大家说明一下，前两天因为新入手了一个ukulele（就是下图这玩意），所以痴迷于学习弹奏，没有更新博客。照这个节奏下去，PaperDaily恐怕是要变成PaperWeekly了。（囧。。。）寒假一定要学会《小幸运》，嗯。
![](ukulele.jpg)

好了，今天开始恢复吧（说不准过两天我又鸽了，嘻嘻嘻）。

今天要讲的这篇是TACL2017的，是关于树状LSTM的。

# 介绍
---
前两年已经有人提出了树状LSTM的概念，之前一直不知道是个啥高大上的结构。其实就是递归神经网络中的结点单元替换成LSTM的结点单元。那有人要问了，LSTM不是有$x$输入吗，还有一个$h_{t-1}$输入，那这树状的哪来这两个输入？其实很简单，只要把这两个输入替换成左右儿子的输出表示就行了。瞬间感觉也没啥意思，就是换了结点函数，使得树状LSTM具有了遗忘功能，从而能够处理很长的句子罢了。

今天介绍的这篇paper就是在这基础上做了两点改进。
* 除了左右儿子作为输入之外，还增加了$x$输入。$x$是啥呢？就是左右儿子中的头结点，头结点的话传统方法是根据规则来判断谁是头结点的，这里省去了这些复杂的步骤，直接将头结点的判断丢进神经网络中训练。
* 增加了反向的树状LSTM，也就是top-down的LSTM。那有人就很好奇了，一个结点分解成两个结点，怎么可能？其实从头结点到任意一个其他结点的路径都可以看成一个独立的LSTM，如果向左参数就是$U_L$，否则就是$U_R$。

# 模型
---
![](1.png)
图1是序列LSTM和树状LSTM的结构区别，这个树状LSTM是之前传统的树状LSTM，不带head结点的。

之前的基本的树状LSTM的结点单元的具体公式如下：
![](5.png)
具体我就不解释了（懒。。。），自行类比序列LSTM。

加入head结点之后，公式区别如下（加粗所示）：
![](2.png)
![](3.png)
![](4.png)

那么top-down是怎么做的呢？
![](6.png)
这里可以看出来，向左向右是用的两套不同的参数。注意到，top-down方向的LSTM前提是一定要用head机制作为支撑！不然头结点的$x$算不出来的话是没有办法计算的哦。

具体训练过程等等就不再阐述了。

# 总结
---
这个模型主要是用在了文本分类和情感分类上，我在想能不能用在我的毕设上面。

我觉得head机制可以加进去，但是反向LSTM貌似是不可行的，因为这里的短语结构树全部是给定的，所以向下计算知道什么时候停止。但是我是做句法分析任务的，没有给定句法树，向下计算无法知道什么时候停止扩展，情况有无数种！向上计算倒是无所谓，最多卡特兰数级别，加上动态规划，可以缩小到$n\log n$级别。